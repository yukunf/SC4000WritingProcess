{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 59291,
     "databundleVersionId": 6678907,
     "isSourceIdPinned": false,
     "sourceType": "competition"
    },
    {
     "sourceId": 13682482,
     "sourceType": "datasetVersion",
     "datasetId": 8701104
    },
    {
     "sourceId": 13682716,
     "sourceType": "datasetVersion",
     "datasetId": 8701282
    },
    {
     "sourceId": 13716662,
     "sourceType": "datasetVersion",
     "datasetId": 8726399
    },
    {
     "sourceId": 637915,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": false,
     "modelInstanceId": 480881,
     "modelId": 496492
    },
    {
     "sourceId": 641335,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 483649,
     "modelId": 499161
    },
    {
     "sourceId": 641788,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 484011,
     "modelId": 499513
    }
   ],
   "dockerImageVersionId": 31192,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Integrated Notebook for Task WritingProcess\n## Data Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---------\n### Idle Removing and Time Regularization from `preprocess.py`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "KAGGLE_ENVIRONMENT = False\n",
    "\n",
    "\n",
    "if KAGGLE_ENVIRONMENT:\n",
    "    TEST_DATA_PATH = \"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\"\n",
    "    TRAIN_DATA_PATH = \"/kaggle/input/trainlogs-unicodefixed/train_logs_raw_unicode_fixed.csv\"\n",
    "    TRAIN_DATA_PREPROCESSED_PATH = \"/kaggle/input/train-preprocessed/train_preprocessed.csv\"\n",
    "    TRAIN_SCORE_PATH = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\"\n",
    "    SUBMISSION_PATH  = '/kaggle/working/submission.csv'\n",
    "else:\n",
    "    TEST_DATA_PATH = \"data/test_logs.csv\"\n",
    "    TRAIN_DATA_PATH = \"data/train_logs_raw_unicode_fixed.csv\"\n",
    "    TRAIN_DATA_PREPROCESSED_PATH = \"data/train_preprocessed.csv\"\n",
    "    TRAIN_SCORE_PATH = \"data/train_logs.csv\"\n",
    "    SUBMISSION_PATH  = 'data/submission.csv'"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "`ftfy` takes tons of time to run so I temporary disabled it on train set, a pre-processed dataset is uploaded to replace it. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Solve ftfy dependency\n",
    "if KAGGLE_ENVIRONMENT:\n",
    "    !pip install /kaggle/input/ftfypkg/ftfy_pkg/wcwidth-0.2.14-py2.py3-none-any.whl --no-index --find-links /kaggle/input/ftfypkg/ftfy_pkg\n",
    "\n",
    "    !pip install /kaggle/input/ftfypkg/ftfy_pkg/ftfy-6.3.1-py3-none-any.whl --no-index --find-links /kaggle/input/ftfypkg/ftfy_pkg\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-13T15:00:24.727035Z",
     "iopub.execute_input": "2025-11-13T15:00:24.727303Z",
     "iopub.status.idle": "2025-11-13T15:00:35.410696Z",
     "shell.execute_reply.started": "2025-11-13T15:00:24.727246Z",
     "shell.execute_reply": "2025-11-13T15:00:35.409512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Looking in links: /kaggle/input/ftfypkg/ftfy_pkg\nProcessing /kaggle/input/ftfypkg/ftfy_pkg/wcwidth-0.2.14-py2.py3-none-any.whl\nInstalling collected packages: wcwidth\n  Attempting uninstall: wcwidth\n    Found existing installation: wcwidth 0.2.13\n    Uninstalling wcwidth-0.2.13:\n      Successfully uninstalled wcwidth-0.2.13\nSuccessfully installed wcwidth-0.2.14\nLooking in links: /kaggle/input/ftfypkg/ftfy_pkg\nProcessing /kaggle/input/ftfypkg/ftfy_pkg/ftfy-6.3.1-py3-none-any.whl\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy==6.3.1) (0.2.14)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.3.1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport ftfy\nimport warnings\nfrom pathlib import Path\nimport re\n\nwarnings.filterwarnings('ignore')",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:35:08.265907Z",
     "start_time": "2025-11-10T14:35:08.262529Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-13T15:00:35.413381Z",
     "iopub.execute_input": "2025-11-13T15:00:35.413665Z",
     "iopub.status.idle": "2025-11-13T15:00:36.476666Z",
     "shell.execute_reply.started": "2025-11-13T15:00:35.413627Z",
     "shell.execute_reply": "2025-11-13T15:00:36.475347Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "class Preprocess:\n\n    def label_encoding(self, df, col=\"id\"):\n        label_encoder = LabelEncoder()\n        label_encoder.fit(df[col])\n        df[col + \"_encoded\"] = label_encoder.transform(df[col])\n        return df\n\n    # remove time that the author havent start writing or is resting\n    # reference: remove_margin for https://www.kaggle.com/code/tomooinubushi/1st-place-solution-training-and-inference-code\n\n    def remove_start_and_end_time(\n        self, df, start_margin=2 * 60 * 1000, end_margin=2 * 60 * 1000\n    ):\n        df = df[df[\"up_event\"] != \"Unidentified\"].reset_index(drop=True)\n        result_df = []\n        grouped_df = df.groupby(\"id_encoded\")\n\n        for _, log in tqdm(grouped_df):\n            valid_events = log[\n                (log.activity != \"Nonproduction\")\n                | (log.up_event != \"Shift\")\n                | (log.up_event != \"CapsLock\")\n            ].down_time.values\n            if len(valid_events) == 0:\n                continue\n            log = log[\n                (log.down_time > valid_events.min() - start_margin)\n                & (log[\"down_time\"] <= valid_events.max() + end_margin)\n            ].copy()\n            log[\"event_id\"] = range(len(log))\n            result_df.append(log)\n\n        result = pd.concat(result_df, ignore_index=True)\n\n        return result\n\n    def remove_rest_time(\n        self, df, time_margin=1 * 60 * 1000, action_margin=5 * 60 * 1000\n    ):\n        down_times, up_times = [], []\n        prev_idx = -1\n        result_df = df[[\"id_encoded\", \"down_time\", \"up_time\"]].values\n        for row in tqdm(result_df):\n            idx, down_time, up_time = int(row[0]), int(row[1]), int(row[2])\n            if prev_idx != idx:\n                prev_down_time = down_time\n                prev_corrected_down_time = 0\n            gap_down_time = np.clip(down_time - prev_down_time, 0, time_margin)\n            action_time = np.clip(up_time - down_time, 0, action_margin)\n\n            new_down_time = prev_corrected_down_time + gap_down_time\n            new_up_time = new_down_time + action_time\n            down_times.append(new_down_time)\n            up_times.append(new_up_time)\n            prev_idx, prev_corrected_down_time, prev_down_time = (\n                idx,\n                new_down_time,\n                down_time,\n            )\n        df[\"down_time\"], df[\"up_time\"] = down_times, up_times\n        return df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:35:08.681597Z",
     "start_time": "2025-11-10T14:35:08.676159Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-13T15:00:36.477722Z",
     "iopub.execute_input": "2025-11-13T15:00:36.478479Z",
     "iopub.status.idle": "2025-11-13T15:00:36.490100Z",
     "shell.execute_reply.started": "2025-11-13T15:00:36.478448Z",
     "shell.execute_reply": "2025-11-13T15:00:36.488998Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "preprocessor = Preprocess()\n# ------------------ Config dataset (In submission we only have test file) ----------------------------\ndf = pd.read_csv(TEST_DATA_PATH)\n# ------------------ Config dataset (TFIDF has to be fit on train and transform on test) ----------------------------\ntrain_df = pd.read_csv(TRAIN_DATA_PATH)\n# Replacing the original dataset, for fast processing and scoring.\ntrain_score_df = pd.read_csv(TRAIN_SCORE_PATH)\n\n\n\n\n\ndf = preprocessor.label_encoding(df)\ndf = preprocessor.remove_start_and_end_time(df)\ndf = preprocessor.remove_rest_time(df)\n\ntrain_df = preprocessor.label_encoding(train_df)\ntrain_df = preprocessor.remove_start_and_end_time(train_df)\ntrain_df = preprocessor.remove_rest_time(train_df)\n\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-13T15:00:36.491193Z",
     "iopub.execute_input": "2025-11-13T15:00:36.491538Z",
     "execution_failed": "2025-11-13T15:01:12.735Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "-----------\n### Event,Unicode Cleaning from `Preprocessing.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def label_encoding(df, col=\"id\"):\n    label_encoder = LabelEncoder()\n    label_encoder.fit(df[col])\n    df[col + \"_encoded\"] = label_encoder.transform(df[col])\n    return df\n\n\n# remove time that the author havent start writing or is resting\n# reference: remove_margin for https://www.kaggle.com/code/tomooinubushi/1st-place-solution-training-and-inference-code\n\ndef remove_procrastination_time(df, start_margin=2*60*1000, end_margin=2*60*1000):\n    df = df[df['up_event'] != 'Unidentified'].reset_index(drop=True)\n    result_df = []\n    grouped_df = df.groupby('id_encoded')\n\n    for _, log in tqdm(grouped_df):\n        valid_events = log[(log.activity != 'Nonproduction') & (\n            log.up_event != 'Shift') & (log.up_event != 'CapsLock')].down_time.values\n        if len(valid_events) == 0:\n            continue\n        log = log[(log.down_time > valid_events.min() - start_margin)\n                  & (log['down_time'] <= valid_events.max() + end_margin)].copy()\n        log['event_id'] = range(len(log))\n        result_df.append(log)\n\n    result = pd.concat(result_df, ignore_index=True)\n\n    return result\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:40:44.909799Z",
     "start_time": "2025-11-10T14:40:44.905798Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class CleanPreprocessor:\n    def cleaning(self,df,skipUnicodeFixing=False):\n        df = label_encoding(df)\n        df = remove_procrastination_time(df)\n        df = df[df['activity'] != 'Nonproduction' ].reset_index(drop=True)\n        cols = ['down_event', 'up_event', 'text_change']\n        if not skipUnicodeFixing:\n            df.loc[:, cols] = df.loc[:, cols].apply(\n                lambda s: s.astype('string').map(lambda x: ftfy.fix_text(x) if x is not pd.NA else x)\n            )\n        \n        drop_events = ['LeftClick','RightClick']\n        df = df[~df['down_event'].isin(drop_events)]\n        df['event_id'] = df.groupby('id').cumcount() + 1 # reset event_id\n        df.reset_index(inplace=True,drop=True)\n        return df\n    \n\ncleaner = CleanPreprocessor()\ndf = cleaner.cleaning(df)\ntrain_df = cleaner.cleaning(train_df,skipUnicodeFixing=True)\n\n        \n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:41:50.658675Z",
     "start_time": "2025-11-10T14:40:46.429632Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "-------\n### Text Essay Rebuilding\nWork is taken from `text_process.py`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TextProcessor:\n    PUNCTUATION_MAP = {\n        \"SPACE\": \" \",\n        \"COMMA\": \",\",\n        \"DOUBLE_QUOTE\": '\"',\n        \"PERIOD\": \".\",\n        \"PARENTHESES_OPEN\": \"(\",\n        \"PARENTHESES_CLOSE\": \")\",\n        \"SQUARE_BRACKET_OPEN\": \"[\",\n        \"SQUARE_BRACKET_CLOSE\": \"]\",\n        \"CURLY_BRACKET_OPEN\": \"{\",\n        \"CURLY_BRACKET_CLOSE\": \"}\",\n        \"EXCLAMATION_MARK\": \"!\",\n        \"QUESTION_MARK\": \"?\",\n    }\n\n    def insert_text(self, text, s, pos):\n        return \"\".join((text[:pos], s, text[pos:]))\n\n    def remove_text(self, text, s, pos):\n        return \"\".join((text[:pos], text[pos + len(s):]))\n\n    def replace_text(self, text, s1, s2, pos):\n        return \"\".join((text[:pos], s2, text[pos + len(s1):]))\n\n    def move_text(self, text, s, pos1, pos2):\n        text = self.remove_text(text, s, pos1)\n        text = self.insert_text(text, s, pos2)\n        return text\n\n    def split_to_word(self, s):\n        s = s.lower()\n        char_sep = \"@\"\n        punctuation_chars = list(self.PUNCTUATION_MAP.values())\n        for pun in punctuation_chars:\n            s = s.replace(pun, char_sep)\n        s_arr = re.split(char_sep, s)\n        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty words\n        return s_arr\n\n    def split_to_sentence(self, s):\n        s = s.lower()\n        char_sep = \"@\"\n        punctuation = [\".\", \"!\", \"?\"]\n        for punc in punctuation:\n            s = s.replace(punc, char_sep)\n        s_arr = re.split(char_sep, s)\n        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty sentences\n        return s_arr\n\n    def split_to_paragraph(self, s):\n        s = s.lower()\n        s_arr = re.split(r'n\\s*n+', s)\n        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty paragraphs\n        return s_arr\n\n    def change_punctuation(self, text):\n        reverse_map = {v: k.lower()\n                       for k, v in self.PUNCTUATION_MAP.items()}\n        result = []\n        for char in text:\n            if char in reverse_map:\n                result.append(' ' + reverse_map[char] + ' ')\n            else:\n                result.append(char)\n        output = \"\".join(result)\n        output = re.sub(r\"\\s+\", \" \", output).strip()\n\n        return output\n\nclass EssayConstructor:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n\n    def recon_writing(self, df):\n        res_all = []\n        len_texts = []\n        sentence_counts = []\n        paragraph_counts = []\n\n        res = \"\"\n        prev_idx = \"\"\n\n        temp_df = df[['id', 'activity', 'up_event', 'text_change',\n                      'cursor_position', 'word_count']].values\n\n        for row in tqdm(temp_df):\n            idx = str(row[0])\n            activity, up_event, text_change = str(\n                row[1]), str(row[2]), str(row[3])\n            cursor_position, _ = int(row[4]), int(row[5])\n\n            # new idx\n            if idx != prev_idx:\n                if prev_idx != \"\":\n                    # append first essay data\n                    res_all.append(res)\n                    len_texts.append(len_text)\n                    sentence_counts.append(sentence_count)\n                    paragraph_counts.append(paragraph_count)\n\n                res, len_text, sentence_count, paragraph_count = \"\", 0, 0, 0\n                prev_idx = idx\n\n            if activity != \"Nonproduction\":\n                # replace the newline character to n\n                text_change = text_change.replace(\"@\", \"/\").replace(\"\\n\", \"n\")\n\n                if (activity == \"Input\") | (activity == \"Paste\"):\n                    res = self.text_processor.insert_text(\n                        res, text_change, cursor_position - len(text_change)\n                    )\n\n                elif activity == \"Remove/Cut\":\n                    res = self.text_processor.remove_text(\n                        res, text_change, cursor_position\n                    )\n\n                elif activity == \"Replace\":\n                    before, after = text_change.split(\" => \")\n                    res = self.text_processor.replace_text(\n                        res, before, after, cursor_position - len(after)\n                    )\n\n                elif \"Move\" in activity:\n                    pos = [int(s) for s in re.findall(r\"\\d+\", activity)]\n                    # pos 0 start pos1 end pos2 start pos3 end\n                    res = self.text_processor.move_text(\n                        res, text_change, pos[0], pos[2]\n                    )\n\n                len_text = len(res)\n                sentence_count = len(\n                    self.text_processor.split_to_sentence(res))\n                paragraph_count = len(\n                    self.text_processor.split_to_paragraph(res))\n\n            prev_up_event = up_event\n\n        # append last essay data\n        res_all.append(res)\n        len_texts.append(len_text)\n        sentence_counts.append(sentence_count)\n        paragraph_counts.append(paragraph_count)\n\n        return res_all, len_texts, sentence_counts, paragraph_counts\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:43:18.925275Z",
     "start_time": "2025-11-10T14:43:18.914955Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "essay_constructor = EssayConstructor()\nreconstructed_texts, len_texts, sentence_counts, paragraph_counts = essay_constructor.recon_writing(\n    df)\nidx = df[\"id\"].unique()\nresult_df = pd.DataFrame({\"id\": idx, \"text\": reconstructed_texts, \"len_text\": len_texts,\n                         \"sentence_count\": sentence_counts, \"paragraph_count\": paragraph_counts})\n\nextracted_text = result_df\n\nreconstructed_texts, len_texts, sentence_counts, paragraph_counts = essay_constructor.recon_writing(\n    train_df)\nidx = train_df[\"id\"].unique()\nextracted_text_train = pd.DataFrame({\"id\": idx, \"text\": reconstructed_texts, \"len_text\": len_texts,\n                         \"sentence_count\": sentence_counts, \"paragraph_count\": paragraph_counts})\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:49:56.317647Z",
     "start_time": "2025-11-10T14:49:12.168504Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "-------------------\n## Feature Engineering\n### Behaviour Feature\n\nThis part is taken from `feature_extraction.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Feature Extraction Functions\n\nExtract different behavioural features from keystroke logs.\nWe want to capture:\n\n- pauses (when people are thinking)\n- bursts (when they're typing continuously)\n- editing behaviour (how much they revise)\n- cursor movement (planning vs going back to edit)\n\n### 2.1 Base Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:04.963959Z",
     "start_time": "2025-11-10T14:50:04.961921Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def extract_features(df):\n    \"\"\"Pull out the main features from the log data\"\"\"\n    \n    # Count up events and get basic stats\n    features = df.groupby(\"id\").agg(\n        events_count=('event_id', 'count'),\n        total_time=('up_time', 'max'),\n        total_chars=('word_count', 'max'),\n        mean_action_time=('action_time', 'mean'),\n        std_action_time=('action_time', 'std'),\n        max_action_time=('action_time', 'max'),\n        min_action_time=('action_time', 'min'),\n        \n        # Count different types of actions\n        backspace_count=('activity', lambda x: (x == \"Remove/Cut\").sum()),\n        paste_count=('activity', lambda x: (x == \"Paste\").sum()),\n        input_count=('activity', lambda x: (x == \"Input\").sum()),\n        move_count=('activity', lambda x: x.str.contains(\"Move\", na=False).sum()),\n        replace_count=('activity', lambda x: (x == \"Replace\").sum()),\n        nonproduction_count=('activity', lambda x: (x == \"Nonproduction\").sum()),\n        \n        # Where the cursor was\n        cursor_pos_mean=('cursor_position', 'mean'),\n        cursor_pos_std=('cursor_position', 'std'),\n        cursor_pos_max=('cursor_position', 'max'),\n        \n        # Word count stats\n        word_count_mean=('word_count', 'mean'),\n        word_count_std=('word_count', 'std'),\n        word_count_diff=('word_count', lambda x: x.max() - x.min()),\n    ).reset_index()\n    \n    # Replace any missing values with 0\n    features = features.fillna(0)\n    \n    # Calculate some ratios\n    features['chars_per_min'] = features['total_chars'] / (features['total_time'] / 60000 + 1e-6)\n    features['events_per_min'] = features['events_count'] / (features['total_time'] / 60000 + 1e-6)\n    features['backspace_ratio'] = features['backspace_count'] / (features['input_count'] + 1)\n    features['paste_ratio'] = features['paste_count'] / (features['events_count'] + 1)\n    features['replace_ratio'] = features['replace_count'] / (features['events_count'] + 1)\n    features['nonproduction_ratio'] = features['nonproduction_count'] / (features['events_count'] + 1)\n    features['revision_ratio'] = (features['backspace_count'] + features['replace_count']) / (features['total_chars'] + 1)\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:05.268536Z",
     "start_time": "2025-11-10T14:50:05.262686Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 Pause Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def pause_features(df):\n    \"\"\"Get features about pauses (gaps between keystrokes)\"\"\"\n    df = df.sort_values([\"id\", \"down_time\"]).copy()\n    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n    \n    # Count pauses at different thresholds (2s, 5s, 10s)\n    pause_2s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 2000).sum()).rename(\"pause_2s_count\")\n    pause_5s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 5000).sum()).rename(\"pause_5s_count\")\n    pause_10s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 10000).sum()).rename(\"pause_10s_count\")\n    \n    # Basic pause stats\n    mean_pause = df.groupby(\"id\")[\"iki\"].mean().rename(\"mean_pause\")\n    median_pause = df.groupby(\"id\")[\"iki\"].median().rename(\"median_pause\")\n    std_pause = df.groupby(\"id\")[\"iki\"].std().rename(\"std_pause\")\n    max_pause = df.groupby(\"id\")[\"iki\"].max().rename(\"max_pause\")\n    min_pause = df.groupby(\"id\")[\"iki\"].min().rename(\"min_pause\")\n    \n    return pause_2s, pause_5s, pause_10s, mean_pause, median_pause, std_pause, max_pause, min_pause\n\n\ndef burst_features(df):\n    \"\"\"Get features about bursts (when they're typing continuously)\"\"\"\n    df = df.sort_values([\"id\", \"down_time\"]).copy()\n    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n    df[\"burst\"] = (df[\"iki\"] > 2000).astype(int)\n    df[\"burst_id\"] = df.groupby(\"id\")[\"burst\"].cumsum()\n    \n    burst_len = df.groupby([\"id\", \"burst_id\"]).size()\n    avg_burst = burst_len.groupby(\"id\").mean().rename(\"avg_burst\")\n    max_burst = burst_len.groupby(\"id\").max().rename(\"max_burst\")\n    std_burst = burst_len.groupby(\"id\").std().rename(\"std_burst\")\n    \n    return avg_burst, max_burst, std_burst\n\n\ndef p_burst_features(df):\n    \"\"\"Get P-burst features (how many words per burst)\"\"\"\n    df = df.sort_values([\"id\", \"down_time\"]).copy()\n    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n    \n    # P-bursts: pauses longer than 2s\n    df[\"p_burst\"] = (df[\"iki\"] > 2000).astype(int)\n    df[\"p_burst_id\"] = df.groupby(\"id\")[\"p_burst\"].cumsum()\n    \n    # How many words in each burst\n    p_burst_words = df.groupby([\"id\", \"p_burst_id\"])[\"word_count\"].apply(lambda x: x.max() - x.min())\n    avg_words_per_p_burst = p_burst_words.groupby(\"id\").mean().rename(\"avg_words_per_p_burst\")\n    \n    return avg_words_per_p_burst",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:06.009040Z",
     "start_time": "2025-11-10T14:50:06.002041Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 Activity Sequence & Text Change Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def activity_sequence_features(df):\n    \"\"\"Get features from activity patterns and transitions\"\"\"\n    features = []\n    \n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val].sort_values('down_time')\n        activities = id_df['activity'].values\n        \n        # Track how activities transition from one to another\n        transitions = {}\n        for i in range(len(activities) - 1):\n            transition = f\"{activities[i]}->{activities[i+1]}\"\n            transitions[transition] = transitions.get(transition, 0) + 1\n        \n        # Common patterns\n        input_to_remove = transitions.get('Input->Remove/Cut', 0)\n        remove_to_input = transitions.get('Remove/Cut->Input', 0)\n        input_to_input = transitions.get('Input->Input', 0)\n        paste_to_input = transitions.get('Paste->Input', 0)\n        \n        # Find the longest streaks of the same activity\n        max_input_streak = 0\n        max_remove_streak = 0\n        current_input_streak = 0\n        current_remove_streak = 0\n        \n        for act in activities:\n            if act == 'Input':\n                current_input_streak += 1\n                max_input_streak = max(max_input_streak, current_input_streak)\n                current_remove_streak = 0\n            elif act == 'Remove/Cut':\n                current_remove_streak += 1\n                max_remove_streak = max(max_remove_streak, current_remove_streak)\n                current_input_streak = 0\n            else:\n                current_input_streak = 0\n                current_remove_streak = 0\n        \n        # How varied are the activities\n        unique_activities = len(set(activities))\n        activity_switches = sum(1 for i in range(len(activities)-1) if activities[i] != activities[i+1])\n        \n        features.append({\n            'id': id_val,\n            'input_to_remove_trans': input_to_remove,\n            'remove_to_input_trans': remove_to_input,\n            'input_to_input_trans': input_to_input,\n            'paste_to_input_trans': paste_to_input,\n            'max_input_streak': max_input_streak,\n            'max_remove_streak': max_remove_streak,\n            'unique_activities': unique_activities,\n            'activity_switches': activity_switches,\n            'activity_switch_rate': activity_switches / len(activities) if len(activities) > 0 else 0\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef text_change_features(df):\n    \"\"\"Features about how the text changes\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    # How much text was added or removed\n    df['text_change'] = df.groupby('id')['word_count'].diff().fillna(0)\n    \n    features = df.groupby('id').agg(\n        total_text_produced=('text_change', lambda x: x[x > 0].sum()),\n        total_text_removed=('text_change', lambda x: abs(x[x < 0].sum())),\n        text_production_rate=('text_change', lambda x: x[x > 0].mean()),\n        text_removal_rate=('text_change', lambda x: x[x < 0].mean()),\n        max_text_addition=('text_change', 'max'),\n        max_text_removal=('text_change', 'min'),\n        text_volatility=('text_change', 'std'),\n        positive_text_changes=('text_change', lambda x: (x > 0).sum()),\n        negative_text_changes=('text_change', lambda x: (x < 0).sum()),\n    ).reset_index()\n    \n    # Calculate some more useful ratios\n    features['text_removal_ratio'] = features['total_text_removed'] / (features['total_text_produced'] + 1)\n    features['net_text_production'] = features['total_text_produced'] - features['total_text_removed']\n    features['text_efficiency'] = features['total_text_produced'] / (features['positive_text_changes'] + 1)\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:07.308050Z",
     "start_time": "2025-11-10T14:50:07.300951Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 2.4 Temporal & Velocity Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def time_based_features(df):\n    \"\"\"Features based on when things happen (early, middle, late)\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    # Split the writing session into three parts\n    df['time_percentile'] = df.groupby('id')['down_time'].rank(pct=True)\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        # Split into early, middle, and late phases\n        early_phase = id_df[id_df['time_percentile'] <= 0.33]\n        middle_phase = id_df[(id_df['time_percentile'] > 0.33) & (id_df['time_percentile'] <= 0.67)]\n        late_phase = id_df[id_df['time_percentile'] > 0.67]\n        \n        features.append({\n            'id': id_val,\n            'early_events': len(early_phase),\n            'middle_events': len(middle_phase),\n            'late_events': len(late_phase),\n            'early_input_ratio': (early_phase['activity'] == 'Input').sum() / (len(early_phase) + 1),\n            'middle_input_ratio': (middle_phase['activity'] == 'Input').sum() / (len(middle_phase) + 1),\n            'late_input_ratio': (late_phase['activity'] == 'Input').sum() / (len(late_phase) + 1),\n            'early_remove_ratio': (early_phase['activity'] == 'Remove/Cut').sum() / (len(early_phase) + 1),\n            'late_remove_ratio': (late_phase['activity'] == 'Remove/Cut').sum() / (len(late_phase) + 1),\n            'middle_paste_ratio': (middle_phase['activity'] == 'Paste').sum() / (len(middle_phase) + 1),\n            'late_phase_activity': len(late_phase) / (len(id_df) + 1),\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef keystroke_velocity_features(df):\n    \"\"\"Features about typing speed\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    df['iki'] = df.groupby('id')['down_time'].diff()\n    \n    # Only look at actual typing events\n    input_df = df[df['activity'] == 'Input'].copy()\n    \n    if len(input_df) == 0:\n        return pd.DataFrame()\n    \n    features = input_df.groupby('id').agg(\n        input_iki_mean=('iki', 'mean'),\n        input_iki_std=('iki', 'std'),\n        input_iki_median=('iki', 'median'),\n        input_iki_min=('iki', 'min'),\n        input_iki_max=('iki', 'max'),\n        fast_keystrokes=('iki', lambda x: (x < 100).sum()),\n        moderate_keystrokes=('iki', lambda x: ((x >= 100) & (x <= 1000)).sum()),\n        slow_keystrokes=('iki', lambda x: (x > 1000).sum()),\n    ).reset_index()\n    \n    # How consistent is the typing\n    features['keystroke_consistency'] = features['input_iki_std'] / (features['input_iki_mean'] + 1)\n    features['fast_keystroke_ratio'] = features['fast_keystrokes'] / (features['fast_keystrokes'] + features['moderate_keystrokes'] + features['slow_keystrokes'] + 1)\n    features['typing_rhythm_score'] = features['moderate_keystrokes'] / (features['fast_keystrokes'] + features['moderate_keystrokes'] + features['slow_keystrokes'] + 1)\n    \n    return features\n\n\ndef word_count_velocity_features(df):\n    \"\"\"Features about how word count changes\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        word_counts = id_df['word_count'].values\n        time_stamps = id_df['down_time'].values\n        \n        # How fast are words being added\n        if len(word_counts) > 1:\n            word_velocity = np.diff(word_counts) / (np.diff(time_stamps) + 1)\n            \n            features.append({\n                'id': id_val,\n                'avg_word_velocity': np.mean(word_velocity),\n                'max_word_velocity': np.max(word_velocity),\n                'min_word_velocity': np.min(word_velocity),\n                'std_word_velocity': np.std(word_velocity),\n                'positive_velocity_ratio': (word_velocity > 0).sum() / len(word_velocity)\n            })\n        else:\n            features.append({\n                'id': id_val,\n                'avg_word_velocity': 0,\n                'max_word_velocity': 0,\n                'min_word_velocity': 0,\n                'std_word_velocity': 0,\n                'positive_velocity_ratio': 0\n            })\n    \n    return pd.DataFrame(features)\n\n\ndef activity_timing_features(df):\n    \"\"\"How much time is spent on each type of activity\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        # Add up time for each activity\n        input_time = id_df[id_df['activity'] == 'Input']['action_time'].sum()\n        remove_time = id_df[id_df['activity'] == 'Remove/Cut']['action_time'].sum()\n        paste_time = id_df[id_df['activity'] == 'Paste']['action_time'].sum()\n        nonprod_time = id_df[id_df['activity'] == 'Nonproduction']['action_time'].sum()\n        \n        total_time = id_df['action_time'].sum()\n        \n        features.append({\n            'id': id_val,\n            'input_time_total': input_time,\n            'remove_time_total': remove_time,\n            'paste_time_total': paste_time,\n            'nonprod_time_total': nonprod_time,\n            'input_time_ratio': input_time / (total_time + 1),\n            'remove_time_ratio': remove_time / (total_time + 1),\n            'productive_time_ratio': (input_time + paste_time) / (total_time + 1),\n        })\n    \n    return pd.DataFrame(features)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:08.357359Z",
     "start_time": "2025-11-10T14:50:08.348076Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 2.5 Revision & Cursor Movement Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def revision_pattern_features(df):\n    \"\"\"Features about revision behaviour and editing patterns\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        # Where in the text are they making changes\n        cursor_positions = id_df['cursor_position'].values\n        activities = id_df['activity'].values\n        word_counts = id_df['word_count'].values\n        \n        # Count edits at start, middle, and end\n        revisions_start = 0\n        revisions_middle = 0\n        revisions_end = 0\n        \n        for i, (pos, act, wc) in enumerate(zip(cursor_positions, activities, word_counts)):\n            if act in ['Remove/Cut', 'Replace'] and wc > 0:\n                relative_pos = pos / (wc + 1)\n                if relative_pos < 0.33:\n                    revisions_start += 1\n                elif relative_pos < 0.67:\n                    revisions_middle += 1\n                else:\n                    revisions_end += 1\n        \n        # Look for write-then-edit cycles\n        review_cycles = 0\n        in_writing = False\n        for act in activities:\n            if act == 'Input':\n                in_writing = True\n            elif act in ['Remove/Cut', 'Replace'] and in_writing:\n                review_cycles += 1\n                in_writing = False\n        \n        # How often they go backwards to edit\n        backward_movements = sum(1 for i in range(len(cursor_positions)-1) \n                                if cursor_positions[i+1] < cursor_positions[i])\n        \n        total_revisions = revisions_start + revisions_middle + revisions_end\n        \n        features.append({\n            'id': id_val,\n            'revisions_at_start': revisions_start,\n            'revisions_at_middle': revisions_middle,\n            'revisions_at_end': revisions_end,\n            'total_revisions': total_revisions,\n            'review_cycles': review_cycles,\n            'backward_movements': backward_movements,\n            'early_revision_ratio': revisions_start / (total_revisions + 1),\n            'end_revision_ratio': revisions_end / (total_revisions + 1),\n            'revision_density': total_revisions / (len(id_df) + 1),\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef cursor_movement_features(df):\n    \"\"\"Features about how the cursor moves around\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    df['cursor_jump'] = df.groupby('id')['cursor_position'].diff().abs()\n    \n    features = df.groupby('id').agg(\n        avg_cursor_jump=('cursor_jump', 'mean'),\n        max_cursor_jump=('cursor_jump', 'max'),\n        total_cursor_movement=('cursor_jump', 'sum'),\n        small_cursor_jumps=('cursor_jump', lambda x: (x <= 5).sum()),\n        medium_cursor_jumps=('cursor_jump', lambda x: ((x > 5) & (x <= 50)).sum()),\n        large_cursor_jumps=('cursor_jump', lambda x: (x > 50).sum()),\n        cursor_jump_std=('cursor_jump', 'std'),\n    ).reset_index()\n    \n    # Where is the cursor most of the time\n    cursor_at_end = df.groupby('id').apply(\n        lambda x: (x['cursor_position'] == x['word_count']).sum() / len(x)\n    ).rename('cursor_at_end_ratio')\n    \n    cursor_at_start = df.groupby('id').apply(\n        lambda x: (x['cursor_position'] == 0).sum() / len(x)\n    ).rename('cursor_at_start_ratio')\n    \n    features = features.merge(cursor_at_end, on='id', how='left')\n    features = features.merge(cursor_at_start, on='id', how='left')\n    \n    # Are they mostly writing forwards\n    features['forward_writing_tendency'] = features['cursor_at_end_ratio']\n    features['navigation_complexity'] = features['large_cursor_jumps'] / (features['total_cursor_movement'] + 1)\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:09.419770Z",
     "start_time": "2025-11-10T14:50:09.412353Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 2.6 Rolling Window & Distribution Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def rolling_features(df, window=10):\n    \"\"\"Look at trends over time using a sliding window\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        if len(id_df) < window:\n            features.append({\n                'id': id_val,\n                'action_time_rolling_mean': id_df['action_time'].mean(),\n                'action_time_rolling_std': id_df['action_time'].std(),\n                'word_count_rolling_trend': 0,\n                'action_time_trend': 0,\n                'action_time_acceleration': 0\n            })\n            continue\n        \n        # Calculate moving averages\n        action_rolling = id_df['action_time'].rolling(window=window, min_periods=1)\n        word_rolling = id_df['word_count'].rolling(window=window, min_periods=1)\n        \n        # Are things speeding up or slowing down\n        word_trend = (word_rolling.mean().iloc[-1] - word_rolling.mean().iloc[0]) if len(id_df) >= window else 0\n        action_trend = (action_rolling.mean().iloc[-1] - action_rolling.mean().iloc[0]) if len(id_df) >= window else 0\n        \n        features.append({\n            'id': id_val,\n            'action_time_rolling_mean': action_rolling.mean().mean(),\n            'action_time_rolling_std': action_rolling.std().mean(),\n            'word_count_rolling_trend': word_trend,\n            'action_time_trend': action_trend,\n            'action_time_acceleration': action_rolling.mean().diff().mean()\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef action_time_distribution_features(df):\n    \"\"\"Statistical properties of action times\"\"\"\n    features = df.groupby('id')['action_time'].agg([\n        ('action_time_q25', lambda x: x.quantile(0.25)),\n        ('action_time_q75', lambda x: x.quantile(0.75)),\n        ('action_time_iqr', lambda x: x.quantile(0.75) - x.quantile(0.25)),\n        ('action_time_skew', lambda x: x.skew()),\n        ('action_time_kurtosis', lambda x: x.kurtosis()),\n    ]).reset_index()\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:10.162807Z",
     "start_time": "2025-11-10T14:50:10.157263Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 2.7 Advanced Event Timing Features\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Main Feature Builder\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_all_features(df):\n    \"\"\"\n    Main function to build all features from log data\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        Input log data with columns: id, event_id, down_time, up_time, \n        action_time, activity, cursor_position, word_count\n    \n    Returns:\n    --------\n    DataFrame with all extracted features\n    \"\"\"\n    print(\"Building all features...\")\n    \n    # Base features\n    print(\"  - Base features\")\n    features = extract_features(df)\n    \n    # Pause features\n    print(\"  - Pause features\")\n    pause_feats = pause_features(df)\n    for feat in pause_feats:\n        features = features.merge(feat, on=\"id\", how=\"left\")\n    \n    # Burst features\n    print(\"  - Burst features\")\n    burst_feats = burst_features(df)\n    for feat in burst_feats:\n        features = features.merge(feat, on=\"id\", how=\"left\")\n    \n    # P-burst features\n    print(\"  - P-burst features\")\n    p_burst_feat = p_burst_features(df)\n    features = features.merge(p_burst_feat, on=\"id\", how=\"left\")\n    \n    # Activity sequence features\n    print(\"  - Activity sequence features\")\n    activity_seq_feat = activity_sequence_features(df)\n    features = features.merge(activity_seq_feat, on=\"id\", how=\"left\")\n    \n    # Text change features\n    print(\"  - Text change features\")\n    text_feat = text_change_features(df)\n    features = features.merge(text_feat, on=\"id\", how=\"left\")\n    \n    # Time-based features\n    print(\"  - Time-based features\")\n    time_feat = time_based_features(df)\n    features = features.merge(time_feat, on=\"id\", how=\"left\")\n    \n    # Keystroke velocity features\n    print(\"  - Keystroke velocity features\")\n    keystroke_feat = keystroke_velocity_features(df)\n    if not keystroke_feat.empty:\n        features = features.merge(keystroke_feat, on=\"id\", how=\"left\")\n    \n    # Revision pattern features\n    print(\"  - Revision pattern features\")\n    revision_feat = revision_pattern_features(df)\n    features = features.merge(revision_feat, on=\"id\", how=\"left\")\n    \n    # Cursor movement features\n    print(\"  - Cursor movement features\")\n    cursor_feat = cursor_movement_features(df)\n    features = features.merge(cursor_feat, on=\"id\", how=\"left\")\n    \n    # Rolling features\n    print(\"  - Rolling window features\")\n    rolling_feat = rolling_features(df, window=10)\n    features = features.merge(rolling_feat, on=\"id\", how=\"left\")\n    \n    # Action time distribution features\n    print(\"  - Action time distribution features\")\n    action_dist_feat = action_time_distribution_features(df)\n    features = features.merge(action_dist_feat, on=\"id\", how=\"left\")\n    \n    # Word count velocity features\n    print(\"  - Word count velocity features\")\n    word_vel_feat = word_count_velocity_features(df)\n    features = features.merge(word_vel_feat, on=\"id\", how=\"left\")\n    \n    # Activity timing features\n    print(\"  - Activity timing features\")\n    activity_time_feat = activity_timing_features(df)\n    features = features.merge(activity_time_feat, on=\"id\", how=\"left\")\n    \n    # Fill NaN and inf values\n    features = features.fillna(0)\n    features = features.replace([np.inf, -np.inf], 0)\n    \n    print(f\"\\nTotal features extracted: {features.shape[1] - 1}\")  # -1 for id column\n    print(f\"Total samples: {features.shape[0]}\")\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:05.132243Z",
     "start_time": "2025-11-10T14:51:05.126243Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Load Data & Extract Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load cleaned training logs\n\n\nlogs = df\nprint(f\"Loaded {len(logs)} rows\")\nprint(f\"Unique IDs: {logs['id'].nunique()}\")\nprint(f\"\\nColumns: {list(logs.columns)}\")\nlogs.head()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:05.742008Z",
     "start_time": "2025-11-10T14:51:05.734947Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Extract all behavioural features\nBehavioral_features_temp = build_all_features(logs)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:06.212726Z",
     "start_time": "2025-11-10T14:51:06.148141Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Inspect Results\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display first few rows\nprint(f\"Feature matrix shape: {Behavioral_features_temp.shape}\")\nprint(f\"\\nFeature names ({len(Behavioral_features_temp.columns)} total):\")\nprint(list(Behavioral_features_temp.columns))\nBehavioral_features_temp.head()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:07.166937Z",
     "start_time": "2025-11-10T14:51:07.157077Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Check for any issues\nprint(\"Missing values per column:\")\nprint(Behavioral_features_temp.isnull().sum().sum())\nprint(\"\\nInfinite values per column:\")\nprint(np.isinf(Behavioral_features_temp.select_dtypes(include=[np.number])).sum().sum())\nprint(\"\\nBasic statistics:\")\nBehavioral_features_temp.describe().T",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:07.752554Z",
     "start_time": "2025-11-10T14:51:07.685770Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Save Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "Behavioural_features = Behavioral_features_temp\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:09.642316Z",
     "start_time": "2025-11-10T14:51:09.639314Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis notebook extracts **comprehensive behavioural features** from keystroke logging data. The features capture:\n\n### Feature Categories (150+ features total):\n\n1. **Base Features**: Event counts, total time, typing speed, activity ratios\n2. **Pause Features**: Gaps between keystrokes at different thresholds (2s, 5s, 10s)\n3. **Burst Features**: When they're typing continuously and how fluently\n4. **Activity Sequence**: How activities transition from one to another, streaks, variety\n5. **Text Change**: How fast they produce/remove text, editing efficiency\n6. **Temporal Patterns**: What they do in early/middle/late stages\n7. **Keystroke Velocity**: Typing speed variations, rhythm, consistency\n8. **Word Count Velocity**: How the word count changes over time\n9. **Activity Timing**: How much time on each type of activity\n10. **Revision Patterns**: Where they edit, review cycles, going backwards\n11. **Cursor Movement**: How they navigate around, jump distances\n12. **Rolling Window**: Trends and changes in typing behaviour\n13. **Distribution Features**: Statistical properties (skew, kurtosis, IQR)\n\n### Output:\n\n- `data/train_behaviour_features.csv` - One row per essay ID with all behavioural features\n\n### Next Steps:\n\n- Combine with text features from `FeatureExtraction_Essay.ipynb`\n- Merge with TF-IDF/SVD features from `tfidf/tfidf.ipynb`\n- Build predictive models using these features\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "------------\n\n### Essay Text Feature",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# reuse code from text_process\nimport sys\nsys.path.append('..')\n\nimport numpy as np\n\ndef Text_Feature_Extraction(extracted_text):\n    features = extracted_text.drop('text',axis=1)\n    texts = extracted_text['text']\n    processor = TextProcessor()\n    for i in range(0,texts.shape[0]):\n        words = processor.split_to_word(texts[i])\n        sentences = processor.split_to_sentence(texts[i])\n        word_lengths = [len(w) for w in words]\n        sent_lengths = [len(processor.split_to_word(s)) for s in sentences]\n        \n        features.loc[i,'word_count'] = len(word_lengths)\n        if len(word_lengths) > 0:\n            features.loc[i,'word_length_mean'] = sum(word_lengths)/len(word_lengths)\n            features.loc[i,'word_length_std'] = pd.Series(word_lengths).std()\n        else:\n            features.loc[i,'word_length_mean'] = 0\n            features.loc[i,'word_length_std'] = 0\n        \n        if len(sent_lengths) > 0:\n            features.loc[i,'sent_length_mean'] = sum(sent_lengths)/len(sent_lengths)\n            features.loc[i,'sent_length_std'] = pd.Series(sent_lengths).std()\n        else:\n            features.loc[i,'sent_length_mean'] = 0\n            features.loc[i,'sent_length_std'] = 0\n    return features\n\nText_Essay_Features = Text_Feature_Extraction(extracted_text)\nTrain_Text_Essay_Features = Text_Feature_Extraction(extracted_text_train)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:43.512767Z",
     "start_time": "2025-11-10T14:51:41.840677Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "Text_Essay_Features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:49.613375Z",
     "start_time": "2025-11-10T14:51:49.607249Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "Train_Text_Essay_Features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:55.006997Z",
     "start_time": "2025-11-10T14:51:54.998996Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.736Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---------\n### TF-IDF Feature Extraction\n\n### Works taken from `texts_tfidf.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pickle\nfrom sklearn.decomposition import TruncatedSVD\n\n\n\ntexts_train = extracted_text_train[['id', 'text']]\ntexts_test = extracted_text[['id', 'text']]\n\n\n# refactor version, completely separate train and test\n\n\n# only fit on train\nvectorizer = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 5),\n    max_features=30000,\n    dtype=np.float32,\n)\nX_train_tfidf = vectorizer.fit_transform(texts_train['text'])\n\nn_features = X_train_tfidf.shape[1]\nsvdsize = min(64, max(1, n_features - 1))\n\nsvd = TruncatedSVD(\n    n_components=svdsize,\n    random_state=42,\n    n_iter=7\n)\nX_train_svd = svd.fit_transform(X_train_tfidf)\n\n\n# train_svd_df = pd.DataFrame(\n#     X_train_svd,\n#     columns=[f'{i:02d}' for i in range(svdsize)]\n# )\n# train_svd_df.insert(0, 'id', texts_train['id'].values)\n# train_svd_df.to_csv(\"/data/train_tfidf_svd.csv\", index=False)\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:21.120054Z",
     "start_time": "2025-11-10T15:36:16.632442Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X_test_tfidf = vectorizer.transform(texts_test['text'])\nX_test_svd = svd.transform(X_test_tfidf)\n\n\nsvdsize = X_test_svd.shape[1]\ntest_svd_df = pd.DataFrame(\n    X_test_svd,\n    columns=[f'{i:02d}' for i in range(svdsize)]\n)\ntest_svd_df.insert(0, 'id', texts_test['id'].values)\nTFIDF_Features = test_svd_df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:21.756745Z",
     "start_time": "2025-11-10T15:36:21.752382Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Works taken from `operation_tfidf.ipynb` ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === TRAIN ===\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n\nactivity_df = train_df[['id', 'activity']]\n\nprint(train_df.head(3))\n\n\ndef rebuild_text(grp):\n    buf = []\n    for op in grp['activity']:\n        buf.append(op[0])\n    return \"\".join(buf)\n\noperations = (\n    activity_df.groupby('id')\n               .apply(rebuild_text)\n               .reset_index(name='operation')\n)\n\nprint(operations.head(3))\n\n# 3) TF-IDF fit on train\nvectorizer = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 5),\n    max_features=30000,\n    dtype=np.float32,\n)\nX_tfidf = vectorizer.fit_transform(operations['operation'])\n\n\n# 4) SVD fit on train\nn_features = X_tfidf.shape[1]\nsvdsize = min(64, n_features - 1)\n\nsvd = TruncatedSVD(\n    n_components=svdsize,\n    random_state=42,\n    n_iter=7\n)\nX_svd = svd.fit_transform(X_tfidf)\n\nsvd_df = pd.DataFrame(\n    X_svd,\n    columns=[f'{i:02d}' for i in range(svdsize)]\n)\nsvd_df.insert(0, 'id', operations['id'].values)\n\nprint(svd_df.head())\ntrain_svd_df_operation = svd_df\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:30.735456Z",
     "start_time": "2025-11-10T15:36:23.251444Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# rebuild raw operation texts\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# === TEST ===\nimport pandas as pd\nimport numpy as np\nimport pickle\n\n\n\ntest_df_activity = df[['id', 'activity']]\n\n\n\ntest_operations = (\n    test_df_activity.groupby('id')\n           .apply(rebuild_text)\n           .reset_index(name='operation')\n)\n\n# 4) only transform, do not fit on tests\nX_test_tfidf = vectorizer.transform(test_operations['operation'])\nX_test_svd   = svd.transform(X_test_tfidf)\n\nsvdsize = X_test_svd.shape[1]\ntest_svd_df = pd.DataFrame(\n    X_test_svd,\n    columns=[f'{i:02d}' for i in range(svdsize)]\n)\ntest_svd_df.insert(0, 'id', test_operations['id'].values)\nTFIDF_Features_Operations = test_svd_df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:30.742619Z",
     "start_time": "2025-11-10T15:36:30.736461Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Concat Data and send to model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def merge_preprocessed_data():\n    dataset_behaviour = Behavioural_features\n    dataset_text = Text_Essay_Features\n    dataset_tfidf_text = TFIDF_Features\n    dataset_tfidf_operation = TFIDF_Features_Operations\n\n    # merge on 'id'\n    merged = dataset_behaviour.merge(dataset_text, on='id', how='inner')\n\n    # rename column name\n    tfidf_text_renamed = dataset_tfidf_text.rename(\n        columns={col: f'tfidf_text_{col}' if col != 'id' else col\n                 for col in dataset_tfidf_text.columns}\n    )\n    tfidf_operation_renamed = dataset_tfidf_operation.rename(\n        columns={col: f'tfidf_operation_{col}' if col != 'id' else col\n                 for col in dataset_tfidf_operation.columns}\n    )\n\n    merged = merged.merge(tfidf_text_renamed, on='id', how='inner')\n    merged = merged.merge(tfidf_operation_renamed, on='id', how='inner')\n\n\n    return merged\n\n\n\n\nmerged_df = merge_preprocessed_data()\n    ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:33.551416Z",
     "start_time": "2025-11-10T15:36:33.543416Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "merged_df",
   "metadata": {
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Extra Feature Extraction\nwork from `model_enhanced.py` `linking_38features.py`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from typing import Dict, List, Tuple, Optional\nfrom sklearn.linear_model import (\n    Ridge,\n    ElasticNet,\n    BayesianRidge,\n    HuberRegressor,\n    PoissonRegressor,\n    PassiveAggressiveRegressor,\n)\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom scipy.optimize import minimize\nfrom tqdm import tqdm\nimport time\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport pickle\nimport catboost as cb\nimport xgboost as xgb\nimport lightgbm as lgb\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n\n\nclass EnhancedEnsembleModel:\n    def __init__(\n        self,\n        n_splits: int = 5,\n        random_state: int = 42,\n    ):\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.kf = KFold(n_splits=n_splits, shuffle=True,\n                        random_state=random_state)\n\n        self.lgb_models: List[lgb.LGBMRegressor] = []\n        self.xgb_models: List[xgb.XGBRegressor] = []\n        self.catboost_models: List[cb.CatBoostRegressor] = []\n        self.stacking_model: Optional[Ridge] = None\n\n        # Advanced components (integrated by default)\n        self.linear_models = self._init_linear_models()\n        self.linear_scaler = MinMaxScaler()\n        self.linear_optimal_weights = None\n\n        # Classifier meta-feature mappings\n        self.score_to_class = {\n            4.0: 9, 3.5: 8, 4.5: 7, 3.0: 6, 2.5: 5,\n            5.0: 4, 5.5: 1, 2.0: 3, 1.5: 2, 6.0: 1,\n            1.0: 0, 0.5: 0,\n        }\n        self.class_to_score = {\n            0: 1.0, 1: 6.0, 2: 1.5, 3: 2.0, 4: 5.0,\n            5: 2.5, 6: 3.0, 7: 4.5, 8: 3.5, 9: 4.0,\n        }\n        self.n_classes = 10\n        self.final_weights = None\n\n        self.lgb_params = {\n            \"objective\": \"regression_l1\",\n            \"metric\": \"rmse\",\n            \"n_estimators\": 12000,\n            \"verbosity\": -1,\n            \"random_state\": random_state,\n            \"reg_alpha\": 0.007678095440286993,\n            \"reg_lambda\": 0.34230534302168353,\n            \"colsample_bytree\": 0.627061253588415,\n            \"subsample\": 0.854942238828458,\n            \"learning_rate\": 0.038697981947473245,\n            \"num_leaves\": 22,\n            \"max_depth\": 37,\n            \"min_child_samples\": 18,\n            \"n_jobs\": 4,\n        }\n\n        self.xgb_params = {\n            \"objective\": \"reg:squarederror\",\n            \"eval_metric\": \"rmse\",\n            \"n_estimators\": 5000,\n            \"random_state\": random_state,\n            \"learning_rate\": 0.03,\n            \"max_depth\": 8,\n            \"min_child_weight\": 3,\n            \"subsample\": 0.8,\n            \"colsample_bytree\": 0.8,\n            \"reg_alpha\": 0.1,\n            \"reg_lambda\": 1.0,\n            \"n_jobs\": 1,\n            \"tree_method\": \"hist\",\n        }\n\n        self.catboost_params = {\n            \"iterations\": 5000,\n            \"learning_rate\": 0.03,\n            \"depth\": 8,\n            \"l2_leaf_reg\": 3,\n            \"random_state\": random_state,\n            \"verbose\": False,\n            \"thread_count\": 4,\n            \"loss_function\": \"RMSE\",\n        }\n\n    def _init_linear_models(self):\n        return [\n            (\"LinearSVR\", LinearSVR(\n                C=0.9, loss=\"squared_epsilon_insensitive\", max_iter=2000)),\n            (\"ElasticNet\", ElasticNet(alpha=0.001, l1_ratio=0.5,\n             random_state=self.random_state, selection=\"cyclic\")),\n            (\"Ridge\", Ridge(alpha=10)),\n            (\"PassiveAggressive\", PassiveAggressiveRegressor(\n                C=0.001, loss=\"squared_epsilon_insensitive\")),\n            (\"Huber\", HuberRegressor(epsilon=1.25, alpha=20)),\n            (\"Poisson\", PoissonRegressor(alpha=0.01)),\n            (\"BayesianRidge\", BayesianRidge()),\n        ]\n\n    def _generate_classifier_meta_features(self, X, y, X_test=None, pca_components=100):\n\n        y_class = pd.Series(y).map(self.score_to_class).values\n\n        print(\"Training MultinomialNB...\")\n        nb_oof, nb_test = self._train_classifier_meta(\n            X, y_class, X_test, pca_components, \"nb\"\n        )\n\n        print(\"Training MLPClassifier...\")\n        mlp_oof, mlp_test = self._train_classifier_meta(\n            X, y_class, X_test, pca_components, \"mlp\"\n        )\n\n        return {\n            \"nb_oof\": nb_oof, \"nb_test\": nb_test,\n            \"mlp_oof\": mlp_oof, \"mlp_test\": mlp_test,\n        }\n\n    def _train_classifier_meta(self, X, y_class, X_test, pca_components, model_type):\n        # Apply PCA and square\n        pca = PCA(n_components=pca_components, random_state=self.random_state)\n        if X_test is not None:\n            combined = pd.concat([X, X_test]) if isinstance(\n                X, pd.DataFrame) else np.vstack([X, X_test])\n            pca.fit(combined.fillna(0) if isinstance(\n                combined, pd.DataFrame) else combined)\n            X_pca = pca.transform(X.fillna(0) if isinstance(\n                X, pd.DataFrame) else X) ** 2\n            X_test_pca = pca.transform(X_test.fillna(0) if isinstance(\n                X_test, pd.DataFrame) else X_test) ** 2\n        else:\n            X_pca = pca.fit_transform(\n                X.fillna(0) if isinstance(X, pd.DataFrame) else X) ** 2\n            X_test_pca = None\n\n        # Cross-validation\n        oof_prob = np.zeros((len(X), self.n_classes))\n        test_prob = np.zeros((len(X_test), self.n_classes)\n                             ) if X_test is not None else None\n\n        skf = StratifiedKFold(n_splits=self.n_splits,\n                              shuffle=True, random_state=self.random_state)\n\n        for train_idx, val_idx in skf.split(X_pca, y_class):\n            X_train, X_val = X_pca[train_idx], X_pca[val_idx]\n            y_train, y_val = y_class[train_idx], y_class[val_idx]\n\n            if model_type == \"nb\":\n                model = MultinomialNB(alpha=1.0)\n            else:\n                model = MLPClassifier(\n                    random_state=self.random_state, max_iter=300)\n\n            model.fit(X_train, y_train)\n            oof_prob[val_idx] = model.predict_proba(X_val)\n\n            if test_prob is not None:\n                test_prob += model.predict_proba(X_test_pca) / self.n_splits\n\n        return oof_prob, test_prob\n\n    def _compute_weighted_score(self, probabilities):\n        weighted_sum = np.zeros(len(probabilities))\n        for i in range(self.n_classes):\n            weighted_sum += probabilities[:, i] * self.class_to_score[i]\n        return weighted_sum\n\n    def _train_linear_ensemble(self, X, y, X_test=None):\n        # Scale features\n        if X_test is not None:\n            combined = pd.concat([X, X_test])\n            self.linear_scaler.fit(combined)\n            X_scaled = pd.DataFrame(\n                self.linear_scaler.transform(X), columns=X.columns)\n            X_test_scaled = pd.DataFrame(\n                self.linear_scaler.transform(X_test), columns=X.columns)\n        else:\n            X_scaled = pd.DataFrame(\n                self.linear_scaler.fit_transform(X), columns=X.columns)\n            X_test_scaled = None\n\n        # Train each model\n        oof_predictions = np.zeros((len(X), len(self.linear_models)))\n        test_predictions = np.zeros(\n            (len(X_test), len(self.linear_models))) if X_test is not None else None\n\n        skf = StratifiedKFold(n_splits=self.n_splits,\n                              shuffle=True, random_state=self.random_state)\n\n        for i, (name, model_template) in enumerate(tqdm(self.linear_models, desc=\"Training linear models\")):\n            oof_preds = np.zeros(len(X))\n            test_preds = np.zeros(len(X_test)) if X_test is not None else None\n\n            for train_idx, val_idx in skf.split(X_scaled, y.astype(str)):\n                from sklearn.base import clone\n                model = clone(model_template)\n\n                X_train = X_scaled.iloc[train_idx].fillna(0)\n                X_val = X_scaled.iloc[val_idx].fillna(0)\n                y_train, y_val = y[train_idx], y[val_idx]\n\n                model.fit(X_train, y_train)\n                oof_preds[val_idx] = model.predict(X_val)\n\n                if test_preds is not None:\n                    test_preds += model.predict(X_test_scaled.fillna(0)\n                                                ) / self.n_splits\n\n            oof_preds = np.clip(oof_preds, 0, 6)\n            oof_predictions[:, i] = oof_preds\n\n            if test_preds is not None:\n                test_preds = np.clip(test_preds, 0, 6)\n                test_predictions[:, i] = test_preds\n\n            rmse = np.sqrt(mean_squared_error(y, oof_preds))\n            print(f\"{name:20s} CV RMSE: {rmse:.5f}\")\n\n        self.linear_optimal_weights = self._optimize_weights(\n            oof_predictions, y)\n\n        ensemble_oof = np.dot(oof_predictions, self.linear_optimal_weights)\n        ensemble_rmse = np.sqrt(mean_squared_error(y, ensemble_oof))\n        print(f\"Linear Ensemble CV RMSE: {ensemble_rmse:.5f}\")\n\n        if test_predictions is not None:\n            ensemble_test = np.dot(\n                test_predictions, self.linear_optimal_weights)\n            return ensemble_oof, ensemble_test\n\n        return ensemble_oof, None\n\n    def _optimize_weights(self, predictions, y_true):\n        def weighted_rmse(weights, preds, y):\n            ensemble_pred = np.dot(preds, weights)\n            return np.sqrt(mean_squared_error(y, ensemble_pred))\n\n        n_models = predictions.shape[1]\n        initial_weights = np.ones(n_models) / n_models\n        constraints = {\"type\": \"eq\", \"fun\": lambda w: sum(w) - 1}\n        bounds = [(0, 1)] * n_models\n\n        opt_result = minimize(\n            weighted_rmse,\n            initial_weights,\n            args=(predictions, y_true),\n            method=\"SLSQP\",\n            bounds=bounds,\n            constraints=constraints,\n        )\n\n        return opt_result.x\n\n    def train_single_model(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray,\n        model_type: str,\n    ) -> Tuple:\n        if model_type == \"lgb\":\n            model = lgb.LGBMRegressor(**self.lgb_params)\n            X_train_np = X_train if isinstance(\n                X_train, np.ndarray) else X_train.values\n            X_val_np = X_val if isinstance(X_val, np.ndarray) else X_val.values\n            model.fit(\n                X_train_np,\n                y_train,\n                eval_set=[(X_val_np, y_val)],\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n\n        elif model_type == \"xgb\":\n            model = xgb.XGBRegressor(\n                **self.xgb_params, early_stopping_rounds=200)\n            model.fit(X_train, y_train, eval_set=[\n                      (X_val, y_val)], verbose=False)\n\n        elif model_type == \"catboost\":\n            model = cb.CatBoostRegressor(**self.catboost_params)\n            model.fit(\n                X_train,\n                y_train,\n                eval_set=[(X_val, y_val)],\n                early_stopping_rounds=200,\n                verbose=False,\n            )\n\n        X_val_pred = X_val if isinstance(X_val, np.ndarray) else X_val.values\n        val_preds = model.predict(X_val_pred)\n        val_score = np.sqrt(mean_squared_error(y_val, val_preds))\n\n        return model, val_preds, val_score\n\n    def fit(self, X: pd.DataFrame, y: np.ndarray, X_test: pd.DataFrame = None) -> Tuple[Dict[str, float], np.ndarray, np.ndarray]:\n        print(\"=\" * 70)\n        print(\"ADVANCED ENHANCED ENSEMBLE MODEL\")\n        print(\"=\" * 70)\n\n        # Step 1: Generate classifier meta-features\n        print(\"\\n\" + \"=\" * 70)\n        print(\"STEP 1: Generating Classifier Meta-Features (NB + MLP)\")\n        print(\"=\" * 70)\n\n        meta_features = self._generate_classifier_meta_features(X, y, X_test)\n\n        # Add meta-features to original features\n        X_enhanced = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n        X_test_enhanced = X_test.copy() if X_test is not None and isinstance(\n            X_test, pd.DataFrame) else (pd.DataFrame(X_test) if X_test is not None else None)\n\n        # Add NB features\n        for i in range(meta_features[\"nb_oof\"].shape[1]):\n            X_enhanced[f\"nb_prob_{i}\"] = meta_features[\"nb_oof\"][:, i]\n            if X_test_enhanced is not None:\n                X_test_enhanced[f\"nb_prob_{i}\"] = meta_features[\"nb_test\"][:, i]\n        X_enhanced[\"nb_weighted\"] = self._compute_weighted_score(\n            meta_features[\"nb_oof\"])\n        if X_test_enhanced is not None:\n            X_test_enhanced[\"nb_weighted\"] = self._compute_weighted_score(\n                meta_features[\"nb_test\"])\n\n        # Add MLP features\n        for i in range(meta_features[\"mlp_oof\"].shape[1]):\n            X_enhanced[f\"mlp_prob_{i}\"] = meta_features[\"mlp_oof\"][:, i]\n            if X_test_enhanced is not None:\n                X_test_enhanced[f\"mlp_prob_{i}\"] = meta_features[\"mlp_test\"][:, i]\n        X_enhanced[\"mlp_weighted\"] = self._compute_weighted_score(\n            meta_features[\"mlp_oof\"])\n        if X_test_enhanced is not None:\n            X_test_enhanced[\"mlp_weighted\"] = self._compute_weighted_score(\n                meta_features[\"mlp_test\"])\n\n        print(f\"\\nEnhanced feature shape: {X_enhanced.shape}\")\n\n        # Step 2: Train base ensemble (LGB + XGB + CatBoost) on enhanced features\n        print(\"\\n\" + \"=\" * 70)\n        print(\"STEP 2: Training Base Ensemble (LGB + XGB + CatBoost)\")\n        print(\"=\" * 70)\n\n        scores, ensemble_oof = self._train_base_ensemble(X_enhanced, y)\n        ensemble_test = self._predict_base_ensemble(\n            X_test_enhanced) if X_test_enhanced is not None else None\n\n        # Step 3: Train linear model ensemble\n        print(\"\\n\" + \"=\" * 70)\n        print(\"STEP 3: Training Linear Model Ensemble\")\n        print(\"=\" * 70)\n\n        linear_oof, linear_test = self._train_linear_ensemble(\n            X_enhanced, y, X_test_enhanced)\n\n        # Step 4: Optimize final ensemble weights\n        print(\"\\n\" + \"=\" * 70)\n        print(\"STEP 4: Optimizing Final Ensemble Weights\")\n        print(\"=\" * 70)\n\n        combined_preds = np.column_stack([ensemble_oof, linear_oof])\n        self.final_weights = self._optimize_weights(combined_preds, y)\n\n        final_oof = ensemble_oof * \\\n            self.final_weights[0] + linear_oof * self.final_weights[1]\n        final_rmse = np.sqrt(mean_squared_error(y, final_oof))\n\n        print(\n            f\"\\nFinal weights: Base Ensemble={self.final_weights[0]:.4f}, Linear={self.final_weights[1]:.4f}\")\n        print(f\"Final CV RMSE: {final_rmse:.5f}\")\n        print(\"=\" * 70)\n\n        # Compute final test predictions\n        final_test = None\n        if ensemble_test is not None and linear_test is not None:\n            final_test = ensemble_test * \\\n                self.final_weights[0] + linear_test * self.final_weights[1]\n\n        scores[\"advanced_ensemble_oof\"] = final_rmse\n\n        return scores, final_oof, final_test\n\n    def _train_base_ensemble(self, X: pd.DataFrame, y: np.ndarray) -> Tuple[Dict[str, float], np.ndarray]:\n        X_np = X.values if isinstance(X, pd.DataFrame) else X\n\n        oof_lgb = np.zeros(len(X))\n        oof_xgb = np.zeros(len(X))\n        oof_catboost = np.zeros(len(X))\n\n        lgb_scores = []\n        xgb_scores = []\n        catboost_scores = []\n\n        for fold, (train_idx, val_idx) in enumerate(self.kf.split(X_np, y)):\n            print(f\"\\nFold {fold + 1}/{self.n_splits}\")\n\n            X_train, X_val = X_np[train_idx], X_np[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n\n            start_time = time.time()\n            lgb_model, lgb_val_preds, lgb_score = self.train_single_model(\n                X_train, y_train, X_val, y_val, \"lgb\"\n            )\n            self.lgb_models.append(lgb_model)\n            oof_lgb[val_idx] = lgb_val_preds\n            lgb_scores.append(lgb_score)\n\n            print(\n                f\"  LightGBM - Val RMSE: {lgb_score:.5f} ({time.time() - start_time:.1f}s)\"\n            )\n\n            start_time = time.time()\n            xgb_model, xgb_val_preds, xgb_score = self.train_single_model(\n                X_train, y_train, X_val, y_val, \"xgb\"\n            )\n            self.xgb_models.append(xgb_model)\n            oof_xgb[val_idx] = xgb_val_preds\n            xgb_scores.append(xgb_score)\n\n            print(\n                f\"  XGBoost   - Val RMSE: {xgb_score:.5f} ({time.time() - start_time:.1f}s)\"\n            )\n\n            start_time = time.time()\n            cb_model, cb_val_preds, cb_score = self.train_single_model(\n                X_train, y_train, X_val, y_val, \"catboost\"\n            )\n            self.catboost_models.append(cb_model)\n            oof_catboost[val_idx] = cb_val_preds\n            catboost_scores.append(cb_score)\n\n            print(\n                f\"  CatBoost  - Val RMSE: {cb_score:.5f} ({time.time() - start_time:.1f}s)\"\n            )\n\n        lgb_oof_score = np.sqrt(mean_squared_error(y, oof_lgb))\n        xgb_oof_score = np.sqrt(mean_squared_error(y, oof_xgb))\n        catboost_oof_score = np.sqrt(mean_squared_error(y, oof_catboost))\n\n        print(\"\\n\" + \"=\" * 50)\n        print(\"Out-of-Fold Results:\")\n        print(f\"  LightGBM:  {lgb_oof_score:.5f}\")\n        print(f\"  XGBoost:   {xgb_oof_score:.5f}\")\n        print(f\"  CatBoost:  {catboost_oof_score:.5f}\")\n\n        stacking_features = np.column_stack([oof_lgb, oof_xgb, oof_catboost])\n        self.stacking_model = Ridge(alpha=1.0, random_state=self.random_state)\n        self.stacking_model.fit(stacking_features, y)\n\n        ensemble_preds = self.stacking_model.predict(stacking_features)\n        ensemble_score = np.sqrt(mean_squared_error(y, ensemble_preds))\n\n        print(f\"  Stacked:   {ensemble_score:.5f}\")\n        print(\n            f\"\\nStacking weights: LGB={self.stacking_model.coef_[0]:.3f}, \"\n            f\"XGB={self.stacking_model.coef_[1]:.3f}, \"\n            f\"CB={self.stacking_model.coef_[2]:.3f}\"\n        )\n\n        return {\n            \"lgb_oof\": lgb_oof_score,\n            \"xgb_oof\": xgb_oof_score,\n            \"catboost_oof\": catboost_oof_score,\n            \"ensemble_oof\": ensemble_score,\n        }, ensemble_preds\n\n    def _predict_base_ensemble(self, X: pd.DataFrame) -> np.ndarray:\n        X_np = X.values if isinstance(X, pd.DataFrame) else X\n\n        lgb_preds = np.zeros(len(X))\n        xgb_preds = np.zeros(len(X))\n        catboost_preds = np.zeros(len(X))\n\n        for lgb_model in self.lgb_models:\n            lgb_preds += lgb_model.predict(X_np) / self.n_splits\n\n        for xgb_model in self.xgb_models:\n            xgb_preds += xgb_model.predict(X_np) / self.n_splits\n\n        for cb_model in self.catboost_models:\n            catboost_preds += cb_model.predict(X_np) / self.n_splits\n\n        stacking_features = np.column_stack(\n            [lgb_preds, xgb_preds, catboost_preds])\n        final_preds = self.stacking_model.predict(stacking_features)\n\n        return final_preds\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict using the full advanced ensemble\n\n        Args:\n            X: Input features\n\n        Returns:\n            Final predictions\n        \"\"\"\n        # Generate meta-features for prediction\n        X_enhanced = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n\n        # For prediction, we need to generate meta-features but we don't have y\n        # So we skip meta-features in predict and only use base predictions\n        # This is a limitation - ideally meta-features should be pre-computed\n\n        # Use base ensemble prediction\n        base_pred = self._predict_base_ensemble(X_enhanced)\n\n        # If we don't have final weights trained, return base prediction\n        if self.final_weights is None:\n            return base_pred\n\n        # Otherwise this would need the full pipeline which requires y for meta-features\n        # For now, return base prediction\n        return base_pred\n\n    def save(self, path: str, use_pickle: bool = True):\n        os.makedirs(path, exist_ok=True)\n\n        if use_pickle:\n            with open(os.path.join(path, \"lgb_models.pkl\"), \"wb\") as f:\n                pickle.dump(self.lgb_models, f)\n            with open(os.path.join(path, \"xgb_models.pkl\"), \"wb\") as f:\n                pickle.dump(self.xgb_models, f)\n            with open(os.path.join(path, \"catboost_models.pkl\"), \"wb\") as f:\n                pickle.dump(self.catboost_models, f)\n            with open(os.path.join(path, \"stacking_model.pkl\"), \"wb\") as f:\n                pickle.dump(self.stacking_model, f)\n            config = {\n                \"n_splits\": self.n_splits,\n                \"random_state\": self.random_state,\n            }\n            with open(os.path.join(path, \"config.pkl\"), \"wb\") as f:\n                pickle.dump(config, f)\n        else:\n            joblib.dump(self.lgb_models, os.path.join(path, \"lgb_models.pkl\"))\n            joblib.dump(self.xgb_models, os.path.join(path, \"xgb_models.pkl\"))\n            joblib.dump(self.catboost_models, os.path.join(\n                path, \"catboost_models.pkl\"))\n            joblib.dump(self.stacking_model, os.path.join(\n                path, \"stacking_model.pkl\"))\n            config = {\n                \"n_splits\": self.n_splits,\n                \"random_state\": self.random_state,\n            }\n            joblib.dump(config, os.path.join(path, \"config.pkl\"))\n\n        print(\n            f\"Model saved to {path} using {'pickle' if use_pickle else 'joblib'}\")\n\n    def load(self, path: str, use_pickle: bool = True):\n        if use_pickle:\n            with open(os.path.join(path, \"lgb_models.pkl\"), \"rb\") as f:\n                self.lgb_models = pickle.load(f)\n            with open(os.path.join(path, \"xgb_models.pkl\"), \"rb\") as f:\n                self.xgb_models = pickle.load(f)\n            with open(os.path.join(path, \"catboost_models.pkl\"), \"rb\") as f:\n                self.catboost_models = pickle.load(f)\n            with open(os.path.join(path, \"stacking_model.pkl\"), \"rb\") as f:\n                self.stacking_model = pickle.load(f)\n            with open(os.path.join(path, \"config.pkl\"), \"rb\") as f:\n                config = pickle.load(f)\n        else:\n            self.lgb_models = joblib.load(os.path.join(path, \"lgb_models.pkl\"))\n            self.xgb_models = joblib.load(os.path.join(path, \"xgb_models.pkl\"))\n            self.catboost_models = joblib.load(\n                os.path.join(path, \"catboost_models.pkl\"))\n            self.stacking_model = joblib.load(\n                os.path.join(path, \"stacking_model.pkl\"))\n            config = joblib.load(os.path.join(path, \"config.pkl\"))\n\n        self.n_splits = config[\"n_splits\"]\n        self.random_state = config[\"random_state\"]\n\n        print(\n            f\"Model loaded from {path} using {'pickle' if use_pickle else 'joblib'}\")\n\n    def load_lgbm_only(self, filepath: str):\n        with open(filepath, \"rb\") as f:\n            self.lgb_models = pickle.load(f)\n\n        print(f\"LightGBM models loaded from {filepath}\")\n\n\ndef train_enhanced_ensemble(\n    data_path: str = \"../data/\",\n    save_path: str = \"../data/\",\n    n_splits: int = 5,\n    random_state: int = 42,\n) -> pd.DataFrame:\n    if not os.path.exists(data_path):\n        data_path = \"data/\"\n\n    train_file = f\"{data_path}train_preprocessed.csv\"\n    test_file = f\"{data_path}test_preprocessed.csv\"\n\n    df_train = pd.read_csv(train_file)\n    df_test = pd.read_csv(test_file)\n\n    df_train = df_train.drop(\"id\", axis=1)\n    test_ids = df_test[\"id\"]\n    df_test = df_test.drop(\"id\", axis=1)\n\n    X = df_train.iloc[:, :-1]\n    y = df_train.iloc[:, -1].values\n    X_test = df_test\n\n    print(f\"Training data shape: {X.shape}\")\n    print(f\"Test data shape: {X_test.shape}\")\n\n    model = EnhancedEnsembleModel(\n        n_splits=n_splits,\n        random_state=random_state,\n    )\n\n    scores, _, test_preds = model.fit(X, y, X_test)\n\n    model_save_path = f\"{save_path}model_enhanced_ensemble\"\n    model.save(model_save_path, use_pickle=True)\n\n    submission = pd.DataFrame({\"id\": test_ids, \"score\": test_preds})\n    submission_path = f\"{save_path}submission_enhanced_ensemble.csv\"\n    submission.to_csv(submission_path, index=False)\n\n    print(f\"\\nSubmission saved to {submission_path}\")\n    print(\"Submission preview:\")\n    print(submission.head())\n    print(\"\\nScore statistics:\")\n    print(submission[\"score\"].describe())\n\n    return submission\n",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# features from 38 th notebook\nimport pandas as pd\nimport numpy as np\nimport polars as pl\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, PCA\nfrom sklearn.manifold import TSNE\nfrom scipy.stats import skew, kurtosis\nfrom tqdm import tqdm\nimport warnings\nimport pickle\nimport os\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass CharacterNGramFeatureExtractor:\n    def __init__(self, ngram_range=(1, 4), analyzer=\"char_wb\", max_features=None):\n        self.ngram_range = ngram_range\n        self.analyzer = analyzer\n        self.max_features = max_features\n        self.vectorizer = CountVectorizer(\n            ngram_range=ngram_range, analyzer=analyzer, max_features=max_features\n        )\n\n    def fit_transform(self, train_essays, test_essays):\n        combined = pd.concat([train_essays[\"essay\"], test_essays[\"essay\"]])\n        self.vectorizer.fit(combined)\n\n        X_train = self.vectorizer.transform(train_essays[\"essay\"])\n        X_train_dense = X_train.todense()\n\n        train_features = pd.DataFrame()\n        for i in range(X_train_dense.shape[1]):\n            L = list(X_train_dense[:, i])\n            train_features[f\"char_ngram_{i}\"] = [int(x) for x in L]\n        train_features[\"id\"] = train_essays[\"id\"].values\n\n        X_test = self.vectorizer.transform(test_essays[\"essay\"])\n        X_test_dense = X_test.todense()\n\n        test_features = pd.DataFrame()\n        for i in range(X_test_dense.shape[1]):\n            L = list(X_test_dense[:, i])\n            test_features[f\"char_ngram_{i}\"] = [int(x) for x in L]\n        test_features[\"id\"] = test_essays[\"id\"].values\n\n        return train_features, test_features\n\n\nclass LDATopicFeatureExtractor:\n    def __init__(self, n_topics=6, max_iter=10, random_state=42):\n        self.n_topics = n_topics\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.models = {}\n        self.vectorizers = {}\n\n    def fit_transform(self, train_essays, test_essays):\n        train_topics = pd.DataFrame({\"id\": train_essays[\"id\"]})\n        test_topics = pd.DataFrame({\"id\": test_essays[\"id\"]})\n\n        # Combine for fitting\n        combined = pd.concat([train_essays, test_essays])\n\n        # 1 stopwords\n        train_topics, test_topics = self._fit_lda_variant(\n            combined,\n            train_essays,\n            test_essays,\n            train_topics,\n            test_topics,\n            \"word_topics\",\n            CountVectorizer(stop_words=\"english\"),\n        )\n\n        # 2 char wb\n        train_topics, test_topics = self._fit_lda_variant(\n            combined,\n            train_essays,\n            test_essays,\n            train_topics,\n            test_topics,\n            \"char_topics\",\n            CountVectorizer(analyzer=\"char_wb\"),\n        )\n\n        # 3. ngram = (5, 6)\n        train_topics, test_topics = self._fit_lda_variant(\n            combined,\n            train_essays,\n            test_essays,\n            train_topics,\n            test_topics,\n            \"char_ngram_topics\",\n            CountVectorizer(analyzer=\"char_wb\", ngram_range=(5, 6)),\n        )\n\n        return train_topics, test_topics\n\n    def _fit_lda_variant(\n        self,\n        combined_essays,\n        train_essays,\n        test_essays,\n        train_topics,\n        test_topics,\n        prefix,\n        vectorizer,\n    ):\n        vectorizer.fit(combined_essays[\"essay\"])\n        self.vectorizers[prefix] = vectorizer\n\n        train_words = pd.DataFrame(\n            vectorizer.transform(train_essays[\"essay\"]).toarray()\n        )\n        test_words = pd.DataFrame(vectorizer.transform(test_essays[\"essay\"]).toarray())\n\n        lda = LatentDirichletAllocation(\n            n_components=self.n_topics,\n            max_iter=self.max_iter,\n            random_state=self.random_state,\n            verbose=False,\n        )\n\n        combined_words = pd.concat([train_words, test_words])\n        lda.fit(combined_words)\n\n        self.models[prefix] = lda\n\n        topic_cols = [f\"{prefix}_{i}\" for i in range(self.n_topics)]\n        train_topics[topic_cols] = lda.transform(train_words)\n        test_topics[topic_cols] = lda.transform(test_words)\n\n        return train_topics, test_topics\n\n\nclass TSNEFeatureExtractor:\n    def __init__(\n        self, perplexities=[20, 50, 80], n_components=2, random_state=42, n_jobs=-1\n    ):\n        self.perplexities = perplexities\n        self.n_components = n_components\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n\n    def fit_transform(self, train_features, test_features):\n        train_tsne = pd.DataFrame()\n        test_tsne = pd.DataFrame()\n\n        combined = pd.concat([train_features, test_features])\n\n        print(\"tsne\")\n        for perplexity in self.perplexities:\n            tsne = TSNE(\n                n_components=self.n_components,\n                random_state=self.random_state,\n                perplexity=perplexity,\n                n_jobs=self.n_jobs,\n                verbose=False,\n            )\n\n            embeddings = tsne.fit_transform(combined.fillna(0))\n\n            train_size = len(train_features)\n            for i in range(self.n_components):\n                col_name = f\"tsne_p{perplexity}_{i}\"\n                train_tsne[col_name] = embeddings[:train_size, i]\n                test_tsne[col_name] = embeddings[train_size:, i]\n\n        return train_tsne, test_tsne\n\n\nclass PolarsFeatureExtractor:\n    def __init__(self):\n        self.num_cols = [\n            \"down_time\",\n            \"up_time\",\n            \"action_time\",\n            \"cursor_position\",\n            \"word_count\",\n        ]\n        self.activities = [\"Input\", \"Remove/Cut\", \"Nonproduction\", \"Replace\", \"Paste\"]\n        self.events = [\n            \"q\",\n            \"Space\",\n            \"Backspace\",\n            \"Shift\",\n            \"ArrowRight\",\n            \"Leftclick\",\n            \"ArrowLeft\",\n            \".\",\n            \",\",\n            \"ArrowDown\",\n            \"ArrowUp\",\n            \"Enter\",\n            \"CapsLock\",\n            \"'\",\n            \"Delete\",\n            \"Unidentified\",\n        ]\n        self.text_changes = [\n            \"q\",\n            \" \",\n            \".\",\n            \",\",\n            \"\\n\",\n            \"'\",\n            '\"',\n            \"-\",\n            \"?\",\n            \";\",\n            \"=\",\n            \"/\",\n            \"\\\\\",\n            \":\",\n            \"n\",\n        ]\n\n    def extract_features(self, df):\n        if isinstance(df, pd.DataFrame):\n            df = pl.from_pandas(df)\n\n        feats = self._count_by_values(df, \"activity\", self.activities)\n        feats = feats.join(\n            self._count_by_values(df, \"text_change\", self.text_changes),\n            on=\"id\",\n            how=\"left\",\n        )\n        feats = feats.join(\n            self._count_by_values(df, \"down_event\", self.events), on=\"id\", how=\"left\"\n        )\n        feats = feats.join(\n            self._count_by_values(df, \"up_event\", self.events), on=\"id\", how=\"left\"\n        )\n        feats = feats.join(self._input_word_stats(df), on=\"id\", how=\"left\")\n        feats = feats.join(self._numerical_stats(df), on=\"id\", how=\"left\")\n        feats = feats.join(self._categorical_stats(df), on=\"id\", how=\"left\")\n        feats = feats.join(self._idle_time_features(df), on=\"id\", how=\"left\")\n        feats = feats.join(self._p_burst_features(df), on=\"id\", how=\"left\")\n        feats = feats.join(self._r_burst_features(df), on=\"id\", how=\"left\")\n\n        return feats.to_pandas()\n\n    def _count_by_values(self, df, colname, values):\n        fts = df.select(pl.col(\"id\").unique(maintain_order=True))\n        for i, value in enumerate(values):\n            tmp_df = df.group_by(\"id\").agg(\n                pl.col(colname).is_in([value]).sum().alias(f\"{colname}_{i}_cnt\")\n            )\n            fts = fts.join(tmp_df, on=\"id\", how=\"left\")\n        return fts\n\n    def _input_word_stats(self, df):\n        temp = df.filter(\n            (~pl.col(\"text_change\").str.contains(\"=>\"))\n            & (pl.col(\"text_change\") != \"NoChange\")\n        )\n        temp = temp.group_by(\"id\").agg(\n            pl.col(\"text_change\").str.concat(\"\").str.extract_all(r\"q+\")\n        )\n        temp = temp.with_columns(\n            input_word_count=pl.col(\"text_change\").list.len(),\n            input_word_length_mean=pl.col(\"text_change\").map_elements(\n                lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0),\n                return_dtype=pl.Float64,\n            ),\n            input_word_length_max=pl.col(\"text_change\").map_elements(\n                lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0),\n                return_dtype=pl.Float64,\n            ),\n            input_word_length_std=pl.col(\"text_change\").map_elements(\n                lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0),\n                return_dtype=pl.Float64,\n            ),\n            input_word_length_median=pl.col(\"text_change\").map_elements(\n                lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0),\n                return_dtype=pl.Float64,\n            ),\n            input_word_length_skew=pl.col(\"text_change\").map_elements(\n                lambda x: skew([len(i) for i in x] if len(x) > 0 else 0),\n                return_dtype=pl.Float64,\n            ),\n        )\n        return temp.drop(\"text_change\")\n\n    def _numerical_stats(self, df):\n        return df.group_by(\"id\").agg(\n            [\n                pl.sum(\"action_time\").alias(\"action_time_sum\"),\n                *[pl.mean(col).alias(f\"{col}_mean\") for col in self.num_cols],\n                *[pl.std(col).alias(f\"{col}_std\") for col in self.num_cols],\n                *[pl.median(col).alias(f\"{col}_median\") for col in self.num_cols],\n                *[pl.min(col).alias(f\"{col}_min\") for col in self.num_cols],\n                *[pl.max(col).alias(f\"{col}_max\") for col in self.num_cols],\n                *[\n                    pl.quantile(col, 0.5).alias(f\"{col}_quantile\")\n                    for col in self.num_cols\n                ],\n            ]\n        )\n\n    def _categorical_stats(self, df):\n        return df.group_by(\"id\").agg(\n            [\n                pl.n_unique(\"activity\").alias(\"activity_nunique\"),\n                pl.n_unique(\"down_event\").alias(\"down_event_nunique\"),\n                pl.n_unique(\"up_event\").alias(\"up_event_nunique\"),\n                pl.n_unique(\"text_change\").alias(\"text_change_nunique\"),\n            ]\n        )\n\n    def _idle_time_features(self, df):\n        temp = df.with_columns(\n            pl.col(\"up_time\").shift().over(\"id\").alias(\"up_time_lagged\")\n        )\n        temp = temp.with_columns(\n            (pl.col(\"down_time\") - pl.col(\"up_time_lagged\"))\n            .abs()\n            .truediv(1000)\n            .fill_null(0)\n            .alias(\"time_diff\")\n        )\n        temp = temp.filter(pl.col(\"activity\").is_in([\"Input\", \"Remove/Cut\"]))\n\n        return temp.group_by(\"id\").agg(\n            [\n                pl.max(\"time_diff\").alias(\"inter_key_largest_latency\"),\n                pl.median(\"time_diff\").alias(\"inter_key_median_latency\"),\n                pl.mean(\"time_diff\").alias(\"mean_pause_time\"),\n                pl.std(\"time_diff\").alias(\"std_pause_time\"),\n                pl.sum(\"time_diff\").alias(\"total_pause_time\"),\n                pl.col(\"time_diff\")\n                .filter((pl.col(\"time_diff\") > 0.5) & (pl.col(\"time_diff\") < 1))\n                .count()\n                .alias(\"pauses_half_sec\"),\n                pl.col(\"time_diff\")\n                .filter((pl.col(\"time_diff\") > 1) & (pl.col(\"time_diff\") < 1.5))\n                .count()\n                .alias(\"pauses_1_sec\"),\n                pl.col(\"time_diff\")\n                .filter((pl.col(\"time_diff\") > 1.5) & (pl.col(\"time_diff\") < 2))\n                .count()\n                .alias(\"pauses_1_half_sec\"),\n                pl.col(\"time_diff\")\n                .filter((pl.col(\"time_diff\") > 2) & (pl.col(\"time_diff\") < 3))\n                .count()\n                .alias(\"pauses_2_sec\"),\n                pl.col(\"time_diff\")\n                .filter(pl.col(\"time_diff\") > 3)\n                .count()\n                .alias(\"pauses_3_sec\"),\n            ]\n        )\n\n    def _p_burst_features(self, df):\n        temp = df.with_columns(\n            pl.col(\"up_time\").shift().over(\"id\").alias(\"up_time_lagged\")\n        )\n        temp = temp.with_columns(\n            (pl.col(\"down_time\") - pl.col(\"up_time_lagged\"))\n            .abs()\n            .truediv(1000)\n            .fill_null(0)\n            .alias(\"time_diff\")\n        )\n        temp = temp.filter(pl.col(\"activity\").is_in([\"Input\", \"Remove/Cut\"]))\n        temp = temp.with_columns((pl.col(\"time_diff\") < 2).alias(\"time_diff\"))\n        temp = temp.with_columns(\n            pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last_distinct())\n            .then(pl.count())\n            .over(pl.col(\"time_diff\").rle_id())\n            .alias(\"P_bursts\")\n        )\n        temp = temp.drop_nulls()\n\n        return temp.group_by(\"id\").agg(\n            [\n                pl.mean(\"P_bursts\").alias(\"P_bursts_mean\"),\n                pl.std(\"P_bursts\").alias(\"P_bursts_std\"),\n                pl.count(\"P_bursts\").alias(\"P_bursts_count\"),\n                pl.median(\"P_bursts\").alias(\"P_bursts_median\"),\n                pl.max(\"P_bursts\").alias(\"P_bursts_max\"),\n                pl.first(\"P_bursts\").alias(\"P_bursts_first\"),\n                pl.last(\"P_bursts\").alias(\"P_bursts_last\"),\n            ]\n        )\n\n    def _r_burst_features(self, df):\n        temp = df.filter(pl.col(\"activity\").is_in([\"Input\", \"Remove/Cut\"]))\n        temp = temp.with_columns(\n            pl.col(\"activity\").is_in([\"Remove/Cut\"]).alias(\"activity\")\n        )\n        temp = temp.with_columns(\n            pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last_distinct())\n            .then(pl.count())\n            .over(pl.col(\"activity\").rle_id())\n            .alias(\"R_bursts\")\n        )\n        temp = temp.drop_nulls()\n\n        return temp.group_by(\"id\").agg(\n            [\n                pl.mean(\"R_bursts\").alias(\"R_bursts_mean\"),\n                pl.std(\"R_bursts\").alias(\"R_bursts_std\"),\n                pl.median(\"R_bursts\").alias(\"R_bursts_median\"),\n                pl.max(\"R_bursts\").alias(\"R_bursts_max\"),\n                pl.first(\"R_bursts\").alias(\"R_bursts_first\"),\n                pl.last(\"R_bursts\").alias(\"R_bursts_last\"),\n            ]\n        )\n",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef run_integration_pipeline(\n    data_path=\"data/\",\n    use_existing_features=True,\n    use_advanced_ensemble=True,\n    use_tabpfn=False,\n    n_splits=5,\n    random_state=42,\n):\n    '''\n\n        Definitions\n        train_logs: raw train data\n        test_logs: raw test data\n        train_scores: y\n\n\n        train_essays: masked paragraphs written\n        test_essays: masked paragraphs written\n\n        test_basic:\n    '''\n\n\n    train_logs = pd.read_csv(TRAIN_DATA_PATH)\n    test_logs = pd.read_csv(TEST_DATA_PATH)\n    train_scores = train_score_df\n    train_essays = extracted_text_train\n    print(\"Loaded cleaned csv\")\n    train_essays = train_essays.rename(columns={\"text\": \"essay\"})\n    test_essays = extracted_text[\n        [\"id\", \"text\"]\n    ]\n    test_essays = test_essays.rename(columns={\"text\": \"essay\"})\n    print(\"Loaded reconstructed essays\")\n    train_basic = pd.read_csv(TRAIN_DATA_PREPROCESSED_PATH)\n    test_basic = merged_df\n    print(\"Loaded existing preprocessed features\")\n\n    # Extract IDs and scores\n\n    train_ids = train_basic[\"id\"].values\n    test_ids = test_basic[\"id\"].values\n    y = train_basic[\"score\"].values\n\n    train_basic = train_basic.drop(columns=[\"id\", \"score\"], errors=\"ignore\")\n    test_basic = test_basic.drop(columns=[\"id\"], errors=\"ignore\")\n    print(\"Loaded Basic Features\")\n    # ngram\n    char_ngram_extractor = CharacterNGramFeatureExtractor(\n        ngram_range=(1, 4),\n        analyzer=\"char_wb\",\n        max_features=1200,\n    )\n\n    train_char_ngrams, test_char_ngrams = char_ngram_extractor.fit_transform(\n        train_essays, test_essays\n    )\n\n    train_char_ngrams = train_char_ngrams.drop(columns=[\"id\"])\n    test_char_ngrams = test_char_ngrams.drop(columns=[\"id\"])\n\n    variance = train_char_ngrams.var()\n    top_features = variance.nlargest(500).index\n    train_char_ngrams = train_char_ngrams[top_features]\n    test_char_ngrams = test_char_ngrams[top_features]\n\n    train_basic = pd.concat(\n        [\n            train_basic.reset_index(drop=True),\n            train_char_ngrams.reset_index(drop=True),\n        ],\n        axis=1,\n    )\n    test_basic = pd.concat(\n        [\n            test_basic.reset_index(drop=True),\n            test_char_ngrams.reset_index(drop=True),\n        ],\n        axis=1,\n    )\n    print(\"N grams complete\")\n\n    # lda\n    lda_extractor = LDATopicFeatureExtractor(\n        n_topics=6, max_iter=10, random_state=random_state\n    )\n\n    train_lda, test_lda = lda_extractor.fit_transform(\n        train_essays, test_essays)\n\n    train_lda = train_lda.drop(columns=[\"id\"])\n    test_lda = test_lda.drop(columns=[\"id\"])\n    train_basic = pd.concat(\n        [train_basic.reset_index(drop=True), train_lda.reset_index(drop=True)],\n        axis=1,\n    )\n    test_basic = pd.concat(\n        [test_basic.reset_index(drop=True), test_lda.reset_index(drop=True)], axis=1\n    )\n\n    # polars\n    polars_extractor = PolarsFeatureExtractor()\n\n    train_polars = polars_extractor.extract_features(train_logs)\n    test_polars = polars_extractor.extract_features(test_logs)\n\n    train_polars_temp = (\n        train_polars.set_index(\"id\").loc[train_ids].reset_index(drop=True)\n    )\n    test_polars_temp = test_polars.set_index(\n        \"id\").loc[test_ids].reset_index(drop=True)\n\n    polars_cols = [\n        c for c in train_polars_temp.columns if c not in train_basic.columns]\n    train_polars_temp = train_polars_temp[polars_cols]\n    test_polars_temp = test_polars_temp[polars_cols]\n\n    train_basic = pd.concat(\n        [\n            train_basic.reset_index(drop=True),\n            train_polars_temp.reset_index(drop=True),\n        ],\n        axis=1,\n    )\n    test_basic = pd.concat(\n        [\n            test_basic.reset_index(drop=True),\n            test_polars_temp.reset_index(drop=True),\n        ],\n        axis=1,\n    )\n    # tsne\n    tsne_extractor = TSNEFeatureExtractor(\n        perplexities=[20, 50, 80],\n        n_components=2,\n        random_state=random_state,\n        n_jobs=-1,\n    )\n\n    train_tsne, test_tsne = tsne_extractor.fit_transform(\n        train_basic, test_basic)\n\n    train_basic = pd.concat(\n        [train_basic.reset_index(drop=True),\n         train_tsne.reset_index(drop=True)],\n        axis=1,\n    )\n    test_basic = pd.concat(\n        [test_basic.reset_index(drop=True), test_tsne.reset_index(drop=True)],\n        axis=1,\n    )\n    print(\"Extra Features completed\")\n\n    # training\n    ensemble = EnhancedEnsembleModel(\n        n_splits=n_splits, random_state=random_state)\n\n    print(\"fit on data\")\n    scores, train_preds, test_preds = ensemble.fit(train_basic, y, test_basic)\n\n    submission = pd.DataFrame({\"id\": test_ids, \"score\": test_preds})\n    print(\"submission complete\")\n    return submission\n\n\n\n\nsubmission = run_integration_pipeline()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Test Submission only",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === TEST ONLY: load model(s) -> predict on test -> write submission ===\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport time\n\n# ---------------- paths ----------------\n\n\n# ---------------- load test ----------------\n# df_test = merged_df\n# test_ids = df_test[\"id\"].values\n# X_test = df_test.drop(columns=[\"id\"])\n\n\n# model_file_candidate = \"/kaggle/input/lgbm-ensemble/scikitlearn/default/1/lgbm.pkl\"\n\n\n# model_obj = None\n# if os.path.exists(model_file_candidate):\n#     model_obj = joblib.load(model_file_candidate)\n#     print(f\"Loaded model from: {model_file_candidate}\")\n# if model_obj is None:\n#     raise FileNotFoundError(\"No saved model found. Expected one of: \" + \", \".join(model_file_candidate))\n\n# def predict_with_model_obj(model_obj, X):\n#     # Multiple Fold Model\n#     if isinstance(model_obj, (list, tuple)):\n#         preds = np.mean([m.predict(X) for m in model_obj], axis=0)\n#         return preds\n#     # Single Model\n#     return model_obj.predict(X)\n\n# test_preds = predict_with_model_obj(model_obj,X_test)\n\n# # ---------------- write submission ----------------\n# submission = pd.DataFrame({\"id\": test_ids, \"score\": test_preds})\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Submission saved to: {SUBMISSION_PATH}\\nSubmission Time {time.asctime( time.localtime(time.time()) )}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:37:47.828246Z",
     "start_time": "2025-11-10T15:37:47.809318Z"
    },
    "trusted": true,
    "execution": {
     "execution_failed": "2025-11-13T15:01:12.737Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
