{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 59291,
     "databundleVersionId": 6678907,
     "isSourceIdPinned": false,
     "sourceType": "competition"
    },
    {
     "sourceId": 13682482,
     "sourceType": "datasetVersion",
     "datasetId": 8701104
    },
    {
     "sourceId": 13682716,
     "sourceType": "datasetVersion",
     "datasetId": 8701282
    },
    {
     "sourceId": 637915,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": false,
     "modelInstanceId": 480881,
     "modelId": 496492
    },
    {
     "sourceId": 641335,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 483649,
     "modelId": 499161
    },
    {
     "sourceId": 641788,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 484011,
     "modelId": 499513
    }
   ],
   "dockerImageVersionId": 31192,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Integrated Notebook for Task WritingProcess\n## Data Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---------\n### Idle Removing and Time Regularization from `preprocess.py`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "`ftfy` takes tons of time to run so I temporary disabled it on train set, a pre-processed dataset is uploaded to replace it. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solve ftfy dependency\n!pip install /kaggle/input/ftfypkg/ftfy_pkg/wcwidth-0.2.14-py2.py3-none-any.whl --no-index --find-links /kaggle/input/ftfypkg/ftfy_pkg\n\n!pip install /kaggle/input/ftfypkg/ftfy_pkg/ftfy-6.3.1-py3-none-any.whl --no-index --find-links /kaggle/input/ftfypkg/ftfy_pkg\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:22:04.784181Z",
     "iopub.execute_input": "2025-11-12T05:22:04.784387Z",
     "iopub.status.idle": "2025-11-12T05:22:16.239341Z",
     "shell.execute_reply.started": "2025-11-12T05:22:04.784368Z",
     "shell.execute_reply": "2025-11-12T05:22:16.237999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Looking in links: /kaggle/input/ftfypkg/ftfy_pkg\nProcessing /kaggle/input/ftfypkg/ftfy_pkg/wcwidth-0.2.14-py2.py3-none-any.whl\nInstalling collected packages: wcwidth\n  Attempting uninstall: wcwidth\n    Found existing installation: wcwidth 0.2.13\n    Uninstalling wcwidth-0.2.13:\n      Successfully uninstalled wcwidth-0.2.13\nSuccessfully installed wcwidth-0.2.14\nLooking in links: /kaggle/input/ftfypkg/ftfy_pkg\nProcessing /kaggle/input/ftfypkg/ftfy_pkg/ftfy-6.3.1-py3-none-any.whl\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy==6.3.1) (0.2.14)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.3.1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport ftfy\nimport warnings\nfrom pathlib import Path\nimport re\n\nwarnings.filterwarnings('ignore')",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:35:08.265907Z",
     "start_time": "2025-11-10T14:35:08.262529Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:22:16.242028Z",
     "iopub.execute_input": "2025-11-12T05:22:16.242347Z",
     "iopub.status.idle": "2025-11-12T05:22:17.328937Z",
     "shell.execute_reply.started": "2025-11-12T05:22:16.242308Z",
     "shell.execute_reply": "2025-11-12T05:22:17.327672Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "class Preprocess:\n\n    def label_encoding(self, df, col=\"id\"):\n        label_encoder = LabelEncoder()\n        label_encoder.fit(df[col])\n        df[col + \"_encoded\"] = label_encoder.transform(df[col])\n        return df\n\n    # remove time that the author havent start writing or is resting\n    # reference: remove_margin for https://www.kaggle.com/code/tomooinubushi/1st-place-solution-training-and-inference-code\n\n    def remove_start_and_end_time(\n        self, df, start_margin=2 * 60 * 1000, end_margin=2 * 60 * 1000\n    ):\n        df = df[df[\"up_event\"] != \"Unidentified\"].reset_index(drop=True)\n        result_df = []\n        grouped_df = df.groupby(\"id_encoded\")\n\n        for _, log in tqdm(grouped_df):\n            valid_events = log[\n                (log.activity != \"Nonproduction\")\n                | (log.up_event != \"Shift\")\n                | (log.up_event != \"CapsLock\")\n            ].down_time.values\n            if len(valid_events) == 0:\n                continue\n            log = log[\n                (log.down_time > valid_events.min() - start_margin)\n                & (log[\"down_time\"] <= valid_events.max() + end_margin)\n            ].copy()\n            log[\"event_id\"] = range(len(log))\n            result_df.append(log)\n\n        result = pd.concat(result_df, ignore_index=True)\n\n        return result\n\n    def remove_rest_time(\n        self, df, time_margin=1 * 60 * 1000, action_margin=5 * 60 * 1000\n    ):\n        down_times, up_times = [], []\n        prev_idx = -1\n        result_df = df[[\"id_encoded\", \"down_time\", \"up_time\"]].values\n        for row in tqdm(result_df):\n            idx, down_time, up_time = int(row[0]), int(row[1]), int(row[2])\n            if prev_idx != idx:\n                prev_down_time = down_time\n                prev_corrected_down_time = 0\n            gap_down_time = np.clip(down_time - prev_down_time, 0, time_margin)\n            action_time = np.clip(up_time - down_time, 0, action_margin)\n\n            new_down_time = prev_corrected_down_time + gap_down_time\n            new_up_time = new_down_time + action_time\n            down_times.append(new_down_time)\n            up_times.append(new_up_time)\n            prev_idx, prev_corrected_down_time, prev_down_time = (\n                idx,\n                new_down_time,\n                down_time,\n            )\n        df[\"down_time\"], df[\"up_time\"] = down_times, up_times\n        return df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:35:08.681597Z",
     "start_time": "2025-11-10T14:35:08.676159Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:22:17.330306Z",
     "iopub.execute_input": "2025-11-12T05:22:17.331119Z",
     "iopub.status.idle": "2025-11-12T05:22:17.343066Z",
     "shell.execute_reply.started": "2025-11-12T05:22:17.331087Z",
     "shell.execute_reply": "2025-11-12T05:22:17.341912Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "preprocessor = Preprocess()\n",
    "# ------------------ Config dataset (In submission we only have test file) ----------------------------\n",
    "df = pd.read_csv(\"data/test_logs.csv\")\n",
    "# ------------------ Config dataset (TFIDF has to be fit on train and transform on test) ----------------------------\n",
    "train_df = pd.read_csv(\"data/train_logs_raw_unicode_fixed.csv\")\n",
    "# Replacing the original dataset, for fast processing and scoring.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = preprocessor.label_encoding(df)\n",
    "df = preprocessor.remove_start_and_end_time(df)\n",
    "df = preprocessor.remove_rest_time(df)\n",
    "\n",
    "train_df = preprocessor.label_encoding(train_df)\n",
    "train_df = preprocessor.remove_start_and_end_time(train_df)\n",
    "train_df = preprocessor.remove_rest_time(train_df)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:22:17.344272Z",
     "iopub.execute_input": "2025-11-12T05:22:17.344685Z",
     "iopub.status.idle": "2025-11-12T05:24:53.887489Z",
     "shell.execute_reply.started": "2025-11-12T05:22:17.344651Z",
     "shell.execute_reply": "2025-11-12T05:24:53.886276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 3/3 [00:00<00:00, 285.28it/s]\n100%|██████████| 6/6 [00:00<00:00, 23280.13it/s]\n100%|██████████| 2471/2471 [00:08<00:00, 303.33it/s]\n100%|██████████| 8399747/8399747 [01:29<00:00, 93871.90it/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "-----------\n### Event,Unicode Cleaning from `Preprocessing.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def label_encoding(df, col=\"id\"):\n    label_encoder = LabelEncoder()\n    label_encoder.fit(df[col])\n    df[col + \"_encoded\"] = label_encoder.transform(df[col])\n    return df\n\n\n# remove time that the author havent start writing or is resting\n# reference: remove_margin for https://www.kaggle.com/code/tomooinubushi/1st-place-solution-training-and-inference-code\n\ndef remove_procrastination_time(df, start_margin=2*60*1000, end_margin=2*60*1000):\n    df = df[df['up_event'] != 'Unidentified'].reset_index(drop=True)\n    result_df = []\n    grouped_df = df.groupby('id_encoded')\n\n    for _, log in tqdm(grouped_df):\n        valid_events = log[(log.activity != 'Nonproduction') & (\n            log.up_event != 'Shift') & (log.up_event != 'CapsLock')].down_time.values\n        if len(valid_events) == 0:\n            continue\n        log = log[(log.down_time > valid_events.min() - start_margin)\n                  & (log['down_time'] <= valid_events.max() + end_margin)].copy()\n        log['event_id'] = range(len(log))\n        result_df.append(log)\n\n    result = pd.concat(result_df, ignore_index=True)\n\n    return result\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:40:44.909799Z",
     "start_time": "2025-11-10T14:40:44.905798Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:24:53.888612Z",
     "iopub.execute_input": "2025-11-12T05:24:53.888922Z",
     "iopub.status.idle": "2025-11-12T05:24:53.897821Z",
     "shell.execute_reply.started": "2025-11-12T05:24:53.888897Z",
     "shell.execute_reply": "2025-11-12T05:24:53.896488Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "class CleanPreprocessor:\n    def cleaning(self,df,skipUnicodeFixing=False):\n        df = label_encoding(df)\n        df = remove_procrastination_time(df)\n        df = df[df['activity'] != 'Nonproduction' ].reset_index(drop=True)\n        cols = ['down_event', 'up_event', 'text_change']\n        if not skipUnicodeFixing:\n            df.loc[:, cols] = df.loc[:, cols].apply(\n                lambda s: s.astype('string').map(lambda x: ftfy.fix_text(x) if x is not pd.NA else x)\n            )\n        \n        drop_events = ['LeftClick','RightClick']\n        df = df[~df['down_event'].isin(drop_events)]\n        df['event_id'] = df.groupby('id').cumcount() + 1 # reset event_id\n        df.reset_index(inplace=True,drop=True)\n        return df\n    \n\ncleaner = CleanPreprocessor()\ndf = cleaner.cleaning(df)\ntrain_df = cleaner.cleaning(train_df,skipUnicodeFixing=True)\n\n        \n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:41:50.658675Z",
     "start_time": "2025-11-10T14:40:46.429632Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:24:53.898889Z",
     "iopub.execute_input": "2025-11-12T05:24:53.899195Z",
     "iopub.status.idle": "2025-11-12T05:25:14.718589Z",
     "shell.execute_reply.started": "2025-11-12T05:24:53.899162Z",
     "shell.execute_reply": "2025-11-12T05:25:14.717544Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 3/3 [00:00<00:00, 578.50it/s]\n100%|██████████| 2471/2471 [00:07<00:00, 313.94it/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "-------\n### Text Essay Rebuilding\nWork is taken from `text_process.py`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TextProcessor:\n    PUNCTUATION_MAP = {\n        \"SPACE\": \" \",\n        \"COMMA\": \",\",\n        \"DOUBLE_QUOTE\": '\"',\n        \"PERIOD\": \".\",\n        \"PARENTHESES_OPEN\": \"(\",\n        \"PARENTHESES_CLOSE\": \")\",\n        \"SQUARE_BRACKET_OPEN\": \"[\",\n        \"SQUARE_BRACKET_CLOSE\": \"]\",\n        \"CURLY_BRACKET_OPEN\": \"{\",\n        \"CURLY_BRACKET_CLOSE\": \"}\",\n        \"EXCLAMATION_MARK\": \"!\",\n        \"QUESTION_MARK\": \"?\",\n    }\n\n    def insert_text(self, text, s, pos):\n        return \"\".join((text[:pos], s, text[pos:]))\n\n    def remove_text(self, text, s, pos):\n        return \"\".join((text[:pos], text[pos + len(s):]))\n\n    def replace_text(self, text, s1, s2, pos):\n        return \"\".join((text[:pos], s2, text[pos + len(s1):]))\n\n    def move_text(self, text, s, pos1, pos2):\n        text = self.remove_text(text, s, pos1)\n        text = self.insert_text(text, s, pos2)\n        return text\n\n    def split_to_word(self, s):\n        s = s.lower()\n        char_sep = \"@\"\n        punctuation_chars = list(self.PUNCTUATION_MAP.values())\n        for pun in punctuation_chars:\n            s = s.replace(pun, char_sep)\n        s_arr = re.split(char_sep, s)\n        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty words\n        return s_arr\n\n    def split_to_sentence(self, s):\n        s = s.lower()\n        char_sep = \"@\"\n        punctuation = [\".\", \"!\", \"?\"]\n        for punc in punctuation:\n            s = s.replace(punc, char_sep)\n        s_arr = re.split(char_sep, s)\n        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty sentences\n        return s_arr\n\n    def split_to_paragraph(self, s):\n        s = s.lower()\n        s_arr = re.split(r'n\\s*n+', s)\n        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty paragraphs\n        return s_arr\n\n    def change_punctuation(self, text):\n        reverse_map = {v: k.lower()\n                       for k, v in self.PUNCTUATION_MAP.items()}\n        result = []\n        for char in text:\n            if char in reverse_map:\n                result.append(' ' + reverse_map[char] + ' ')\n            else:\n                result.append(char)\n        output = \"\".join(result)\n        output = re.sub(r\"\\s+\", \" \", output).strip()\n\n        return output\n\nclass EssayConstructor:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n\n    def recon_writing(self, df):\n        res_all = []\n        len_texts = []\n        sentence_counts = []\n        paragraph_counts = []\n\n        res = \"\"\n        prev_idx = \"\"\n\n        temp_df = df[['id', 'activity', 'up_event', 'text_change',\n                      'cursor_position', 'word_count']].values\n\n        for row in tqdm(temp_df):\n            idx = str(row[0])\n            activity, up_event, text_change = str(\n                row[1]), str(row[2]), str(row[3])\n            cursor_position, _ = int(row[4]), int(row[5])\n\n            # new idx\n            if idx != prev_idx:\n                if prev_idx != \"\":\n                    # append first essay data\n                    res_all.append(res)\n                    len_texts.append(len_text)\n                    sentence_counts.append(sentence_count)\n                    paragraph_counts.append(paragraph_count)\n\n                res, len_text, sentence_count, paragraph_count = \"\", 0, 0, 0\n                prev_idx = idx\n\n            if activity != \"Nonproduction\":\n                # replace the newline character to n\n                text_change = text_change.replace(\"@\", \"/\").replace(\"\\n\", \"n\")\n\n                if (activity == \"Input\") | (activity == \"Paste\"):\n                    res = self.text_processor.insert_text(\n                        res, text_change, cursor_position - len(text_change)\n                    )\n\n                elif activity == \"Remove/Cut\":\n                    res = self.text_processor.remove_text(\n                        res, text_change, cursor_position\n                    )\n\n                elif activity == \"Replace\":\n                    before, after = text_change.split(\" => \")\n                    res = self.text_processor.replace_text(\n                        res, before, after, cursor_position - len(after)\n                    )\n\n                elif \"Move\" in activity:\n                    pos = [int(s) for s in re.findall(r\"\\d+\", activity)]\n                    # pos 0 start pos1 end pos2 start pos3 end\n                    res = self.text_processor.move_text(\n                        res, text_change, pos[0], pos[2]\n                    )\n\n                len_text = len(res)\n                sentence_count = len(\n                    self.text_processor.split_to_sentence(res))\n                paragraph_count = len(\n                    self.text_processor.split_to_paragraph(res))\n\n            prev_up_event = up_event\n\n        # append last essay data\n        res_all.append(res)\n        len_texts.append(len_text)\n        sentence_counts.append(sentence_count)\n        paragraph_counts.append(paragraph_count)\n\n        return res_all, len_texts, sentence_counts, paragraph_counts\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:43:18.925275Z",
     "start_time": "2025-11-10T14:43:18.914955Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:25:14.721866Z",
     "iopub.execute_input": "2025-11-12T05:25:14.722185Z",
     "iopub.status.idle": "2025-11-12T05:25:14.741623Z",
     "shell.execute_reply.started": "2025-11-12T05:25:14.722158Z",
     "shell.execute_reply": "2025-11-12T05:25:14.740560Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "essay_constructor = EssayConstructor()\nreconstructed_texts, len_texts, sentence_counts, paragraph_counts = essay_constructor.recon_writing(\n    df)\nidx = df[\"id\"].unique()\nresult_df = pd.DataFrame({\"id\": idx, \"text\": reconstructed_texts, \"len_text\": len_texts,\n                         \"sentence_count\": sentence_counts, \"paragraph_count\": paragraph_counts})\n\nextracted_text = result_df\n\nreconstructed_texts, len_texts, sentence_counts, paragraph_counts = essay_constructor.recon_writing(\n    train_df)\nidx = train_df[\"id\"].unique()\nextracted_text_train = pd.DataFrame({\"id\": idx, \"text\": reconstructed_texts, \"len_text\": len_texts,\n                         \"sentence_count\": sentence_counts, \"paragraph_count\": paragraph_counts})\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:49:56.317647Z",
     "start_time": "2025-11-10T14:49:12.168504Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:25:14.742690Z",
     "iopub.execute_input": "2025-11-12T05:25:14.742956Z",
     "iopub.status.idle": "2025-11-12T05:26:58.833862Z",
     "shell.execute_reply.started": "2025-11-12T05:25:14.742935Z",
     "shell.execute_reply": "2025-11-12T05:26:58.832600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 6/6 [00:00<00:00, 13865.47it/s]\n100%|██████████| 7702047/7702047 [01:41<00:00, 75766.38it/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "-------------------\n## Feature Engineering\n### Behaviour Feature\n\nThis part is taken from `feature_extraction.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Feature Extraction Functions\n\nExtract different behavioural features from keystroke logs.\nWe want to capture:\n\n- pauses (when people are thinking)\n- bursts (when they're typing continuously)\n- editing behaviour (how much they revise)\n- cursor movement (planning vs going back to edit)\n\n### 2.1 Base Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:04.963959Z",
     "start_time": "2025-11-10T14:50:04.961921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def extract_features(df):\n    \"\"\"Pull out the main features from the log data\"\"\"\n    \n    # Count up events and get basic stats\n    features = df.groupby(\"id\").agg(\n        events_count=('event_id', 'count'),\n        total_time=('up_time', 'max'),\n        total_chars=('word_count', 'max'),\n        mean_action_time=('action_time', 'mean'),\n        std_action_time=('action_time', 'std'),\n        max_action_time=('action_time', 'max'),\n        min_action_time=('action_time', 'min'),\n        \n        # Count different types of actions\n        backspace_count=('activity', lambda x: (x == \"Remove/Cut\").sum()),\n        paste_count=('activity', lambda x: (x == \"Paste\").sum()),\n        input_count=('activity', lambda x: (x == \"Input\").sum()),\n        move_count=('activity', lambda x: x.str.contains(\"Move\", na=False).sum()),\n        replace_count=('activity', lambda x: (x == \"Replace\").sum()),\n        nonproduction_count=('activity', lambda x: (x == \"Nonproduction\").sum()),\n        \n        # Where the cursor was\n        cursor_pos_mean=('cursor_position', 'mean'),\n        cursor_pos_std=('cursor_position', 'std'),\n        cursor_pos_max=('cursor_position', 'max'),\n        \n        # Word count stats\n        word_count_mean=('word_count', 'mean'),\n        word_count_std=('word_count', 'std'),\n        word_count_diff=('word_count', lambda x: x.max() - x.min()),\n    ).reset_index()\n    \n    # Replace any missing values with 0\n    features = features.fillna(0)\n    \n    # Calculate some ratios\n    features['chars_per_min'] = features['total_chars'] / (features['total_time'] / 60000 + 1e-6)\n    features['events_per_min'] = features['events_count'] / (features['total_time'] / 60000 + 1e-6)\n    features['backspace_ratio'] = features['backspace_count'] / (features['input_count'] + 1)\n    features['paste_ratio'] = features['paste_count'] / (features['events_count'] + 1)\n    features['replace_ratio'] = features['replace_count'] / (features['events_count'] + 1)\n    features['nonproduction_ratio'] = features['nonproduction_count'] / (features['events_count'] + 1)\n    features['revision_ratio'] = (features['backspace_count'] + features['replace_count']) / (features['total_chars'] + 1)\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:05.268536Z",
     "start_time": "2025-11-10T14:50:05.262686Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.834963Z",
     "iopub.execute_input": "2025-11-12T05:26:58.835288Z",
     "iopub.status.idle": "2025-11-12T05:26:58.846535Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.835255Z",
     "shell.execute_reply": "2025-11-12T05:26:58.845492Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 Pause Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def pause_features(df):\n    \"\"\"Get features about pauses (gaps between keystrokes)\"\"\"\n    df = df.sort_values([\"id\", \"down_time\"]).copy()\n    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n    \n    # Count pauses at different thresholds (2s, 5s, 10s)\n    pause_2s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 2000).sum()).rename(\"pause_2s_count\")\n    pause_5s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 5000).sum()).rename(\"pause_5s_count\")\n    pause_10s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 10000).sum()).rename(\"pause_10s_count\")\n    \n    # Basic pause stats\n    mean_pause = df.groupby(\"id\")[\"iki\"].mean().rename(\"mean_pause\")\n    median_pause = df.groupby(\"id\")[\"iki\"].median().rename(\"median_pause\")\n    std_pause = df.groupby(\"id\")[\"iki\"].std().rename(\"std_pause\")\n    max_pause = df.groupby(\"id\")[\"iki\"].max().rename(\"max_pause\")\n    min_pause = df.groupby(\"id\")[\"iki\"].min().rename(\"min_pause\")\n    \n    return pause_2s, pause_5s, pause_10s, mean_pause, median_pause, std_pause, max_pause, min_pause\n\n\ndef burst_features(df):\n    \"\"\"Get features about bursts (when they're typing continuously)\"\"\"\n    df = df.sort_values([\"id\", \"down_time\"]).copy()\n    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n    df[\"burst\"] = (df[\"iki\"] > 2000).astype(int)\n    df[\"burst_id\"] = df.groupby(\"id\")[\"burst\"].cumsum()\n    \n    burst_len = df.groupby([\"id\", \"burst_id\"]).size()\n    avg_burst = burst_len.groupby(\"id\").mean().rename(\"avg_burst\")\n    max_burst = burst_len.groupby(\"id\").max().rename(\"max_burst\")\n    std_burst = burst_len.groupby(\"id\").std().rename(\"std_burst\")\n    \n    return avg_burst, max_burst, std_burst\n\n\ndef p_burst_features(df):\n    \"\"\"Get P-burst features (how many words per burst)\"\"\"\n    df = df.sort_values([\"id\", \"down_time\"]).copy()\n    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n    \n    # P-bursts: pauses longer than 2s\n    df[\"p_burst\"] = (df[\"iki\"] > 2000).astype(int)\n    df[\"p_burst_id\"] = df.groupby(\"id\")[\"p_burst\"].cumsum()\n    \n    # How many words in each burst\n    p_burst_words = df.groupby([\"id\", \"p_burst_id\"])[\"word_count\"].apply(lambda x: x.max() - x.min())\n    avg_words_per_p_burst = p_burst_words.groupby(\"id\").mean().rename(\"avg_words_per_p_burst\")\n    \n    return avg_words_per_p_burst",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:06.009040Z",
     "start_time": "2025-11-10T14:50:06.002041Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.847580Z",
     "iopub.execute_input": "2025-11-12T05:26:58.847938Z",
     "iopub.status.idle": "2025-11-12T05:26:58.874536Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.847907Z",
     "shell.execute_reply": "2025-11-12T05:26:58.873515Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 Activity Sequence & Text Change Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def activity_sequence_features(df):\n    \"\"\"Get features from activity patterns and transitions\"\"\"\n    features = []\n    \n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val].sort_values('down_time')\n        activities = id_df['activity'].values\n        \n        # Track how activities transition from one to another\n        transitions = {}\n        for i in range(len(activities) - 1):\n            transition = f\"{activities[i]}->{activities[i+1]}\"\n            transitions[transition] = transitions.get(transition, 0) + 1\n        \n        # Common patterns\n        input_to_remove = transitions.get('Input->Remove/Cut', 0)\n        remove_to_input = transitions.get('Remove/Cut->Input', 0)\n        input_to_input = transitions.get('Input->Input', 0)\n        paste_to_input = transitions.get('Paste->Input', 0)\n        \n        # Find the longest streaks of the same activity\n        max_input_streak = 0\n        max_remove_streak = 0\n        current_input_streak = 0\n        current_remove_streak = 0\n        \n        for act in activities:\n            if act == 'Input':\n                current_input_streak += 1\n                max_input_streak = max(max_input_streak, current_input_streak)\n                current_remove_streak = 0\n            elif act == 'Remove/Cut':\n                current_remove_streak += 1\n                max_remove_streak = max(max_remove_streak, current_remove_streak)\n                current_input_streak = 0\n            else:\n                current_input_streak = 0\n                current_remove_streak = 0\n        \n        # How varied are the activities\n        unique_activities = len(set(activities))\n        activity_switches = sum(1 for i in range(len(activities)-1) if activities[i] != activities[i+1])\n        \n        features.append({\n            'id': id_val,\n            'input_to_remove_trans': input_to_remove,\n            'remove_to_input_trans': remove_to_input,\n            'input_to_input_trans': input_to_input,\n            'paste_to_input_trans': paste_to_input,\n            'max_input_streak': max_input_streak,\n            'max_remove_streak': max_remove_streak,\n            'unique_activities': unique_activities,\n            'activity_switches': activity_switches,\n            'activity_switch_rate': activity_switches / len(activities) if len(activities) > 0 else 0\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef text_change_features(df):\n    \"\"\"Features about how the text changes\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    # How much text was added or removed\n    df['text_change'] = df.groupby('id')['word_count'].diff().fillna(0)\n    \n    features = df.groupby('id').agg(\n        total_text_produced=('text_change', lambda x: x[x > 0].sum()),\n        total_text_removed=('text_change', lambda x: abs(x[x < 0].sum())),\n        text_production_rate=('text_change', lambda x: x[x > 0].mean()),\n        text_removal_rate=('text_change', lambda x: x[x < 0].mean()),\n        max_text_addition=('text_change', 'max'),\n        max_text_removal=('text_change', 'min'),\n        text_volatility=('text_change', 'std'),\n        positive_text_changes=('text_change', lambda x: (x > 0).sum()),\n        negative_text_changes=('text_change', lambda x: (x < 0).sum()),\n    ).reset_index()\n    \n    # Calculate some more useful ratios\n    features['text_removal_ratio'] = features['total_text_removed'] / (features['total_text_produced'] + 1)\n    features['net_text_production'] = features['total_text_produced'] - features['total_text_removed']\n    features['text_efficiency'] = features['total_text_produced'] / (features['positive_text_changes'] + 1)\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:07.308050Z",
     "start_time": "2025-11-10T14:50:07.300951Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.875786Z",
     "iopub.execute_input": "2025-11-12T05:26:58.876108Z",
     "iopub.status.idle": "2025-11-12T05:26:58.909997Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.876077Z",
     "shell.execute_reply": "2025-11-12T05:26:58.909138Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "### 2.4 Temporal & Velocity Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def time_based_features(df):\n    \"\"\"Features based on when things happen (early, middle, late)\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    # Split the writing session into three parts\n    df['time_percentile'] = df.groupby('id')['down_time'].rank(pct=True)\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        # Split into early, middle, and late phases\n        early_phase = id_df[id_df['time_percentile'] <= 0.33]\n        middle_phase = id_df[(id_df['time_percentile'] > 0.33) & (id_df['time_percentile'] <= 0.67)]\n        late_phase = id_df[id_df['time_percentile'] > 0.67]\n        \n        features.append({\n            'id': id_val,\n            'early_events': len(early_phase),\n            'middle_events': len(middle_phase),\n            'late_events': len(late_phase),\n            'early_input_ratio': (early_phase['activity'] == 'Input').sum() / (len(early_phase) + 1),\n            'middle_input_ratio': (middle_phase['activity'] == 'Input').sum() / (len(middle_phase) + 1),\n            'late_input_ratio': (late_phase['activity'] == 'Input').sum() / (len(late_phase) + 1),\n            'early_remove_ratio': (early_phase['activity'] == 'Remove/Cut').sum() / (len(early_phase) + 1),\n            'late_remove_ratio': (late_phase['activity'] == 'Remove/Cut').sum() / (len(late_phase) + 1),\n            'middle_paste_ratio': (middle_phase['activity'] == 'Paste').sum() / (len(middle_phase) + 1),\n            'late_phase_activity': len(late_phase) / (len(id_df) + 1),\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef keystroke_velocity_features(df):\n    \"\"\"Features about typing speed\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    df['iki'] = df.groupby('id')['down_time'].diff()\n    \n    # Only look at actual typing events\n    input_df = df[df['activity'] == 'Input'].copy()\n    \n    if len(input_df) == 0:\n        return pd.DataFrame()\n    \n    features = input_df.groupby('id').agg(\n        input_iki_mean=('iki', 'mean'),\n        input_iki_std=('iki', 'std'),\n        input_iki_median=('iki', 'median'),\n        input_iki_min=('iki', 'min'),\n        input_iki_max=('iki', 'max'),\n        fast_keystrokes=('iki', lambda x: (x < 100).sum()),\n        moderate_keystrokes=('iki', lambda x: ((x >= 100) & (x <= 1000)).sum()),\n        slow_keystrokes=('iki', lambda x: (x > 1000).sum()),\n    ).reset_index()\n    \n    # How consistent is the typing\n    features['keystroke_consistency'] = features['input_iki_std'] / (features['input_iki_mean'] + 1)\n    features['fast_keystroke_ratio'] = features['fast_keystrokes'] / (features['fast_keystrokes'] + features['moderate_keystrokes'] + features['slow_keystrokes'] + 1)\n    features['typing_rhythm_score'] = features['moderate_keystrokes'] / (features['fast_keystrokes'] + features['moderate_keystrokes'] + features['slow_keystrokes'] + 1)\n    \n    return features\n\n\ndef word_count_velocity_features(df):\n    \"\"\"Features about how word count changes\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        word_counts = id_df['word_count'].values\n        time_stamps = id_df['down_time'].values\n        \n        # How fast are words being added\n        if len(word_counts) > 1:\n            word_velocity = np.diff(word_counts) / (np.diff(time_stamps) + 1)\n            \n            features.append({\n                'id': id_val,\n                'avg_word_velocity': np.mean(word_velocity),\n                'max_word_velocity': np.max(word_velocity),\n                'min_word_velocity': np.min(word_velocity),\n                'std_word_velocity': np.std(word_velocity),\n                'positive_velocity_ratio': (word_velocity > 0).sum() / len(word_velocity)\n            })\n        else:\n            features.append({\n                'id': id_val,\n                'avg_word_velocity': 0,\n                'max_word_velocity': 0,\n                'min_word_velocity': 0,\n                'std_word_velocity': 0,\n                'positive_velocity_ratio': 0\n            })\n    \n    return pd.DataFrame(features)\n\n\ndef activity_timing_features(df):\n    \"\"\"How much time is spent on each type of activity\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        # Add up time for each activity\n        input_time = id_df[id_df['activity'] == 'Input']['action_time'].sum()\n        remove_time = id_df[id_df['activity'] == 'Remove/Cut']['action_time'].sum()\n        paste_time = id_df[id_df['activity'] == 'Paste']['action_time'].sum()\n        nonprod_time = id_df[id_df['activity'] == 'Nonproduction']['action_time'].sum()\n        \n        total_time = id_df['action_time'].sum()\n        \n        features.append({\n            'id': id_val,\n            'input_time_total': input_time,\n            'remove_time_total': remove_time,\n            'paste_time_total': paste_time,\n            'nonprod_time_total': nonprod_time,\n            'input_time_ratio': input_time / (total_time + 1),\n            'remove_time_ratio': remove_time / (total_time + 1),\n            'productive_time_ratio': (input_time + paste_time) / (total_time + 1),\n        })\n    \n    return pd.DataFrame(features)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:08.357359Z",
     "start_time": "2025-11-10T14:50:08.348076Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.911198Z",
     "iopub.execute_input": "2025-11-12T05:26:58.911530Z",
     "iopub.status.idle": "2025-11-12T05:26:58.934994Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.911505Z",
     "shell.execute_reply": "2025-11-12T05:26:58.934173Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": "### 2.5 Revision & Cursor Movement Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def revision_pattern_features(df):\n    \"\"\"Features about revision behaviour and editing patterns\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        # Where in the text are they making changes\n        cursor_positions = id_df['cursor_position'].values\n        activities = id_df['activity'].values\n        word_counts = id_df['word_count'].values\n        \n        # Count edits at start, middle, and end\n        revisions_start = 0\n        revisions_middle = 0\n        revisions_end = 0\n        \n        for i, (pos, act, wc) in enumerate(zip(cursor_positions, activities, word_counts)):\n            if act in ['Remove/Cut', 'Replace'] and wc > 0:\n                relative_pos = pos / (wc + 1)\n                if relative_pos < 0.33:\n                    revisions_start += 1\n                elif relative_pos < 0.67:\n                    revisions_middle += 1\n                else:\n                    revisions_end += 1\n        \n        # Look for write-then-edit cycles\n        review_cycles = 0\n        in_writing = False\n        for act in activities:\n            if act == 'Input':\n                in_writing = True\n            elif act in ['Remove/Cut', 'Replace'] and in_writing:\n                review_cycles += 1\n                in_writing = False\n        \n        # How often they go backwards to edit\n        backward_movements = sum(1 for i in range(len(cursor_positions)-1) \n                                if cursor_positions[i+1] < cursor_positions[i])\n        \n        total_revisions = revisions_start + revisions_middle + revisions_end\n        \n        features.append({\n            'id': id_val,\n            'revisions_at_start': revisions_start,\n            'revisions_at_middle': revisions_middle,\n            'revisions_at_end': revisions_end,\n            'total_revisions': total_revisions,\n            'review_cycles': review_cycles,\n            'backward_movements': backward_movements,\n            'early_revision_ratio': revisions_start / (total_revisions + 1),\n            'end_revision_ratio': revisions_end / (total_revisions + 1),\n            'revision_density': total_revisions / (len(id_df) + 1),\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef cursor_movement_features(df):\n    \"\"\"Features about how the cursor moves around\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    df['cursor_jump'] = df.groupby('id')['cursor_position'].diff().abs()\n    \n    features = df.groupby('id').agg(\n        avg_cursor_jump=('cursor_jump', 'mean'),\n        max_cursor_jump=('cursor_jump', 'max'),\n        total_cursor_movement=('cursor_jump', 'sum'),\n        small_cursor_jumps=('cursor_jump', lambda x: (x <= 5).sum()),\n        medium_cursor_jumps=('cursor_jump', lambda x: ((x > 5) & (x <= 50)).sum()),\n        large_cursor_jumps=('cursor_jump', lambda x: (x > 50).sum()),\n        cursor_jump_std=('cursor_jump', 'std'),\n    ).reset_index()\n    \n    # Where is the cursor most of the time\n    cursor_at_end = df.groupby('id').apply(\n        lambda x: (x['cursor_position'] == x['word_count']).sum() / len(x)\n    ).rename('cursor_at_end_ratio')\n    \n    cursor_at_start = df.groupby('id').apply(\n        lambda x: (x['cursor_position'] == 0).sum() / len(x)\n    ).rename('cursor_at_start_ratio')\n    \n    features = features.merge(cursor_at_end, on='id', how='left')\n    features = features.merge(cursor_at_start, on='id', how='left')\n    \n    # Are they mostly writing forwards\n    features['forward_writing_tendency'] = features['cursor_at_end_ratio']\n    features['navigation_complexity'] = features['large_cursor_jumps'] / (features['total_cursor_movement'] + 1)\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:09.419770Z",
     "start_time": "2025-11-10T14:50:09.412353Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.936015Z",
     "iopub.execute_input": "2025-11-12T05:26:58.936306Z",
     "iopub.status.idle": "2025-11-12T05:26:58.956697Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.936285Z",
     "shell.execute_reply": "2025-11-12T05:26:58.955542Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "### 2.6 Rolling Window & Distribution Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def rolling_features(df, window=10):\n    \"\"\"Look at trends over time using a sliding window\"\"\"\n    df = df.sort_values(['id', 'down_time']).copy()\n    \n    features = []\n    for id_val in df['id'].unique():\n        id_df = df[df['id'] == id_val]\n        \n        if len(id_df) < window:\n            features.append({\n                'id': id_val,\n                'action_time_rolling_mean': id_df['action_time'].mean(),\n                'action_time_rolling_std': id_df['action_time'].std(),\n                'word_count_rolling_trend': 0,\n                'action_time_trend': 0,\n                'action_time_acceleration': 0\n            })\n            continue\n        \n        # Calculate moving averages\n        action_rolling = id_df['action_time'].rolling(window=window, min_periods=1)\n        word_rolling = id_df['word_count'].rolling(window=window, min_periods=1)\n        \n        # Are things speeding up or slowing down\n        word_trend = (word_rolling.mean().iloc[-1] - word_rolling.mean().iloc[0]) if len(id_df) >= window else 0\n        action_trend = (action_rolling.mean().iloc[-1] - action_rolling.mean().iloc[0]) if len(id_df) >= window else 0\n        \n        features.append({\n            'id': id_val,\n            'action_time_rolling_mean': action_rolling.mean().mean(),\n            'action_time_rolling_std': action_rolling.std().mean(),\n            'word_count_rolling_trend': word_trend,\n            'action_time_trend': action_trend,\n            'action_time_acceleration': action_rolling.mean().diff().mean()\n        })\n    \n    return pd.DataFrame(features)\n\n\ndef action_time_distribution_features(df):\n    \"\"\"Statistical properties of action times\"\"\"\n    features = df.groupby('id')['action_time'].agg([\n        ('action_time_q25', lambda x: x.quantile(0.25)),\n        ('action_time_q75', lambda x: x.quantile(0.75)),\n        ('action_time_iqr', lambda x: x.quantile(0.75) - x.quantile(0.25)),\n        ('action_time_skew', lambda x: x.skew()),\n        ('action_time_kurtosis', lambda x: x.kurtosis()),\n    ]).reset_index()\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:50:10.162807Z",
     "start_time": "2025-11-10T14:50:10.157263Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.957914Z",
     "iopub.execute_input": "2025-11-12T05:26:58.958269Z",
     "iopub.status.idle": "2025-11-12T05:26:58.985137Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.958235Z",
     "shell.execute_reply": "2025-11-12T05:26:58.984152Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "### 2.7 Advanced Event Timing Features\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Main Feature Builder\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_all_features(df):\n    \"\"\"\n    Main function to build all features from log data\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        Input log data with columns: id, event_id, down_time, up_time, \n        action_time, activity, cursor_position, word_count\n    \n    Returns:\n    --------\n    DataFrame with all extracted features\n    \"\"\"\n    print(\"Building all features...\")\n    \n    # Base features\n    print(\"  - Base features\")\n    features = extract_features(df)\n    \n    # Pause features\n    print(\"  - Pause features\")\n    pause_feats = pause_features(df)\n    for feat in pause_feats:\n        features = features.merge(feat, on=\"id\", how=\"left\")\n    \n    # Burst features\n    print(\"  - Burst features\")\n    burst_feats = burst_features(df)\n    for feat in burst_feats:\n        features = features.merge(feat, on=\"id\", how=\"left\")\n    \n    # P-burst features\n    print(\"  - P-burst features\")\n    p_burst_feat = p_burst_features(df)\n    features = features.merge(p_burst_feat, on=\"id\", how=\"left\")\n    \n    # Activity sequence features\n    print(\"  - Activity sequence features\")\n    activity_seq_feat = activity_sequence_features(df)\n    features = features.merge(activity_seq_feat, on=\"id\", how=\"left\")\n    \n    # Text change features\n    print(\"  - Text change features\")\n    text_feat = text_change_features(df)\n    features = features.merge(text_feat, on=\"id\", how=\"left\")\n    \n    # Time-based features\n    print(\"  - Time-based features\")\n    time_feat = time_based_features(df)\n    features = features.merge(time_feat, on=\"id\", how=\"left\")\n    \n    # Keystroke velocity features\n    print(\"  - Keystroke velocity features\")\n    keystroke_feat = keystroke_velocity_features(df)\n    if not keystroke_feat.empty:\n        features = features.merge(keystroke_feat, on=\"id\", how=\"left\")\n    \n    # Revision pattern features\n    print(\"  - Revision pattern features\")\n    revision_feat = revision_pattern_features(df)\n    features = features.merge(revision_feat, on=\"id\", how=\"left\")\n    \n    # Cursor movement features\n    print(\"  - Cursor movement features\")\n    cursor_feat = cursor_movement_features(df)\n    features = features.merge(cursor_feat, on=\"id\", how=\"left\")\n    \n    # Rolling features\n    print(\"  - Rolling window features\")\n    rolling_feat = rolling_features(df, window=10)\n    features = features.merge(rolling_feat, on=\"id\", how=\"left\")\n    \n    # Action time distribution features\n    print(\"  - Action time distribution features\")\n    action_dist_feat = action_time_distribution_features(df)\n    features = features.merge(action_dist_feat, on=\"id\", how=\"left\")\n    \n    # Word count velocity features\n    print(\"  - Word count velocity features\")\n    word_vel_feat = word_count_velocity_features(df)\n    features = features.merge(word_vel_feat, on=\"id\", how=\"left\")\n    \n    # Activity timing features\n    print(\"  - Activity timing features\")\n    activity_time_feat = activity_timing_features(df)\n    features = features.merge(activity_time_feat, on=\"id\", how=\"left\")\n    \n    # Fill NaN and inf values\n    features = features.fillna(0)\n    features = features.replace([np.inf, -np.inf], 0)\n    \n    print(f\"\\nTotal features extracted: {features.shape[1] - 1}\")  # -1 for id column\n    print(f\"Total samples: {features.shape[0]}\")\n    \n    return features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:05.132243Z",
     "start_time": "2025-11-10T14:51:05.126243Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:58.986341Z",
     "iopub.execute_input": "2025-11-12T05:26:58.986782Z",
     "iopub.status.idle": "2025-11-12T05:26:59.012876Z",
     "shell.execute_reply.started": "2025-11-12T05:26:58.986748Z",
     "shell.execute_reply": "2025-11-12T05:26:59.011873Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Load Data & Extract Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load cleaned training logs\n\n\nlogs = df\nprint(f\"Loaded {len(logs)} rows\")\nprint(f\"Unique IDs: {logs['id'].nunique()}\")\nprint(f\"\\nColumns: {list(logs.columns)}\")\nlogs.head()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:05.742008Z",
     "start_time": "2025-11-10T14:51:05.734947Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:59.013963Z",
     "iopub.execute_input": "2025-11-12T05:26:59.014304Z",
     "iopub.status.idle": "2025-11-12T05:26:59.066697Z",
     "shell.execute_reply.started": "2025-11-12T05:26:59.014275Z",
     "shell.execute_reply": "2025-11-12T05:26:59.065727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loaded 6 rows\nUnique IDs: 3\n\nColumns: ['id', 'event_id', 'down_time', 'up_time', 'action_time', 'activity', 'down_event', 'up_event', 'text_change', 'cursor_position', 'word_count', 'id_encoded']\n",
     "output_type": "stream"
    },
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id  event_id  down_time  up_time  action_time activity down_event  \\\n0  0000aaaa         1          0       85           85    Input      Space   \n1  0000aaaa         2      60000    60087           87    Input      Space   \n2  2222bbbb         1          0       67           67    Input          q   \n3  2222bbbb         2          0       46           46    Input          q   \n4  4444cccc         1          0       94           94    Input      Space   \n\n  up_event text_change  cursor_position  word_count  id_encoded  \n0    Space                            0           0           0  \n1    Space                            1           0           0  \n2        q           q                0           1           1  \n3        q           q                1           1           1  \n4    Space                            0           0           2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>event_id</th>\n      <th>down_time</th>\n      <th>up_time</th>\n      <th>action_time</th>\n      <th>activity</th>\n      <th>down_event</th>\n      <th>up_event</th>\n      <th>text_change</th>\n      <th>cursor_position</th>\n      <th>word_count</th>\n      <th>id_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1</td>\n      <td>0</td>\n      <td>85</td>\n      <td>85</td>\n      <td>Input</td>\n      <td>Space</td>\n      <td>Space</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000aaaa</td>\n      <td>2</td>\n      <td>60000</td>\n      <td>60087</td>\n      <td>87</td>\n      <td>Input</td>\n      <td>Space</td>\n      <td>Space</td>\n      <td></td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2222bbbb</td>\n      <td>1</td>\n      <td>0</td>\n      <td>67</td>\n      <td>67</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2222bbbb</td>\n      <td>2</td>\n      <td>0</td>\n      <td>46</td>\n      <td>46</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4444cccc</td>\n      <td>1</td>\n      <td>0</td>\n      <td>94</td>\n      <td>94</td>\n      <td>Input</td>\n      <td>Space</td>\n      <td>Space</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "# Extract all behavioural features\nBehavioral_features_temp = build_all_features(logs)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:06.212726Z",
     "start_time": "2025-11-10T14:51:06.148141Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:59.067619Z",
     "iopub.execute_input": "2025-11-12T05:26:59.067906Z",
     "iopub.status.idle": "2025-11-12T05:26:59.262986Z",
     "shell.execute_reply.started": "2025-11-12T05:26:59.067884Z",
     "shell.execute_reply": "2025-11-12T05:26:59.261988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Building all features...\n  - Base features\n  - Pause features\n  - Burst features\n  - P-burst features\n  - Activity sequence features\n  - Text change features\n  - Time-based features\n  - Keystroke velocity features\n  - Revision pattern features\n  - Cursor movement features\n  - Rolling window features\n  - Action time distribution features\n  - Word count velocity features\n  - Activity timing features\n\nTotal features extracted: 122\nTotal samples: 3\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Inspect Results\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display first few rows\nprint(f\"Feature matrix shape: {Behavioral_features_temp.shape}\")\nprint(f\"\\nFeature names ({len(Behavioral_features_temp.columns)} total):\")\nprint(list(Behavioral_features_temp.columns))\nBehavioral_features_temp.head()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:07.166937Z",
     "start_time": "2025-11-10T14:51:07.157077Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:59.264063Z",
     "iopub.execute_input": "2025-11-12T05:26:59.264368Z",
     "iopub.status.idle": "2025-11-12T05:26:59.286893Z",
     "shell.execute_reply.started": "2025-11-12T05:26:59.264339Z",
     "shell.execute_reply": "2025-11-12T05:26:59.285864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Feature matrix shape: (3, 123)\n\nFeature names (123 total):\n['id', 'events_count', 'total_time', 'total_chars', 'mean_action_time', 'std_action_time', 'max_action_time', 'min_action_time', 'backspace_count', 'paste_count', 'input_count', 'move_count', 'replace_count', 'nonproduction_count', 'cursor_pos_mean', 'cursor_pos_std', 'cursor_pos_max', 'word_count_mean', 'word_count_std', 'word_count_diff', 'chars_per_min', 'events_per_min', 'backspace_ratio', 'paste_ratio', 'replace_ratio', 'nonproduction_ratio', 'revision_ratio', 'pause_2s_count', 'pause_5s_count', 'pause_10s_count', 'mean_pause', 'median_pause', 'std_pause', 'max_pause', 'min_pause', 'avg_burst', 'max_burst', 'std_burst', 'avg_words_per_p_burst', 'input_to_remove_trans', 'remove_to_input_trans', 'input_to_input_trans', 'paste_to_input_trans', 'max_input_streak', 'max_remove_streak', 'unique_activities', 'activity_switches', 'activity_switch_rate', 'total_text_produced', 'total_text_removed', 'text_production_rate', 'text_removal_rate', 'max_text_addition', 'max_text_removal', 'text_volatility', 'positive_text_changes', 'negative_text_changes', 'text_removal_ratio', 'net_text_production', 'text_efficiency', 'early_events', 'middle_events', 'late_events', 'early_input_ratio', 'middle_input_ratio', 'late_input_ratio', 'early_remove_ratio', 'late_remove_ratio', 'middle_paste_ratio', 'late_phase_activity', 'input_iki_mean', 'input_iki_std', 'input_iki_median', 'input_iki_min', 'input_iki_max', 'fast_keystrokes', 'moderate_keystrokes', 'slow_keystrokes', 'keystroke_consistency', 'fast_keystroke_ratio', 'typing_rhythm_score', 'revisions_at_start', 'revisions_at_middle', 'revisions_at_end', 'total_revisions', 'review_cycles', 'backward_movements', 'early_revision_ratio', 'end_revision_ratio', 'revision_density', 'avg_cursor_jump', 'max_cursor_jump', 'total_cursor_movement', 'small_cursor_jumps', 'medium_cursor_jumps', 'large_cursor_jumps', 'cursor_jump_std', 'cursor_at_end_ratio', 'cursor_at_start_ratio', 'forward_writing_tendency', 'navigation_complexity', 'action_time_rolling_mean', 'action_time_rolling_std', 'word_count_rolling_trend', 'action_time_trend', 'action_time_acceleration', 'action_time_q25', 'action_time_q75', 'action_time_iqr', 'action_time_skew', 'action_time_kurtosis', 'avg_word_velocity', 'max_word_velocity', 'min_word_velocity', 'std_word_velocity', 'positive_velocity_ratio', 'input_time_total', 'remove_time_total', 'paste_time_total', 'nonprod_time_total', 'input_time_ratio', 'remove_time_ratio', 'productive_time_ratio']\n",
     "output_type": "stream"
    },
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id  events_count  total_time  total_chars  mean_action_time  \\\n0  0000aaaa             2       60087            0              86.0   \n1  2222bbbb             2          67            1              56.5   \n2  4444cccc             2          94            1              75.0   \n\n   std_action_time  max_action_time  min_action_time  backspace_count  \\\n0         1.414214               87               85                0   \n1        14.849242               67               46                0   \n2        26.870058               94               56                0   \n\n   paste_count  ...  min_word_velocity  std_word_velocity  \\\n0            0  ...                0.0                0.0   \n1            0  ...                0.0                0.0   \n2            0  ...                1.0                0.0   \n\n   positive_velocity_ratio  input_time_total  remove_time_total  \\\n0                      0.0               172                  0   \n1                      0.0               113                  0   \n2                      1.0               150                  0   \n\n   paste_time_total  nonprod_time_total  input_time_ratio  remove_time_ratio  \\\n0                 0                   0          0.994220                0.0   \n1                 0                   0          0.991228                0.0   \n2                 0                   0          0.993377                0.0   \n\n   productive_time_ratio  \n0               0.994220  \n1               0.991228  \n2               0.993377  \n\n[3 rows x 123 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>events_count</th>\n      <th>total_time</th>\n      <th>total_chars</th>\n      <th>mean_action_time</th>\n      <th>std_action_time</th>\n      <th>max_action_time</th>\n      <th>min_action_time</th>\n      <th>backspace_count</th>\n      <th>paste_count</th>\n      <th>...</th>\n      <th>min_word_velocity</th>\n      <th>std_word_velocity</th>\n      <th>positive_velocity_ratio</th>\n      <th>input_time_total</th>\n      <th>remove_time_total</th>\n      <th>paste_time_total</th>\n      <th>nonprod_time_total</th>\n      <th>input_time_ratio</th>\n      <th>remove_time_ratio</th>\n      <th>productive_time_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2</td>\n      <td>60087</td>\n      <td>0</td>\n      <td>86.0</td>\n      <td>1.414214</td>\n      <td>87</td>\n      <td>85</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.994220</td>\n      <td>0.0</td>\n      <td>0.994220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>2</td>\n      <td>67</td>\n      <td>1</td>\n      <td>56.5</td>\n      <td>14.849242</td>\n      <td>67</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>113</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.991228</td>\n      <td>0.0</td>\n      <td>0.991228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>2</td>\n      <td>94</td>\n      <td>1</td>\n      <td>75.0</td>\n      <td>26.870058</td>\n      <td>94</td>\n      <td>56</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.993377</td>\n      <td>0.0</td>\n      <td>0.993377</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 123 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "# Check for any issues\nprint(\"Missing values per column:\")\nprint(Behavioral_features_temp.isnull().sum().sum())\nprint(\"\\nInfinite values per column:\")\nprint(np.isinf(Behavioral_features_temp.select_dtypes(include=[np.number])).sum().sum())\nprint(\"\\nBasic statistics:\")\nBehavioral_features_temp.describe().T",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:07.752554Z",
     "start_time": "2025-11-10T14:51:07.685770Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:59.288897Z",
     "iopub.execute_input": "2025-11-12T05:26:59.289269Z",
     "iopub.status.idle": "2025-11-12T05:26:59.474559Z",
     "shell.execute_reply.started": "2025-11-12T05:26:59.289241Z",
     "shell.execute_reply": "2025-11-12T05:26:59.473304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Missing values per column:\n0\n\nInfinite values per column:\n0\n\nBasic statistics:\n",
     "output_type": "stream"
    },
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       count          mean           std        min  \\\nevents_count             3.0      2.000000      0.000000   2.000000   \ntotal_time               3.0  20082.666667  34644.771558  67.000000   \ntotal_chars              3.0      0.666667      0.577350   0.000000   \nmean_action_time         3.0     72.500000     14.908052  56.500000   \nstd_action_time          3.0     14.377838     12.734468   1.414214   \n...                      ...           ...           ...        ...   \npaste_time_total         3.0      0.000000      0.000000   0.000000   \nnonprod_time_total       3.0      0.000000      0.000000   0.000000   \ninput_time_ratio         3.0      0.992942      0.001543   0.991228   \nremove_time_ratio        3.0      0.000000      0.000000   0.000000   \nproductive_time_ratio    3.0      0.992942      0.001543   0.991228   \n\n                             25%        50%           75%           max  \nevents_count            2.000000   2.000000      2.000000      2.000000  \ntotal_time             80.500000  94.000000  30090.500000  60087.000000  \ntotal_chars             0.500000   1.000000      1.000000      1.000000  \nmean_action_time       65.750000  75.000000     80.500000     86.000000  \nstd_action_time         8.131728  14.849242     20.859650     26.870058  \n...                          ...        ...           ...           ...  \npaste_time_total        0.000000   0.000000      0.000000      0.000000  \nnonprod_time_total      0.000000   0.000000      0.000000      0.000000  \ninput_time_ratio        0.992303   0.993377      0.993799      0.994220  \nremove_time_ratio       0.000000   0.000000      0.000000      0.000000  \nproductive_time_ratio   0.992303   0.993377      0.993799      0.994220  \n\n[122 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>events_count</th>\n      <td>3.0</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>total_time</th>\n      <td>3.0</td>\n      <td>20082.666667</td>\n      <td>34644.771558</td>\n      <td>67.000000</td>\n      <td>80.500000</td>\n      <td>94.000000</td>\n      <td>30090.500000</td>\n      <td>60087.000000</td>\n    </tr>\n    <tr>\n      <th>total_chars</th>\n      <td>3.0</td>\n      <td>0.666667</td>\n      <td>0.577350</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>mean_action_time</th>\n      <td>3.0</td>\n      <td>72.500000</td>\n      <td>14.908052</td>\n      <td>56.500000</td>\n      <td>65.750000</td>\n      <td>75.000000</td>\n      <td>80.500000</td>\n      <td>86.000000</td>\n    </tr>\n    <tr>\n      <th>std_action_time</th>\n      <td>3.0</td>\n      <td>14.377838</td>\n      <td>12.734468</td>\n      <td>1.414214</td>\n      <td>8.131728</td>\n      <td>14.849242</td>\n      <td>20.859650</td>\n      <td>26.870058</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>paste_time_total</th>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>nonprod_time_total</th>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>input_time_ratio</th>\n      <td>3.0</td>\n      <td>0.992942</td>\n      <td>0.001543</td>\n      <td>0.991228</td>\n      <td>0.992303</td>\n      <td>0.993377</td>\n      <td>0.993799</td>\n      <td>0.994220</td>\n    </tr>\n    <tr>\n      <th>remove_time_ratio</th>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>productive_time_ratio</th>\n      <td>3.0</td>\n      <td>0.992942</td>\n      <td>0.001543</td>\n      <td>0.991228</td>\n      <td>0.992303</td>\n      <td>0.993377</td>\n      <td>0.993799</td>\n      <td>0.994220</td>\n    </tr>\n  </tbody>\n</table>\n<p>122 rows × 8 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Save Features\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "Behavioural_features = Behavioral_features_temp\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:09.642316Z",
     "start_time": "2025-11-10T14:51:09.639314Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:59.476183Z",
     "iopub.execute_input": "2025-11-12T05:26:59.476474Z",
     "iopub.status.idle": "2025-11-12T05:26:59.483099Z",
     "shell.execute_reply.started": "2025-11-12T05:26:59.476429Z",
     "shell.execute_reply": "2025-11-12T05:26:59.481182Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis notebook extracts **comprehensive behavioural features** from keystroke logging data. The features capture:\n\n### Feature Categories (150+ features total):\n\n1. **Base Features**: Event counts, total time, typing speed, activity ratios\n2. **Pause Features**: Gaps between keystrokes at different thresholds (2s, 5s, 10s)\n3. **Burst Features**: When they're typing continuously and how fluently\n4. **Activity Sequence**: How activities transition from one to another, streaks, variety\n5. **Text Change**: How fast they produce/remove text, editing efficiency\n6. **Temporal Patterns**: What they do in early/middle/late stages\n7. **Keystroke Velocity**: Typing speed variations, rhythm, consistency\n8. **Word Count Velocity**: How the word count changes over time\n9. **Activity Timing**: How much time on each type of activity\n10. **Revision Patterns**: Where they edit, review cycles, going backwards\n11. **Cursor Movement**: How they navigate around, jump distances\n12. **Rolling Window**: Trends and changes in typing behaviour\n13. **Distribution Features**: Statistical properties (skew, kurtosis, IQR)\n\n### Output:\n\n- `data/train_behaviour_features.csv` - One row per essay ID with all behavioural features\n\n### Next Steps:\n\n- Combine with text features from `FeatureExtraction_Essay.ipynb`\n- Merge with TF-IDF/SVD features from `tfidf/tfidf.ipynb`\n- Build predictive models using these features\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "------------\n\n### Essay Text Feature",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# reuse code from text_process\nimport sys\nsys.path.append('..')\n\nimport numpy as np\n\ndef Text_Feature_Extraction(extracted_text):\n    features = extracted_text.drop('text',axis=1)\n    texts = extracted_text['text']\n    processor = TextProcessor()\n    for i in range(0,texts.shape[0]):\n        words = processor.split_to_word(texts[i])\n        sentences = processor.split_to_sentence(texts[i])\n        word_lengths = [len(w) for w in words]\n        sent_lengths = [len(processor.split_to_word(s)) for s in sentences]\n        \n        features.loc[i,'word_count'] = len(word_lengths)\n        if len(word_lengths) > 0:\n            features.loc[i,'word_length_mean'] = sum(word_lengths)/len(word_lengths)\n            features.loc[i,'word_length_std'] = pd.Series(word_lengths).std()\n        else:\n            features.loc[i,'word_length_mean'] = 0\n            features.loc[i,'word_length_std'] = 0\n        \n        if len(sent_lengths) > 0:\n            features.loc[i,'sent_length_mean'] = sum(sent_lengths)/len(sent_lengths)\n            features.loc[i,'sent_length_std'] = pd.Series(sent_lengths).std()\n        else:\n            features.loc[i,'sent_length_mean'] = 0\n            features.loc[i,'sent_length_std'] = 0\n    return features\n\nText_Essay_Features = Text_Feature_Extraction(extracted_text)\nTrain_Text_Essay_Features = Text_Feature_Extraction(extracted_text_train)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:43.512767Z",
     "start_time": "2025-11-10T14:51:41.840677Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:26:59.484813Z",
     "iopub.execute_input": "2025-11-12T05:26:59.485196Z",
     "iopub.status.idle": "2025-11-12T05:27:03.158251Z",
     "shell.execute_reply.started": "2025-11-12T05:26:59.485164Z",
     "shell.execute_reply": "2025-11-12T05:27:03.157296Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "Text_Essay_Features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:49.613375Z",
     "start_time": "2025-11-10T14:51:49.607249Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:03.162166Z",
     "iopub.execute_input": "2025-11-12T05:27:03.162741Z",
     "iopub.status.idle": "2025-11-12T05:27:03.175432Z",
     "shell.execute_reply.started": "2025-11-12T05:27:03.162710Z",
     "shell.execute_reply": "2025-11-12T05:27:03.174557Z"
    }
   },
   "outputs": [
    {
     "execution_count": 22,
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id  len_text  sentence_count  paragraph_count  word_count  \\\n0  0000aaaa         2               0                0         0.0   \n1  2222bbbb         2               1                1         1.0   \n2  4444cccc         2               1                1         1.0   \n\n   word_length_mean  word_length_std  sent_length_mean  sent_length_std  \n0               0.0              0.0               0.0              0.0  \n1               2.0              NaN               1.0              NaN  \n2               1.0              NaN               1.0              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>len_text</th>\n      <th>sentence_count</th>\n      <th>paragraph_count</th>\n      <th>word_count</th>\n      <th>word_length_mean</th>\n      <th>word_length_std</th>\n      <th>sent_length_mean</th>\n      <th>sent_length_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2222bbbb</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444cccc</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "Train_Text_Essay_Features",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:51:55.006997Z",
     "start_time": "2025-11-10T14:51:54.998996Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:03.179606Z",
     "iopub.execute_input": "2025-11-12T05:27:03.179862Z",
     "iopub.status.idle": "2025-11-12T05:27:03.204287Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [
    {
     "execution_count": 23,
     "output_type": "execute_result",
     "data": {
      "text/plain": "            id  len_text  sentence_count  paragraph_count  word_count  \\\n0     001519c8      1528              14                3       256.0   \n1     0022f953      1675              16                1       330.0   \n2     0042269b      2587              19                6       408.0   \n3     0059420b      1154              13                1       208.0   \n4     0075873a      1425              16                5       255.0   \n...        ...       ...             ...              ...         ...   \n2466  ffb8c745      1634              13                1       276.0   \n2467  ffbef7e5      2335              30                6       444.0   \n2468  ffccd6fd      2761               4                3       203.0   \n2469  ffec5b38      2552              27                1       417.0   \n2470  fff05981      1492              11                5       244.0   \n\n      word_length_mean  word_length_std  sent_length_mean  sent_length_std  \n0             4.835938         2.472682         18.285714         6.497675  \n1             3.900000         2.140718         20.625000        13.608208  \n2             5.254902         2.743538         21.473684         5.263801  \n3             4.485577         2.633816         16.000000         6.493587  \n4             4.450980         2.437516         15.937500         8.667708  \n...                ...              ...               ...              ...  \n2466          4.717391         2.606256         21.230769         6.622611  \n2467          4.155405         2.163872         14.800000         7.526505  \n2468          4.497537         2.464349         50.750000        10.719919  \n2469          4.997602         2.915887         15.444444         5.631869  \n2470          5.049180         2.944906         22.181818        17.040060  \n\n[2471 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>len_text</th>\n      <th>sentence_count</th>\n      <th>paragraph_count</th>\n      <th>word_count</th>\n      <th>word_length_mean</th>\n      <th>word_length_std</th>\n      <th>sent_length_mean</th>\n      <th>sent_length_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001519c8</td>\n      <td>1528</td>\n      <td>14</td>\n      <td>3</td>\n      <td>256.0</td>\n      <td>4.835938</td>\n      <td>2.472682</td>\n      <td>18.285714</td>\n      <td>6.497675</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022f953</td>\n      <td>1675</td>\n      <td>16</td>\n      <td>1</td>\n      <td>330.0</td>\n      <td>3.900000</td>\n      <td>2.140718</td>\n      <td>20.625000</td>\n      <td>13.608208</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0042269b</td>\n      <td>2587</td>\n      <td>19</td>\n      <td>6</td>\n      <td>408.0</td>\n      <td>5.254902</td>\n      <td>2.743538</td>\n      <td>21.473684</td>\n      <td>5.263801</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0059420b</td>\n      <td>1154</td>\n      <td>13</td>\n      <td>1</td>\n      <td>208.0</td>\n      <td>4.485577</td>\n      <td>2.633816</td>\n      <td>16.000000</td>\n      <td>6.493587</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0075873a</td>\n      <td>1425</td>\n      <td>16</td>\n      <td>5</td>\n      <td>255.0</td>\n      <td>4.450980</td>\n      <td>2.437516</td>\n      <td>15.937500</td>\n      <td>8.667708</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2466</th>\n      <td>ffb8c745</td>\n      <td>1634</td>\n      <td>13</td>\n      <td>1</td>\n      <td>276.0</td>\n      <td>4.717391</td>\n      <td>2.606256</td>\n      <td>21.230769</td>\n      <td>6.622611</td>\n    </tr>\n    <tr>\n      <th>2467</th>\n      <td>ffbef7e5</td>\n      <td>2335</td>\n      <td>30</td>\n      <td>6</td>\n      <td>444.0</td>\n      <td>4.155405</td>\n      <td>2.163872</td>\n      <td>14.800000</td>\n      <td>7.526505</td>\n    </tr>\n    <tr>\n      <th>2468</th>\n      <td>ffccd6fd</td>\n      <td>2761</td>\n      <td>4</td>\n      <td>3</td>\n      <td>203.0</td>\n      <td>4.497537</td>\n      <td>2.464349</td>\n      <td>50.750000</td>\n      <td>10.719919</td>\n    </tr>\n    <tr>\n      <th>2469</th>\n      <td>ffec5b38</td>\n      <td>2552</td>\n      <td>27</td>\n      <td>1</td>\n      <td>417.0</td>\n      <td>4.997602</td>\n      <td>2.915887</td>\n      <td>15.444444</td>\n      <td>5.631869</td>\n    </tr>\n    <tr>\n      <th>2470</th>\n      <td>fff05981</td>\n      <td>1492</td>\n      <td>11</td>\n      <td>5</td>\n      <td>244.0</td>\n      <td>5.049180</td>\n      <td>2.944906</td>\n      <td>22.181818</td>\n      <td>17.040060</td>\n    </tr>\n  </tbody>\n</table>\n<p>2471 rows × 9 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---------\n### TF-IDF Feature Extraction\n\n### Works taken from `texts_tfidf.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pickle\nfrom sklearn.decomposition import TruncatedSVD\n\n\n\ntexts_train = extracted_text_train[['id', 'text']]\ntexts_test = extracted_text[['id', 'text']]\n\n\n# refactor version, completely separate train and test\n\n\n# only fit on train\nvectorizer = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 5),\n    max_features=30000,\n    dtype=np.float32,\n)\nX_train_tfidf = vectorizer.fit_transform(texts_train['text'])\n\nn_features = X_train_tfidf.shape[1]\nsvdsize = min(64, max(1, n_features - 1))\n\nsvd = TruncatedSVD(\n    n_components=svdsize,\n    random_state=42,\n    n_iter=7\n)\nX_train_svd = svd.fit_transform(X_train_tfidf)\n\n\n# train_svd_df = pd.DataFrame(\n#     X_train_svd,\n#     columns=[f'{i:02d}' for i in range(svdsize)]\n# )\n# train_svd_df.insert(0, 'id', texts_train['id'].values)\n# train_svd_df.to_csv(\"/data/train_tfidf_svd.csv\", index=False)\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:21.120054Z",
     "start_time": "2025-11-10T15:36:16.632442Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:03.205138Z",
     "iopub.execute_input": "2025-11-12T05:27:03.205378Z",
     "iopub.status.idle": "2025-11-12T05:27:11.695748Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X_test_tfidf = vectorizer.transform(texts_test['text'])\nX_test_svd = svd.transform(X_test_tfidf)\n\n\nsvdsize = X_test_svd.shape[1]\ntest_svd_df = pd.DataFrame(\n    X_test_svd,\n    columns=[f'{i:02d}' for i in range(svdsize)]\n)\ntest_svd_df.insert(0, 'id', texts_test['id'].values)\nTFIDF_Features = test_svd_df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:21.756745Z",
     "start_time": "2025-11-10T15:36:21.752382Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:11.696485Z",
     "iopub.execute_input": "2025-11-12T05:27:11.696742Z",
     "iopub.status.idle": "2025-11-12T05:27:11.706681Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Works taken from `operation_tfidf.ipynb` ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === TRAIN ===\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n\nactivity_df = train_df[['id', 'activity']]\n\nprint(train_df.head(3))\n\n\ndef rebuild_text(grp):\n    buf = []\n    for op in grp['activity']:\n        buf.append(op[0])\n    return \"\".join(buf)\n\noperations = (\n    activity_df.groupby('id')\n               .apply(rebuild_text)\n               .reset_index(name='operation')\n)\n\nprint(operations.head(3))\n\n# 3) TF-IDF fit on train\nvectorizer = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 5),\n    max_features=30000,\n    dtype=np.float32,\n)\nX_tfidf = vectorizer.fit_transform(operations['operation'])\n\n\n# 4) SVD fit on train\nn_features = X_tfidf.shape[1]\nsvdsize = min(64, n_features - 1)\n\nsvd = TruncatedSVD(\n    n_components=svdsize,\n    random_state=42,\n    n_iter=7\n)\nX_svd = svd.fit_transform(X_tfidf)\n\nsvd_df = pd.DataFrame(\n    X_svd,\n    columns=[f'{i:02d}' for i in range(svdsize)]\n)\nsvd_df.insert(0, 'id', operations['id'].values)\n\nprint(svd_df.head())\ntrain_svd_df_operation = svd_df\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:30.735456Z",
     "start_time": "2025-11-10T15:36:23.251444Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:11.708932Z",
     "iopub.execute_input": "2025-11-12T05:27:11.709477Z",
     "iopub.status.idle": "2025-11-12T05:27:24.369307Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "         id  event_id  down_time  up_time  action_time activity down_event  \\\n0  001519c8         1      60147    60238           91    Input          q   \n1  001519c8         2      60657    60784          127    Input          q   \n2  001519c8         3      60757    60861          104    Input          q   \n\n  up_event text_change  cursor_position  word_count  id_encoded  \n0        q           q                1           1           0  \n1        q           q                2           1           0  \n2        q           q                3           1           0  \n         id                                          operation\n0  001519c8  IIIIIIIIIIIIIIIRIIIIIIIIIIIIIIRIIIIIIIIIIIIIII...\n1  0022f953  IIIIIIIIIIIIIIIIIIIRRRRRRRRRRRIIIIIIIIIIIIIIRR...\n2  0042269b  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...\n         id        00        01        02        03        04        05  \\\n0  001519c8  0.997578  0.063804  0.024434 -0.005175  0.003506 -0.002932   \n1  0022f953  0.999739 -0.007825  0.020793 -0.003091  0.001099 -0.000876   \n2  0042269b  0.999841 -0.009007 -0.015063  0.000017  0.001906 -0.001066   \n3  0059420b  0.998693 -0.023286  0.040242  0.001943 -0.018245 -0.007376   \n4  0075873a  0.990940  0.125214 -0.048481 -0.000406 -0.002281  0.000420   \n\n         06        07        08  ...            54        55        56  \\\n0 -0.000140 -0.001695 -0.000111  ... -4.467778e-04  0.003221 -0.000051   \n1  0.003140 -0.000388 -0.000878  ... -4.081614e-05  0.000022  0.000068   \n2 -0.000430  0.000686  0.001471  ...  1.130626e-06 -0.000003 -0.000014   \n3 -0.000376 -0.000458 -0.000934  ...  6.879382e-04  0.000287 -0.000122   \n4 -0.000107  0.000848 -0.000552  ... -3.743917e-07  0.000004 -0.000024   \n\n         57        58        59        60            61        62        63  \n0  0.001064  0.000748 -0.000045 -0.000242  4.258873e-04 -0.000215 -0.000825  \n1  0.000071  0.000065  0.000114  0.000172 -8.241925e-05 -0.000044  0.000059  \n2 -0.000027  0.000011 -0.000001  0.000004  2.542511e-07 -0.000011  0.000007  \n3 -0.000133 -0.000951  0.000892 -0.000046  4.150365e-04  0.000187  0.000181  \n4 -0.000004 -0.000009 -0.000004 -0.000057  2.270564e-05  0.000004 -0.000027  \n\n[5 rows x 65 columns]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# rebuild raw operation texts\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# === TEST ===\nimport pandas as pd\nimport numpy as np\nimport pickle\n\n\n\ntest_df_activity = df[['id', 'activity']]\n\n\n\ntest_operations = (\n    test_df_activity.groupby('id')\n           .apply(rebuild_text)\n           .reset_index(name='operation')\n)\n\n# 4) only transform, do not fit on tests\nX_test_tfidf = vectorizer.transform(test_operations['operation'])\nX_test_svd   = svd.transform(X_test_tfidf)\n\nsvdsize = X_test_svd.shape[1]\ntest_svd_df = pd.DataFrame(\n    X_test_svd,\n    columns=[f'{i:02d}' for i in range(svdsize)]\n)\ntest_svd_df.insert(0, 'id', test_operations['id'].values)\nTFIDF_Features_Operations = test_svd_df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:30.742619Z",
     "start_time": "2025-11-10T15:36:30.736461Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:24.370260Z",
     "iopub.execute_input": "2025-11-12T05:27:24.370661Z",
     "iopub.status.idle": "2025-11-12T05:27:24.387311Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Concat Data and send to model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ndef merge_preprocessed_data(data_dir='data'):\n    dataset_behaviour = Behavioural_features\n    dataset_text = Text_Essay_Features\n    dataset_tfidf_text = TFIDF_Features\n    dataset_tfidf_operation = TFIDF_Features_Operations\n\n    # merge on 'id'\n    merged = dataset_behaviour.merge(dataset_text, on='id', how='inner')\n\n    # rename column name\n    tfidf_text_renamed = dataset_tfidf_text.rename(\n        columns={col: f'tfidf_text_{col}' if col != 'id' else col\n                 for col in dataset_tfidf_text.columns}\n    )\n    tfidf_operation_renamed = dataset_tfidf_operation.rename(\n        columns={col: f'tfidf_operation_{col}' if col != 'id' else col\n                 for col in dataset_tfidf_operation.columns}\n    )\n\n    merged = merged.merge(tfidf_text_renamed, on='id', how='inner')\n    merged = merged.merge(tfidf_operation_renamed, on='id', how='inner')\n\n\n    return merged\n\n\nif __name__ == '__main__':\n    merged_df = merge_preprocessed_data(\"data\")\n    ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:36:33.551416Z",
     "start_time": "2025-11-10T15:36:33.543416Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:24.389965Z",
     "iopub.execute_input": "2025-11-12T05:27:24.391094Z",
     "iopub.status.idle": "2025-11-12T05:27:24.415659Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Test Submission only",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# === TEST ONLY: load model(s) -> predict on test -> write submission ===\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ---------------- paths ----------------\n",
    "sub_path  = 'submission.csv'\n",
    "\n",
    "# ---------------- load test ----------------\n",
    "df_test = merged_df\n",
    "test_ids = df_test[\"id\"].values\n",
    "X_test = df_test.drop(columns=[\"id\"])\n",
    "\n",
    "\n",
    "model_file_candidate = \"stacking_model.pkl.pkl\"\n",
    "\n",
    "\n",
    "model_obj = None\n",
    "if os.path.exists(model_file_candidate):\n",
    "    model_obj = joblib.load(model_file_candidate)\n",
    "    print(f\"Loaded model from: {model_file_candidate}\")\n",
    "if model_obj is None:\n",
    "    raise FileNotFoundError(\"No saved model found. Expected one of: \" + \", \".join(model_file_candidate))\n",
    "\n",
    "def predict_with_model_obj(model_obj, X):\n",
    "    # Multiple Fold Model\n",
    "    if isinstance(model_obj, (list, tuple)):\n",
    "        preds = np.mean([m.predict(X) for m in model_obj], axis=0)\n",
    "        return preds\n",
    "    # Single Model\n",
    "    return model_obj.predict(X)\n",
    "\n",
    "test_preds = predict_with_model_obj(model_obj,X_test)\n",
    "\n",
    "# ---------------- write submission ----------------\n",
    "submission = pd.DataFrame({\"id\": test_ids, \"score\": test_preds})\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(f\"Submission saved to: {sub_path}\\nSubmission Time {time.asctime( time.localtime(time.time()) )}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:37:47.828246Z",
     "start_time": "2025-11-10T15:37:47.809318Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-12T05:27:24.417928Z",
     "iopub.execute_input": "2025-11-12T05:27:24.418217Z",
     "iopub.status.idle": "2025-11-12T05:27:31.235722Z",
     "execution_failed": "2025-11-12T06:07:36.317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loaded model from: /kaggle/input/lgbm-ensemble/scikitlearn/default/1/lgbm.pkl\nSubmission saved to: /kaggle/working/submission.csv\nSubmission Time Wed Nov 12 05:27:31 2025\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
