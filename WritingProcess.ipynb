{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f76a021a4706679",
   "metadata": {},
   "source": [
    "# Integrated Notebook for Task WritingProcess\n",
    "## Data Preprocessing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---------\n",
    "### Idle Removing and Time Regularization from `preprocess.py`"
   ],
   "id": "74f1e5af8a395b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:22.126138Z",
     "start_time": "2025-11-10T17:52:22.122851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ftfy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "cc472e698188990a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:22.663263Z",
     "start_time": "2025-11-10T17:52:22.660264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## Temp Code to fix Unicode, so dont need to do it in live\n",
    "# train = pd.read_csv('data/train_logs.csv')\n",
    "# cols = ['down_event', 'up_event', 'text_change']\n",
    "# train.loc[:, cols] = train.loc[:, cols].apply(\n",
    "#             lambda s: s.astype('string').map(lambda x: ftfy.fix_text(x) if x is not pd.NA else x)\n",
    "#         )\n",
    "# train.to_csv('data/train_logs_raw_unicode_fixed.csv', index=False)"
   ],
   "id": "f686f592e6c62d70",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:23.105152Z",
     "start_time": "2025-11-10T17:52:23.097890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Preprocess:\n",
    "\n",
    "    def label_encoding(self, df, col=\"id\"):\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(df[col])\n",
    "        df[col + \"_encoded\"] = label_encoder.transform(df[col])\n",
    "        return df\n",
    "\n",
    "    # remove time that the author havent start writing or is resting\n",
    "    # reference: remove_margin for https://www.kaggle.com/code/tomooinubushi/1st-place-solution-training-and-inference-code\n",
    "\n",
    "    def remove_start_and_end_time(\n",
    "        self, df, start_margin=2 * 60 * 1000, end_margin=2 * 60 * 1000\n",
    "    ):\n",
    "        df = df[df[\"up_event\"] != \"Unidentified\"].reset_index(drop=True)\n",
    "        result_df = []\n",
    "        grouped_df = df.groupby(\"id_encoded\")\n",
    "\n",
    "        for _, log in tqdm(grouped_df):\n",
    "            valid_events = log[\n",
    "                (log.activity != \"Nonproduction\")\n",
    "                | (log.up_event != \"Shift\")\n",
    "                | (log.up_event != \"CapsLock\")\n",
    "            ].down_time.values\n",
    "            if len(valid_events) == 0:\n",
    "                continue\n",
    "            log = log[\n",
    "                (log.down_time > valid_events.min() - start_margin)\n",
    "                & (log[\"down_time\"] <= valid_events.max() + end_margin)\n",
    "            ].copy()\n",
    "            log[\"event_id\"] = range(len(log))\n",
    "            result_df.append(log)\n",
    "\n",
    "        result = pd.concat(result_df, ignore_index=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def remove_rest_time(\n",
    "        self, df, time_margin=1 * 60 * 1000, action_margin=5 * 60 * 1000\n",
    "    ):\n",
    "        down_times, up_times = [], []\n",
    "        prev_idx = -1\n",
    "        result_df = df[[\"id_encoded\", \"down_time\", \"up_time\"]].values\n",
    "        for row in tqdm(result_df):\n",
    "            idx, down_time, up_time = int(row[0]), int(row[1]), int(row[2])\n",
    "            if prev_idx != idx:\n",
    "                prev_down_time = down_time\n",
    "                prev_corrected_down_time = 0\n",
    "            gap_down_time = np.clip(down_time - prev_down_time, 0, time_margin)\n",
    "            action_time = np.clip(up_time - down_time, 0, action_margin)\n",
    "\n",
    "            new_down_time = prev_corrected_down_time + gap_down_time\n",
    "            new_up_time = new_down_time + action_time\n",
    "            down_times.append(new_down_time)\n",
    "            up_times.append(new_up_time)\n",
    "            prev_idx, prev_corrected_down_time, prev_down_time = (\n",
    "                idx,\n",
    "                new_down_time,\n",
    "                down_time,\n",
    "            )\n",
    "        df[\"down_time\"], df[\"up_time\"] = down_times, up_times\n",
    "        return df"
   ],
   "id": "f46666a4bdbed0bb",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T15:55:47.475394Z",
     "start_time": "2025-11-10T15:54:15.822395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor = Preprocess()\n",
    "# ------------------ Config dataset (In submission we only have test file) ----------------------------\n",
    "df = pd.read_csv(\"data/test_logs.csv\")\n",
    "# ------------------ Config dataset (TFIDF has to be fit on train and transform on test) ----------------------------\n",
    "train_df = pd.read_csv(\"data/train_logs.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = preprocessor.label_encoding(df)\n",
    "df = preprocessor.remove_start_and_end_time(df)\n",
    "df = preprocessor.remove_rest_time(df)\n",
    "\n",
    "train_df = preprocessor.label_encoding(train_df)\n",
    "train_df = preprocessor.remove_start_and_end_time(train_df)\n",
    "train_df = preprocessor.remove_rest_time(train_df)\n",
    "\n",
    "\n"
   ],
   "id": "5ee90682bf30955e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1486.64it/s]\n",
      "100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "100%|██████████| 2471/2471 [00:03<00:00, 768.87it/s]\n",
      "100%|██████████| 8399747/8399747 [01:17<00:00, 108505.36it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "14ac11512dd86d21",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Event,Unicode Cleaning from `Preprocessing.ipynb`"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:27.743359Z",
     "start_time": "2025-11-10T17:52:27.739066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def label_encoding(df, col=\"id\"):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df[col])\n",
    "    df[col + \"_encoded\"] = label_encoder.transform(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "# remove time that the author havent start writing or is resting\n",
    "# reference: remove_margin for https://www.kaggle.com/code/tomooinubushi/1st-place-solution-training-and-inference-code\n",
    "\n",
    "def remove_procrastination_time(df, start_margin=2*60*1000, end_margin=2*60*1000):\n",
    "    df = df[df['up_event'] != 'Unidentified'].reset_index(drop=True)\n",
    "    result_df = []\n",
    "    grouped_df = df.groupby('id_encoded')\n",
    "\n",
    "    for _, log in tqdm(grouped_df):\n",
    "        valid_events = log[(log.activity != 'Nonproduction') & (\n",
    "            log.up_event != 'Shift') & (log.up_event != 'CapsLock')].down_time.values\n",
    "        if len(valid_events) == 0:\n",
    "            continue\n",
    "        log = log[(log.down_time > valid_events.min() - start_margin)\n",
    "                  & (log['down_time'] <= valid_events.max() + end_margin)].copy()\n",
    "        log['event_id'] = range(len(log))\n",
    "        result_df.append(log)\n",
    "\n",
    "    result = pd.concat(result_df, ignore_index=True)\n",
    "\n",
    "    return result\n"
   ],
   "id": "7f500759e7bd5c27",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:36.197552Z",
     "start_time": "2025-11-10T17:52:28.275583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CleanPreprocessor:\n",
    "    def cleaning(self,df):\n",
    "        df = label_encoding(df)\n",
    "        df = remove_procrastination_time(df)\n",
    "        df = df[df['activity'] != 'Nonproduction' ].reset_index(drop=True)\n",
    "        cols = ['down_event', 'up_event', 'text_change']\n",
    "\n",
    "        df.loc[:, cols] = df.loc[:, cols].apply(\n",
    "            lambda s: s.astype('string').map(lambda x: ftfy.fix_text(x) if x is not pd.NA else x)\n",
    "        )\n",
    "        \n",
    "        drop_events = ['LeftClick','RightClick']\n",
    "        df = df[~df['down_event'].isin(drop_events)]\n",
    "        df['event_id'] = df.groupby('id').cumcount() + 1 # reset event_id\n",
    "        df.reset_index(inplace=True,drop=True)\n",
    "        return df\n",
    "    \n",
    "\n",
    "cleaner = CleanPreprocessor()\n",
    "df = cleaner.cleaning(df)\n",
    "train_df = cleaner.cleaning(train_df)\n",
    "\n",
    "        \n"
   ],
   "id": "acb6f3ac8f1b32ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 999.99it/s]\n",
      "100%|██████████| 2471/2471 [00:03<00:00, 708.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     19\u001B[39m cleaner = CleanPreprocessor()\n\u001B[32m     20\u001B[39m df = cleaner.cleaning(df)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m train_df = \u001B[43mcleaner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcleaning\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mCleanPreprocessor.cleaning\u001B[39m\u001B[34m(self, df)\u001B[39m\n\u001B[32m      3\u001B[39m df = label_encoding(df)\n\u001B[32m      4\u001B[39m df = remove_procrastination_time(df)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m df = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mactivity\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mNonproduction\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreset_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdrop\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m cols = [\u001B[33m'\u001B[39m\u001B[33mdown_event\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mup_event\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mtext_change\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      8\u001B[39m df.loc[:, cols] = df.loc[:, cols].apply(\n\u001B[32m      9\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m s: s.astype(\u001B[33m'\u001B[39m\u001B[33mstring\u001B[39m\u001B[33m'\u001B[39m).map(\u001B[38;5;28;01mlambda\u001B[39;00m x: ftfy.fix_text(x) \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pd.NA \u001B[38;5;28;01melse\u001B[39;00m x)\n\u001B[32m     10\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\pandas\\core\\frame.py:6439\u001B[39m, in \u001B[36mDataFrame.reset_index\u001B[39m\u001B[34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001B[39m\n\u001B[32m   6437\u001B[39m     new_obj = \u001B[38;5;28mself\u001B[39m\n\u001B[32m   6438\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m6439\u001B[39m     new_obj = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   6440\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m allow_duplicates \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lib.no_default:\n\u001B[32m   6441\u001B[39m     allow_duplicates = validate_bool_kwarg(allow_duplicates, \u001B[33m\"\u001B[39m\u001B[33mallow_duplicates\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\pandas\\core\\generic.py:6833\u001B[39m, in \u001B[36mNDFrame.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m   6684\u001B[39m \u001B[38;5;129m@final\u001B[39m\n\u001B[32m   6685\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcopy\u001B[39m(\u001B[38;5;28mself\u001B[39m, deep: bool_t | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mTrue\u001B[39;00m) -> Self:\n\u001B[32m   6686\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   6687\u001B[39m \u001B[33;03m    Make a copy of this object's indices and data.\u001B[39;00m\n\u001B[32m   6688\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   6831\u001B[39m \u001B[33;03m    dtype: int64\u001B[39;00m\n\u001B[32m   6832\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m6833\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_mgr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6834\u001B[39m     \u001B[38;5;28mself\u001B[39m._clear_item_cache()\n\u001B[32m   6835\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001B[32m   6836\u001B[39m         \u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mcopy\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   6837\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:612\u001B[39m, in \u001B[36mBaseBlockManager.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m    609\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    610\u001B[39m         new_axes = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.axes)\n\u001B[32m--> \u001B[39m\u001B[32m612\u001B[39m res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcopy\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    613\u001B[39m res.axes = new_axes\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ndim > \u001B[32m1\u001B[39m:\n\u001B[32m    616\u001B[39m     \u001B[38;5;66;03m# Avoid needing to re-compute these\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001B[39m, in \u001B[36mBaseBlockManager.apply\u001B[39m\u001B[34m(self, f, align_keys, **kwargs)\u001B[39m\n\u001B[32m    361\u001B[39m         applied = b.apply(f, **kwargs)\n\u001B[32m    362\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m         applied = \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    364\u001B[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001B[32m    366\u001B[39m out = \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).from_blocks(result_blocks, \u001B[38;5;28mself\u001B[39m.axes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:826\u001B[39m, in \u001B[36mBlock.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    825\u001B[39m     refs = \u001B[38;5;28mself\u001B[39m.refs\n\u001B[32m--> \u001B[39m\u001B[32m826\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplacement\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_mgr_locs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mndim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:41.607051Z",
     "start_time": "2025-11-10T17:52:36.904456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dont want to waste my time running Unicode Cleaning so just read first \n",
    "# TODO DELETE THIS WHEN SUBMITTING\n",
    "train_df = pd.read_csv('data/train_logs_clean.csv')"
   ],
   "id": "67e936dfccbd7210",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-------\n",
    "### Text Essay Rebuilding\n",
    "Work is taken from `text_process.py`"
   ],
   "id": "e37223415ec44e56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:52:52.360760Z",
     "start_time": "2025-11-10T17:52:52.350233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextProcessor:\n",
    "    PUNCTUATION_MAP = {\n",
    "        \"SPACE\": \" \",\n",
    "        \"COMMA\": \",\",\n",
    "        \"DOUBLE_QUOTE\": '\"',\n",
    "        \"PERIOD\": \".\",\n",
    "        \"PARENTHESES_OPEN\": \"(\",\n",
    "        \"PARENTHESES_CLOSE\": \")\",\n",
    "        \"SQUARE_BRACKET_OPEN\": \"[\",\n",
    "        \"SQUARE_BRACKET_CLOSE\": \"]\",\n",
    "        \"CURLY_BRACKET_OPEN\": \"{\",\n",
    "        \"CURLY_BRACKET_CLOSE\": \"}\",\n",
    "        \"EXCLAMATION_MARK\": \"!\",\n",
    "        \"QUESTION_MARK\": \"?\",\n",
    "    }\n",
    "\n",
    "    def insert_text(self, text, s, pos):\n",
    "        return \"\".join((text[:pos], s, text[pos:]))\n",
    "\n",
    "    def remove_text(self, text, s, pos):\n",
    "        return \"\".join((text[:pos], text[pos + len(s):]))\n",
    "\n",
    "    def replace_text(self, text, s1, s2, pos):\n",
    "        return \"\".join((text[:pos], s2, text[pos + len(s1):]))\n",
    "\n",
    "    def move_text(self, text, s, pos1, pos2):\n",
    "        text = self.remove_text(text, s, pos1)\n",
    "        text = self.insert_text(text, s, pos2)\n",
    "        return text\n",
    "\n",
    "    def split_to_word(self, s):\n",
    "        s = s.lower()\n",
    "        char_sep = \"@\"\n",
    "        punctuation_chars = list(self.PUNCTUATION_MAP.values())\n",
    "        for pun in punctuation_chars:\n",
    "            s = s.replace(pun, char_sep)\n",
    "        s_arr = re.split(char_sep, s)\n",
    "        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty words\n",
    "        return s_arr\n",
    "\n",
    "    def split_to_sentence(self, s):\n",
    "        s = s.lower()\n",
    "        char_sep = \"@\"\n",
    "        punctuation = [\".\", \"!\", \"?\"]\n",
    "        for punc in punctuation:\n",
    "            s = s.replace(punc, char_sep)\n",
    "        s_arr = re.split(char_sep, s)\n",
    "        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty sentences\n",
    "        return s_arr\n",
    "\n",
    "    def split_to_paragraph(self, s):\n",
    "        s = s.lower()\n",
    "        s_arr = re.split(r'n\\s*n+', s)\n",
    "        s_arr = [w for w in s_arr if w.strip()]  # Keep non-empty paragraphs\n",
    "        return s_arr\n",
    "\n",
    "    def change_punctuation(self, text):\n",
    "        reverse_map = {v: k.lower()\n",
    "                       for k, v in self.PUNCTUATION_MAP.items()}\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if char in reverse_map:\n",
    "                result.append(' ' + reverse_map[char] + ' ')\n",
    "            else:\n",
    "                result.append(char)\n",
    "        output = \"\".join(result)\n",
    "        output = re.sub(r\"\\s+\", \" \", output).strip()\n",
    "\n",
    "        return output\n",
    "\n",
    "class EssayConstructor:\n",
    "    def __init__(self):\n",
    "        self.text_processor = TextProcessor()\n",
    "\n",
    "    def recon_writing(self, df):\n",
    "        res_all = []\n",
    "        len_texts = []\n",
    "        sentence_counts = []\n",
    "        paragraph_counts = []\n",
    "\n",
    "        res = \"\"\n",
    "        prev_idx = \"\"\n",
    "\n",
    "        temp_df = df[['id', 'activity', 'up_event', 'text_change',\n",
    "                      'cursor_position', 'word_count']].values\n",
    "\n",
    "        for row in tqdm(temp_df):\n",
    "            idx = str(row[0])\n",
    "            activity, up_event, text_change = str(\n",
    "                row[1]), str(row[2]), str(row[3])\n",
    "            cursor_position, _ = int(row[4]), int(row[5])\n",
    "\n",
    "            # new idx\n",
    "            if idx != prev_idx:\n",
    "                if prev_idx != \"\":\n",
    "                    # append first essay data\n",
    "                    res_all.append(res)\n",
    "                    len_texts.append(len_text)\n",
    "                    sentence_counts.append(sentence_count)\n",
    "                    paragraph_counts.append(paragraph_count)\n",
    "\n",
    "                res, len_text, sentence_count, paragraph_count = \"\", 0, 0, 0\n",
    "                prev_idx = idx\n",
    "\n",
    "            if activity != \"Nonproduction\":\n",
    "                # replace the newline character to n\n",
    "                text_change = text_change.replace(\"@\", \"/\").replace(\"\\n\", \"n\")\n",
    "\n",
    "                if (activity == \"Input\") | (activity == \"Paste\"):\n",
    "                    res = self.text_processor.insert_text(\n",
    "                        res, text_change, cursor_position - len(text_change)\n",
    "                    )\n",
    "\n",
    "                elif activity == \"Remove/Cut\":\n",
    "                    res = self.text_processor.remove_text(\n",
    "                        res, text_change, cursor_position\n",
    "                    )\n",
    "\n",
    "                elif activity == \"Replace\":\n",
    "                    before, after = text_change.split(\" => \")\n",
    "                    res = self.text_processor.replace_text(\n",
    "                        res, before, after, cursor_position - len(after)\n",
    "                    )\n",
    "\n",
    "                elif \"Move\" in activity:\n",
    "                    pos = [int(s) for s in re.findall(r\"\\d+\", activity)]\n",
    "                    # pos 0 start pos1 end pos2 start pos3 end\n",
    "                    res = self.text_processor.move_text(\n",
    "                        res, text_change, pos[0], pos[2]\n",
    "                    )\n",
    "\n",
    "                len_text = len(res)\n",
    "                sentence_count = len(\n",
    "                    self.text_processor.split_to_sentence(res))\n",
    "                paragraph_count = len(\n",
    "                    self.text_processor.split_to_paragraph(res))\n",
    "\n",
    "            prev_up_event = up_event\n",
    "\n",
    "        # append last essay data\n",
    "        res_all.append(res)\n",
    "        len_texts.append(len_text)\n",
    "        sentence_counts.append(sentence_count)\n",
    "        paragraph_counts.append(paragraph_count)\n",
    "\n",
    "        return res_all, len_texts, sentence_counts, paragraph_counts\n"
   ],
   "id": "1ec1c77dcdf4db6",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:35.131815Z",
     "start_time": "2025-11-10T17:52:52.995546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "essay_constructor = EssayConstructor()\n",
    "reconstructed_texts, len_texts, sentence_counts, paragraph_counts = essay_constructor.recon_writing(\n",
    "    df)\n",
    "idx = df[\"id\"].unique()\n",
    "result_df = pd.DataFrame({\"id\": idx, \"text\": reconstructed_texts, \"len_text\": len_texts,\n",
    "                         \"sentence_count\": sentence_counts, \"paragraph_count\": paragraph_counts})\n",
    "\n",
    "extracted_text = result_df\n",
    "\n",
    "reconstructed_texts, len_texts, sentence_counts, paragraph_counts = essay_constructor.recon_writing(\n",
    "    train_df)\n",
    "idx = train_df[\"id\"].unique()\n",
    "extracted_text_train = pd.DataFrame({\"id\": idx, \"text\": reconstructed_texts, \"len_text\": len_texts,\n",
    "                         \"sentence_count\": sentence_counts, \"paragraph_count\": paragraph_counts})\n"
   ],
   "id": "f4fb07326d0b40d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "100%|██████████| 7702047/7702047 [00:40<00:00, 187927.81it/s]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-------------------\n",
    "## Feature Engineering\n",
    "### Behaviour Feature\n",
    "\n",
    "This part is taken from `feature_extraction.ipynb`"
   ],
   "id": "a462b52335010734"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Feature Extraction Functions\n",
    "\n",
    "Extract different behavioural features from keystroke logs.\n",
    "We want to capture:\n",
    "\n",
    "- pauses (when people are thinking)\n",
    "- bursts (when they're typing continuously)\n",
    "- editing behaviour (how much they revise)\n",
    "- cursor movement (planning vs going back to edit)\n",
    "\n",
    "### 2.1 Base Features\n"
   ],
   "id": "9d7beaba4ec2037e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:37.752695Z",
     "start_time": "2025-11-10T17:53:37.750660Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7592a07b9e61f583",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:38.504966Z",
     "start_time": "2025-11-10T17:53:38.499633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"Pull out the main features from the log data\"\"\"\n",
    "    \n",
    "    # Count up events and get basic stats\n",
    "    features = df.groupby(\"id\").agg(\n",
    "        events_count=('event_id', 'count'),\n",
    "        total_time=('up_time', 'max'),\n",
    "        total_chars=('word_count', 'max'),\n",
    "        mean_action_time=('action_time', 'mean'),\n",
    "        std_action_time=('action_time', 'std'),\n",
    "        max_action_time=('action_time', 'max'),\n",
    "        min_action_time=('action_time', 'min'),\n",
    "        \n",
    "        # Count different types of actions\n",
    "        backspace_count=('activity', lambda x: (x == \"Remove/Cut\").sum()),\n",
    "        paste_count=('activity', lambda x: (x == \"Paste\").sum()),\n",
    "        input_count=('activity', lambda x: (x == \"Input\").sum()),\n",
    "        move_count=('activity', lambda x: x.str.contains(\"Move\", na=False).sum()),\n",
    "        replace_count=('activity', lambda x: (x == \"Replace\").sum()),\n",
    "        nonproduction_count=('activity', lambda x: (x == \"Nonproduction\").sum()),\n",
    "        \n",
    "        # Where the cursor was\n",
    "        cursor_pos_mean=('cursor_position', 'mean'),\n",
    "        cursor_pos_std=('cursor_position', 'std'),\n",
    "        cursor_pos_max=('cursor_position', 'max'),\n",
    "        \n",
    "        # Word count stats\n",
    "        word_count_mean=('word_count', 'mean'),\n",
    "        word_count_std=('word_count', 'std'),\n",
    "        word_count_diff=('word_count', lambda x: x.max() - x.min()),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Replace any missing values with 0\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Calculate some ratios\n",
    "    features['chars_per_min'] = features['total_chars'] / (features['total_time'] / 60000 + 1e-6)\n",
    "    features['events_per_min'] = features['events_count'] / (features['total_time'] / 60000 + 1e-6)\n",
    "    features['backspace_ratio'] = features['backspace_count'] / (features['input_count'] + 1)\n",
    "    features['paste_ratio'] = features['paste_count'] / (features['events_count'] + 1)\n",
    "    features['replace_ratio'] = features['replace_count'] / (features['events_count'] + 1)\n",
    "    features['nonproduction_ratio'] = features['nonproduction_count'] / (features['events_count'] + 1)\n",
    "    features['revision_ratio'] = (features['backspace_count'] + features['replace_count']) / (features['total_chars'] + 1)\n",
    "    \n",
    "    return features"
   ],
   "id": "ef7a22be6690db7c",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Pause Features\n",
   "id": "1cfc10d8e4c2daff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:39.226162Z",
     "start_time": "2025-11-10T17:53:39.219697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pause_features(df):\n",
    "    \"\"\"Get features about pauses (gaps between keystrokes)\"\"\"\n",
    "    df = df.sort_values([\"id\", \"down_time\"]).copy()\n",
    "    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n",
    "    \n",
    "    # Count pauses at different thresholds (2s, 5s, 10s)\n",
    "    pause_2s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 2000).sum()).rename(\"pause_2s_count\")\n",
    "    pause_5s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 5000).sum()).rename(\"pause_5s_count\")\n",
    "    pause_10s = df.groupby(\"id\")[\"iki\"].apply(lambda x: (x > 10000).sum()).rename(\"pause_10s_count\")\n",
    "    \n",
    "    # Basic pause stats\n",
    "    mean_pause = df.groupby(\"id\")[\"iki\"].mean().rename(\"mean_pause\")\n",
    "    median_pause = df.groupby(\"id\")[\"iki\"].median().rename(\"median_pause\")\n",
    "    std_pause = df.groupby(\"id\")[\"iki\"].std().rename(\"std_pause\")\n",
    "    max_pause = df.groupby(\"id\")[\"iki\"].max().rename(\"max_pause\")\n",
    "    min_pause = df.groupby(\"id\")[\"iki\"].min().rename(\"min_pause\")\n",
    "    \n",
    "    return pause_2s, pause_5s, pause_10s, mean_pause, median_pause, std_pause, max_pause, min_pause\n",
    "\n",
    "\n",
    "def burst_features(df):\n",
    "    \"\"\"Get features about bursts (when they're typing continuously)\"\"\"\n",
    "    df = df.sort_values([\"id\", \"down_time\"]).copy()\n",
    "    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n",
    "    df[\"burst\"] = (df[\"iki\"] > 2000).astype(int)\n",
    "    df[\"burst_id\"] = df.groupby(\"id\")[\"burst\"].cumsum()\n",
    "    \n",
    "    burst_len = df.groupby([\"id\", \"burst_id\"]).size()\n",
    "    avg_burst = burst_len.groupby(\"id\").mean().rename(\"avg_burst\")\n",
    "    max_burst = burst_len.groupby(\"id\").max().rename(\"max_burst\")\n",
    "    std_burst = burst_len.groupby(\"id\").std().rename(\"std_burst\")\n",
    "    \n",
    "    return avg_burst, max_burst, std_burst\n",
    "\n",
    "\n",
    "def p_burst_features(df):\n",
    "    \"\"\"Get P-burst features (how many words per burst)\"\"\"\n",
    "    df = df.sort_values([\"id\", \"down_time\"]).copy()\n",
    "    df[\"iki\"] = df.groupby(\"id\")[\"down_time\"].diff()\n",
    "    \n",
    "    # P-bursts: pauses longer than 2s\n",
    "    df[\"p_burst\"] = (df[\"iki\"] > 2000).astype(int)\n",
    "    df[\"p_burst_id\"] = df.groupby(\"id\")[\"p_burst\"].cumsum()\n",
    "    \n",
    "    # How many words in each burst\n",
    "    p_burst_words = df.groupby([\"id\", \"p_burst_id\"])[\"word_count\"].apply(lambda x: x.max() - x.min())\n",
    "    avg_words_per_p_burst = p_burst_words.groupby(\"id\").mean().rename(\"avg_words_per_p_burst\")\n",
    "    \n",
    "    return avg_words_per_p_burst"
   ],
   "id": "112d8947a87d6079",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Activity Sequence & Text Change Features\n",
   "id": "984b069840a67419"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:40.003936Z",
     "start_time": "2025-11-10T17:53:39.996114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def activity_sequence_features(df):\n",
    "    \"\"\"Get features from activity patterns and transitions\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for id_val in df['id'].unique():\n",
    "        id_df = df[df['id'] == id_val].sort_values('down_time')\n",
    "        activities = id_df['activity'].values\n",
    "        \n",
    "        # Track how activities transition from one to another\n",
    "        transitions = {}\n",
    "        for i in range(len(activities) - 1):\n",
    "            transition = f\"{activities[i]}->{activities[i+1]}\"\n",
    "            transitions[transition] = transitions.get(transition, 0) + 1\n",
    "        \n",
    "        # Common patterns\n",
    "        input_to_remove = transitions.get('Input->Remove/Cut', 0)\n",
    "        remove_to_input = transitions.get('Remove/Cut->Input', 0)\n",
    "        input_to_input = transitions.get('Input->Input', 0)\n",
    "        paste_to_input = transitions.get('Paste->Input', 0)\n",
    "        \n",
    "        # Find the longest streaks of the same activity\n",
    "        max_input_streak = 0\n",
    "        max_remove_streak = 0\n",
    "        current_input_streak = 0\n",
    "        current_remove_streak = 0\n",
    "        \n",
    "        for act in activities:\n",
    "            if act == 'Input':\n",
    "                current_input_streak += 1\n",
    "                max_input_streak = max(max_input_streak, current_input_streak)\n",
    "                current_remove_streak = 0\n",
    "            elif act == 'Remove/Cut':\n",
    "                current_remove_streak += 1\n",
    "                max_remove_streak = max(max_remove_streak, current_remove_streak)\n",
    "                current_input_streak = 0\n",
    "            else:\n",
    "                current_input_streak = 0\n",
    "                current_remove_streak = 0\n",
    "        \n",
    "        # How varied are the activities\n",
    "        unique_activities = len(set(activities))\n",
    "        activity_switches = sum(1 for i in range(len(activities)-1) if activities[i] != activities[i+1])\n",
    "        \n",
    "        features.append({\n",
    "            'id': id_val,\n",
    "            'input_to_remove_trans': input_to_remove,\n",
    "            'remove_to_input_trans': remove_to_input,\n",
    "            'input_to_input_trans': input_to_input,\n",
    "            'paste_to_input_trans': paste_to_input,\n",
    "            'max_input_streak': max_input_streak,\n",
    "            'max_remove_streak': max_remove_streak,\n",
    "            'unique_activities': unique_activities,\n",
    "            'activity_switches': activity_switches,\n",
    "            'activity_switch_rate': activity_switches / len(activities) if len(activities) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def text_change_features(df):\n",
    "    \"\"\"Features about how the text changes\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    \n",
    "    # How much text was added or removed\n",
    "    df['text_change'] = df.groupby('id')['word_count'].diff().fillna(0)\n",
    "    \n",
    "    features = df.groupby('id').agg(\n",
    "        total_text_produced=('text_change', lambda x: x[x > 0].sum()),\n",
    "        total_text_removed=('text_change', lambda x: abs(x[x < 0].sum())),\n",
    "        text_production_rate=('text_change', lambda x: x[x > 0].mean()),\n",
    "        text_removal_rate=('text_change', lambda x: x[x < 0].mean()),\n",
    "        max_text_addition=('text_change', 'max'),\n",
    "        max_text_removal=('text_change', 'min'),\n",
    "        text_volatility=('text_change', 'std'),\n",
    "        positive_text_changes=('text_change', lambda x: (x > 0).sum()),\n",
    "        negative_text_changes=('text_change', lambda x: (x < 0).sum()),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate some more useful ratios\n",
    "    features['text_removal_ratio'] = features['total_text_removed'] / (features['total_text_produced'] + 1)\n",
    "    features['net_text_production'] = features['total_text_produced'] - features['total_text_removed']\n",
    "    features['text_efficiency'] = features['total_text_produced'] / (features['positive_text_changes'] + 1)\n",
    "    \n",
    "    return features"
   ],
   "id": "d175fb896ee72c41",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 Temporal & Velocity Features\n",
   "id": "7720ab0d27756f14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:40.879966Z",
     "start_time": "2025-11-10T17:53:40.869494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def time_based_features(df):\n",
    "    \"\"\"Features based on when things happen (early, middle, late)\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    \n",
    "    # Split the writing session into three parts\n",
    "    df['time_percentile'] = df.groupby('id')['down_time'].rank(pct=True)\n",
    "    \n",
    "    features = []\n",
    "    for id_val in df['id'].unique():\n",
    "        id_df = df[df['id'] == id_val]\n",
    "        \n",
    "        # Split into early, middle, and late phases\n",
    "        early_phase = id_df[id_df['time_percentile'] <= 0.33]\n",
    "        middle_phase = id_df[(id_df['time_percentile'] > 0.33) & (id_df['time_percentile'] <= 0.67)]\n",
    "        late_phase = id_df[id_df['time_percentile'] > 0.67]\n",
    "        \n",
    "        features.append({\n",
    "            'id': id_val,\n",
    "            'early_events': len(early_phase),\n",
    "            'middle_events': len(middle_phase),\n",
    "            'late_events': len(late_phase),\n",
    "            'early_input_ratio': (early_phase['activity'] == 'Input').sum() / (len(early_phase) + 1),\n",
    "            'middle_input_ratio': (middle_phase['activity'] == 'Input').sum() / (len(middle_phase) + 1),\n",
    "            'late_input_ratio': (late_phase['activity'] == 'Input').sum() / (len(late_phase) + 1),\n",
    "            'early_remove_ratio': (early_phase['activity'] == 'Remove/Cut').sum() / (len(early_phase) + 1),\n",
    "            'late_remove_ratio': (late_phase['activity'] == 'Remove/Cut').sum() / (len(late_phase) + 1),\n",
    "            'middle_paste_ratio': (middle_phase['activity'] == 'Paste').sum() / (len(middle_phase) + 1),\n",
    "            'late_phase_activity': len(late_phase) / (len(id_df) + 1),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def keystroke_velocity_features(df):\n",
    "    \"\"\"Features about typing speed\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    df['iki'] = df.groupby('id')['down_time'].diff()\n",
    "    \n",
    "    # Only look at actual typing events\n",
    "    input_df = df[df['activity'] == 'Input'].copy()\n",
    "    \n",
    "    if len(input_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    features = input_df.groupby('id').agg(\n",
    "        input_iki_mean=('iki', 'mean'),\n",
    "        input_iki_std=('iki', 'std'),\n",
    "        input_iki_median=('iki', 'median'),\n",
    "        input_iki_min=('iki', 'min'),\n",
    "        input_iki_max=('iki', 'max'),\n",
    "        fast_keystrokes=('iki', lambda x: (x < 100).sum()),\n",
    "        moderate_keystrokes=('iki', lambda x: ((x >= 100) & (x <= 1000)).sum()),\n",
    "        slow_keystrokes=('iki', lambda x: (x > 1000).sum()),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # How consistent is the typing\n",
    "    features['keystroke_consistency'] = features['input_iki_std'] / (features['input_iki_mean'] + 1)\n",
    "    features['fast_keystroke_ratio'] = features['fast_keystrokes'] / (features['fast_keystrokes'] + features['moderate_keystrokes'] + features['slow_keystrokes'] + 1)\n",
    "    features['typing_rhythm_score'] = features['moderate_keystrokes'] / (features['fast_keystrokes'] + features['moderate_keystrokes'] + features['slow_keystrokes'] + 1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def word_count_velocity_features(df):\n",
    "    \"\"\"Features about how word count changes\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    \n",
    "    features = []\n",
    "    for id_val in df['id'].unique():\n",
    "        id_df = df[df['id'] == id_val]\n",
    "        \n",
    "        word_counts = id_df['word_count'].values\n",
    "        time_stamps = id_df['down_time'].values\n",
    "        \n",
    "        # How fast are words being added\n",
    "        if len(word_counts) > 1:\n",
    "            word_velocity = np.diff(word_counts) / (np.diff(time_stamps) + 1)\n",
    "            \n",
    "            features.append({\n",
    "                'id': id_val,\n",
    "                'avg_word_velocity': np.mean(word_velocity),\n",
    "                'max_word_velocity': np.max(word_velocity),\n",
    "                'min_word_velocity': np.min(word_velocity),\n",
    "                'std_word_velocity': np.std(word_velocity),\n",
    "                'positive_velocity_ratio': (word_velocity > 0).sum() / len(word_velocity)\n",
    "            })\n",
    "        else:\n",
    "            features.append({\n",
    "                'id': id_val,\n",
    "                'avg_word_velocity': 0,\n",
    "                'max_word_velocity': 0,\n",
    "                'min_word_velocity': 0,\n",
    "                'std_word_velocity': 0,\n",
    "                'positive_velocity_ratio': 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def activity_timing_features(df):\n",
    "    \"\"\"How much time is spent on each type of activity\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    \n",
    "    features = []\n",
    "    for id_val in df['id'].unique():\n",
    "        id_df = df[df['id'] == id_val]\n",
    "        \n",
    "        # Add up time for each activity\n",
    "        input_time = id_df[id_df['activity'] == 'Input']['action_time'].sum()\n",
    "        remove_time = id_df[id_df['activity'] == 'Remove/Cut']['action_time'].sum()\n",
    "        paste_time = id_df[id_df['activity'] == 'Paste']['action_time'].sum()\n",
    "        nonprod_time = id_df[id_df['activity'] == 'Nonproduction']['action_time'].sum()\n",
    "        \n",
    "        total_time = id_df['action_time'].sum()\n",
    "        \n",
    "        features.append({\n",
    "            'id': id_val,\n",
    "            'input_time_total': input_time,\n",
    "            'remove_time_total': remove_time,\n",
    "            'paste_time_total': paste_time,\n",
    "            'nonprod_time_total': nonprod_time,\n",
    "            'input_time_ratio': input_time / (total_time + 1),\n",
    "            'remove_time_ratio': remove_time / (total_time + 1),\n",
    "            'productive_time_ratio': (input_time + paste_time) / (total_time + 1),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)"
   ],
   "id": "6dc48e126edb0954",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 Revision & Cursor Movement Features\n",
   "id": "693e39b2c96196b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:41.838680Z",
     "start_time": "2025-11-10T17:53:41.828673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def revision_pattern_features(df):\n",
    "    \"\"\"Features about revision behaviour and editing patterns\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    \n",
    "    features = []\n",
    "    for id_val in df['id'].unique():\n",
    "        id_df = df[df['id'] == id_val]\n",
    "        \n",
    "        # Where in the text are they making changes\n",
    "        cursor_positions = id_df['cursor_position'].values\n",
    "        activities = id_df['activity'].values\n",
    "        word_counts = id_df['word_count'].values\n",
    "        \n",
    "        # Count edits at start, middle, and end\n",
    "        revisions_start = 0\n",
    "        revisions_middle = 0\n",
    "        revisions_end = 0\n",
    "        \n",
    "        for i, (pos, act, wc) in enumerate(zip(cursor_positions, activities, word_counts)):\n",
    "            if act in ['Remove/Cut', 'Replace'] and wc > 0:\n",
    "                relative_pos = pos / (wc + 1)\n",
    "                if relative_pos < 0.33:\n",
    "                    revisions_start += 1\n",
    "                elif relative_pos < 0.67:\n",
    "                    revisions_middle += 1\n",
    "                else:\n",
    "                    revisions_end += 1\n",
    "        \n",
    "        # Look for write-then-edit cycles\n",
    "        review_cycles = 0\n",
    "        in_writing = False\n",
    "        for act in activities:\n",
    "            if act == 'Input':\n",
    "                in_writing = True\n",
    "            elif act in ['Remove/Cut', 'Replace'] and in_writing:\n",
    "                review_cycles += 1\n",
    "                in_writing = False\n",
    "        \n",
    "        # How often they go backwards to edit\n",
    "        backward_movements = sum(1 for i in range(len(cursor_positions)-1) \n",
    "                                if cursor_positions[i+1] < cursor_positions[i])\n",
    "        \n",
    "        total_revisions = revisions_start + revisions_middle + revisions_end\n",
    "        \n",
    "        features.append({\n",
    "            'id': id_val,\n",
    "            'revisions_at_start': revisions_start,\n",
    "            'revisions_at_middle': revisions_middle,\n",
    "            'revisions_at_end': revisions_end,\n",
    "            'total_revisions': total_revisions,\n",
    "            'review_cycles': review_cycles,\n",
    "            'backward_movements': backward_movements,\n",
    "            'early_revision_ratio': revisions_start / (total_revisions + 1),\n",
    "            'end_revision_ratio': revisions_end / (total_revisions + 1),\n",
    "            'revision_density': total_revisions / (len(id_df) + 1),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def cursor_movement_features(df):\n",
    "    \"\"\"Features about how the cursor moves around\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    df['cursor_jump'] = df.groupby('id')['cursor_position'].diff().abs()\n",
    "    \n",
    "    features = df.groupby('id').agg(\n",
    "        avg_cursor_jump=('cursor_jump', 'mean'),\n",
    "        max_cursor_jump=('cursor_jump', 'max'),\n",
    "        total_cursor_movement=('cursor_jump', 'sum'),\n",
    "        small_cursor_jumps=('cursor_jump', lambda x: (x <= 5).sum()),\n",
    "        medium_cursor_jumps=('cursor_jump', lambda x: ((x > 5) & (x <= 50)).sum()),\n",
    "        large_cursor_jumps=('cursor_jump', lambda x: (x > 50).sum()),\n",
    "        cursor_jump_std=('cursor_jump', 'std'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Where is the cursor most of the time\n",
    "    cursor_at_end = df.groupby('id').apply(\n",
    "        lambda x: (x['cursor_position'] == x['word_count']).sum() / len(x)\n",
    "    ).rename('cursor_at_end_ratio')\n",
    "    \n",
    "    cursor_at_start = df.groupby('id').apply(\n",
    "        lambda x: (x['cursor_position'] == 0).sum() / len(x)\n",
    "    ).rename('cursor_at_start_ratio')\n",
    "    \n",
    "    features = features.merge(cursor_at_end, on='id', how='left')\n",
    "    features = features.merge(cursor_at_start, on='id', how='left')\n",
    "    \n",
    "    # Are they mostly writing forwards\n",
    "    features['forward_writing_tendency'] = features['cursor_at_end_ratio']\n",
    "    features['navigation_complexity'] = features['large_cursor_jumps'] / (features['total_cursor_movement'] + 1)\n",
    "    \n",
    "    return features"
   ],
   "id": "f6826a0b5ed8dbe2",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Rolling Window & Distribution Features\n",
   "id": "8590c32459696b96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:42.597482Z",
     "start_time": "2025-11-10T17:53:42.591484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rolling_features(df, window=10):\n",
    "    \"\"\"Look at trends over time using a sliding window\"\"\"\n",
    "    df = df.sort_values(['id', 'down_time']).copy()\n",
    "    \n",
    "    features = []\n",
    "    for id_val in df['id'].unique():\n",
    "        id_df = df[df['id'] == id_val]\n",
    "        \n",
    "        if len(id_df) < window:\n",
    "            features.append({\n",
    "                'id': id_val,\n",
    "                'action_time_rolling_mean': id_df['action_time'].mean(),\n",
    "                'action_time_rolling_std': id_df['action_time'].std(),\n",
    "                'word_count_rolling_trend': 0,\n",
    "                'action_time_trend': 0,\n",
    "                'action_time_acceleration': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Calculate moving averages\n",
    "        action_rolling = id_df['action_time'].rolling(window=window, min_periods=1)\n",
    "        word_rolling = id_df['word_count'].rolling(window=window, min_periods=1)\n",
    "        \n",
    "        # Are things speeding up or slowing down\n",
    "        word_trend = (word_rolling.mean().iloc[-1] - word_rolling.mean().iloc[0]) if len(id_df) >= window else 0\n",
    "        action_trend = (action_rolling.mean().iloc[-1] - action_rolling.mean().iloc[0]) if len(id_df) >= window else 0\n",
    "        \n",
    "        features.append({\n",
    "            'id': id_val,\n",
    "            'action_time_rolling_mean': action_rolling.mean().mean(),\n",
    "            'action_time_rolling_std': action_rolling.std().mean(),\n",
    "            'word_count_rolling_trend': word_trend,\n",
    "            'action_time_trend': action_trend,\n",
    "            'action_time_acceleration': action_rolling.mean().diff().mean()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def action_time_distribution_features(df):\n",
    "    \"\"\"Statistical properties of action times\"\"\"\n",
    "    features = df.groupby('id')['action_time'].agg([\n",
    "        ('action_time_q25', lambda x: x.quantile(0.25)),\n",
    "        ('action_time_q75', lambda x: x.quantile(0.75)),\n",
    "        ('action_time_iqr', lambda x: x.quantile(0.75) - x.quantile(0.25)),\n",
    "        ('action_time_skew', lambda x: x.skew()),\n",
    "        ('action_time_kurtosis', lambda x: x.kurtosis()),\n",
    "    ]).reset_index()\n",
    "    \n",
    "    return features"
   ],
   "id": "e09792b870b3f317",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 Advanced Event Timing Features\n",
   "id": "b9d9fdecfaa30942"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Main Feature Builder\n",
   "id": "d383d0d56340dbc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:44.053892Z",
     "start_time": "2025-11-10T17:53:44.047839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_all_features(df):\n",
    "    \"\"\"\n",
    "    Main function to build all features from log data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input log data with columns: id, event_id, down_time, up_time, \n",
    "        action_time, activity, cursor_position, word_count\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with all extracted features\n",
    "    \"\"\"\n",
    "    print(\"Building all features...\")\n",
    "    \n",
    "    # Base features\n",
    "    print(\"  - Base features\")\n",
    "    features = extract_features(df)\n",
    "    \n",
    "    # Pause features\n",
    "    print(\"  - Pause features\")\n",
    "    pause_feats = pause_features(df)\n",
    "    for feat in pause_feats:\n",
    "        features = features.merge(feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Burst features\n",
    "    print(\"  - Burst features\")\n",
    "    burst_feats = burst_features(df)\n",
    "    for feat in burst_feats:\n",
    "        features = features.merge(feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # P-burst features\n",
    "    print(\"  - P-burst features\")\n",
    "    p_burst_feat = p_burst_features(df)\n",
    "    features = features.merge(p_burst_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Activity sequence features\n",
    "    print(\"  - Activity sequence features\")\n",
    "    activity_seq_feat = activity_sequence_features(df)\n",
    "    features = features.merge(activity_seq_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Text change features\n",
    "    print(\"  - Text change features\")\n",
    "    text_feat = text_change_features(df)\n",
    "    features = features.merge(text_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Time-based features\n",
    "    print(\"  - Time-based features\")\n",
    "    time_feat = time_based_features(df)\n",
    "    features = features.merge(time_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Keystroke velocity features\n",
    "    print(\"  - Keystroke velocity features\")\n",
    "    keystroke_feat = keystroke_velocity_features(df)\n",
    "    if not keystroke_feat.empty:\n",
    "        features = features.merge(keystroke_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Revision pattern features\n",
    "    print(\"  - Revision pattern features\")\n",
    "    revision_feat = revision_pattern_features(df)\n",
    "    features = features.merge(revision_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Cursor movement features\n",
    "    print(\"  - Cursor movement features\")\n",
    "    cursor_feat = cursor_movement_features(df)\n",
    "    features = features.merge(cursor_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Rolling features\n",
    "    print(\"  - Rolling window features\")\n",
    "    rolling_feat = rolling_features(df, window=10)\n",
    "    features = features.merge(rolling_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Action time distribution features\n",
    "    print(\"  - Action time distribution features\")\n",
    "    action_dist_feat = action_time_distribution_features(df)\n",
    "    features = features.merge(action_dist_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Word count velocity features\n",
    "    print(\"  - Word count velocity features\")\n",
    "    word_vel_feat = word_count_velocity_features(df)\n",
    "    features = features.merge(word_vel_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Activity timing features\n",
    "    print(\"  - Activity timing features\")\n",
    "    activity_time_feat = activity_timing_features(df)\n",
    "    features = features.merge(activity_time_feat, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Fill NaN and inf values\n",
    "    features = features.fillna(0)\n",
    "    features = features.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    print(f\"\\nTotal features extracted: {features.shape[1] - 1}\")  # -1 for id column\n",
    "    print(f\"Total samples: {features.shape[0]}\")\n",
    "    \n",
    "    return features"
   ],
   "id": "7d6603aa5462c9be",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Load Data & Extract Features\n",
   "id": "46e134096fba22d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:46.148153Z",
     "start_time": "2025-11-10T17:53:46.140158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load cleaned training logs\n",
    "\n",
    "\n",
    "logs = df\n",
    "print(f\"Loaded {len(logs)} rows\")\n",
    "print(f\"Unique IDs: {logs['id'].nunique()}\")\n",
    "print(f\"\\nColumns: {list(logs.columns)}\")\n",
    "logs.head()"
   ],
   "id": "74d476571f3c4f92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 rows\n",
      "Unique IDs: 3\n",
      "\n",
      "Columns: ['id', 'event_id', 'down_time', 'up_time', 'action_time', 'activity', 'down_event', 'up_event', 'text_change', 'cursor_position', 'word_count', 'id_encoded']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         id  event_id  down_time  up_time  action_time activity down_event  \\\n",
       "0  0000aaaa         1          0       85           85    Input      Space   \n",
       "1  0000aaaa         2      60000    60087           87    Input      Space   \n",
       "2  2222bbbb         1          0       67           67    Input          q   \n",
       "3  2222bbbb         2          0       46           46    Input          q   \n",
       "4  4444cccc         1          0       94           94    Input      Space   \n",
       "\n",
       "  up_event text_change  cursor_position  word_count  id_encoded  \n",
       "0    Space                            0           0           0  \n",
       "1    Space                            1           0           0  \n",
       "2        q           q                0           1           1  \n",
       "3        q           q                1           1           1  \n",
       "4    Space                            0           0           2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>down_time</th>\n",
       "      <th>up_time</th>\n",
       "      <th>action_time</th>\n",
       "      <th>activity</th>\n",
       "      <th>down_event</th>\n",
       "      <th>up_event</th>\n",
       "      <th>text_change</th>\n",
       "      <th>cursor_position</th>\n",
       "      <th>word_count</th>\n",
       "      <th>id_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>Input</td>\n",
       "      <td>Space</td>\n",
       "      <td>Space</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>60000</td>\n",
       "      <td>60087</td>\n",
       "      <td>87</td>\n",
       "      <td>Input</td>\n",
       "      <td>Space</td>\n",
       "      <td>Space</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>Input</td>\n",
       "      <td>Space</td>\n",
       "      <td>Space</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:47.560347Z",
     "start_time": "2025-11-10T17:53:47.489733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract all behavioural features\n",
    "Behavioral_features_temp = build_all_features(logs)"
   ],
   "id": "c75d3182feb04d82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building all features...\n",
      "  - Base features\n",
      "  - Pause features\n",
      "  - Burst features\n",
      "  - P-burst features\n",
      "  - Activity sequence features\n",
      "  - Text change features\n",
      "  - Time-based features\n",
      "  - Keystroke velocity features\n",
      "  - Revision pattern features\n",
      "  - Cursor movement features\n",
      "  - Rolling window features\n",
      "  - Action time distribution features\n",
      "  - Word count velocity features\n",
      "  - Activity timing features\n",
      "\n",
      "Total features extracted: 122\n",
      "Total samples: 3\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Inspect Results\n",
   "id": "8542569a45559e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:50.583514Z",
     "start_time": "2025-11-10T17:53:50.573516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display first few rows\n",
    "print(f\"Feature matrix shape: {Behavioral_features_temp.shape}\")\n",
    "print(f\"\\nFeature names ({len(Behavioral_features_temp.columns)} total):\")\n",
    "print(list(Behavioral_features_temp.columns))\n",
    "Behavioral_features_temp.head()"
   ],
   "id": "415a74eb6426afa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (3, 123)\n",
      "\n",
      "Feature names (123 total):\n",
      "['id', 'events_count', 'total_time', 'total_chars', 'mean_action_time', 'std_action_time', 'max_action_time', 'min_action_time', 'backspace_count', 'paste_count', 'input_count', 'move_count', 'replace_count', 'nonproduction_count', 'cursor_pos_mean', 'cursor_pos_std', 'cursor_pos_max', 'word_count_mean', 'word_count_std', 'word_count_diff', 'chars_per_min', 'events_per_min', 'backspace_ratio', 'paste_ratio', 'replace_ratio', 'nonproduction_ratio', 'revision_ratio', 'pause_2s_count', 'pause_5s_count', 'pause_10s_count', 'mean_pause', 'median_pause', 'std_pause', 'max_pause', 'min_pause', 'avg_burst', 'max_burst', 'std_burst', 'avg_words_per_p_burst', 'input_to_remove_trans', 'remove_to_input_trans', 'input_to_input_trans', 'paste_to_input_trans', 'max_input_streak', 'max_remove_streak', 'unique_activities', 'activity_switches', 'activity_switch_rate', 'total_text_produced', 'total_text_removed', 'text_production_rate', 'text_removal_rate', 'max_text_addition', 'max_text_removal', 'text_volatility', 'positive_text_changes', 'negative_text_changes', 'text_removal_ratio', 'net_text_production', 'text_efficiency', 'early_events', 'middle_events', 'late_events', 'early_input_ratio', 'middle_input_ratio', 'late_input_ratio', 'early_remove_ratio', 'late_remove_ratio', 'middle_paste_ratio', 'late_phase_activity', 'input_iki_mean', 'input_iki_std', 'input_iki_median', 'input_iki_min', 'input_iki_max', 'fast_keystrokes', 'moderate_keystrokes', 'slow_keystrokes', 'keystroke_consistency', 'fast_keystroke_ratio', 'typing_rhythm_score', 'revisions_at_start', 'revisions_at_middle', 'revisions_at_end', 'total_revisions', 'review_cycles', 'backward_movements', 'early_revision_ratio', 'end_revision_ratio', 'revision_density', 'avg_cursor_jump', 'max_cursor_jump', 'total_cursor_movement', 'small_cursor_jumps', 'medium_cursor_jumps', 'large_cursor_jumps', 'cursor_jump_std', 'cursor_at_end_ratio', 'cursor_at_start_ratio', 'forward_writing_tendency', 'navigation_complexity', 'action_time_rolling_mean', 'action_time_rolling_std', 'word_count_rolling_trend', 'action_time_trend', 'action_time_acceleration', 'action_time_q25', 'action_time_q75', 'action_time_iqr', 'action_time_skew', 'action_time_kurtosis', 'avg_word_velocity', 'max_word_velocity', 'min_word_velocity', 'std_word_velocity', 'positive_velocity_ratio', 'input_time_total', 'remove_time_total', 'paste_time_total', 'nonprod_time_total', 'input_time_ratio', 'remove_time_ratio', 'productive_time_ratio']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         id  events_count  total_time  total_chars  mean_action_time  \\\n",
       "0  0000aaaa             2       60087            0              86.0   \n",
       "1  2222bbbb             2          67            1              56.5   \n",
       "2  4444cccc             2          94            1              75.0   \n",
       "\n",
       "   std_action_time  max_action_time  min_action_time  backspace_count  \\\n",
       "0         1.414214               87               85                0   \n",
       "1        14.849242               67               46                0   \n",
       "2        26.870058               94               56                0   \n",
       "\n",
       "   paste_count  ...  min_word_velocity  std_word_velocity  \\\n",
       "0            0  ...                0.0                0.0   \n",
       "1            0  ...                0.0                0.0   \n",
       "2            0  ...                1.0                0.0   \n",
       "\n",
       "   positive_velocity_ratio  input_time_total  remove_time_total  \\\n",
       "0                      0.0               172                  0   \n",
       "1                      0.0               113                  0   \n",
       "2                      1.0               150                  0   \n",
       "\n",
       "   paste_time_total  nonprod_time_total  input_time_ratio  remove_time_ratio  \\\n",
       "0                 0                   0          0.994220                0.0   \n",
       "1                 0                   0          0.991228                0.0   \n",
       "2                 0                   0          0.993377                0.0   \n",
       "\n",
       "   productive_time_ratio  \n",
       "0               0.994220  \n",
       "1               0.991228  \n",
       "2               0.993377  \n",
       "\n",
       "[3 rows x 123 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>events_count</th>\n",
       "      <th>total_time</th>\n",
       "      <th>total_chars</th>\n",
       "      <th>mean_action_time</th>\n",
       "      <th>std_action_time</th>\n",
       "      <th>max_action_time</th>\n",
       "      <th>min_action_time</th>\n",
       "      <th>backspace_count</th>\n",
       "      <th>paste_count</th>\n",
       "      <th>...</th>\n",
       "      <th>min_word_velocity</th>\n",
       "      <th>std_word_velocity</th>\n",
       "      <th>positive_velocity_ratio</th>\n",
       "      <th>input_time_total</th>\n",
       "      <th>remove_time_total</th>\n",
       "      <th>paste_time_total</th>\n",
       "      <th>nonprod_time_total</th>\n",
       "      <th>input_time_ratio</th>\n",
       "      <th>remove_time_ratio</th>\n",
       "      <th>productive_time_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>60087</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>56.5</td>\n",
       "      <td>14.849242</td>\n",
       "      <td>67</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>26.870058</td>\n",
       "      <td>94</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 123 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:51.563317Z",
     "start_time": "2025-11-10T17:53:51.489318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for any issues\n",
    "print(\"Missing values per column:\")\n",
    "print(Behavioral_features_temp.isnull().sum().sum())\n",
    "print(\"\\nInfinite values per column:\")\n",
    "print(np.isinf(Behavioral_features_temp.select_dtypes(include=[np.number])).sum().sum())\n",
    "print(\"\\nBasic statistics:\")\n",
    "Behavioral_features_temp.describe().T"
   ],
   "id": "7ffe84df865bf2a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "0\n",
      "\n",
      "Infinite values per column:\n",
      "0\n",
      "\n",
      "Basic statistics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       count          mean           std        min  \\\n",
       "events_count             3.0      2.000000      0.000000   2.000000   \n",
       "total_time               3.0  20082.666667  34644.771558  67.000000   \n",
       "total_chars              3.0      0.666667      0.577350   0.000000   \n",
       "mean_action_time         3.0     72.500000     14.908052  56.500000   \n",
       "std_action_time          3.0     14.377838     12.734468   1.414214   \n",
       "...                      ...           ...           ...        ...   \n",
       "paste_time_total         3.0      0.000000      0.000000   0.000000   \n",
       "nonprod_time_total       3.0      0.000000      0.000000   0.000000   \n",
       "input_time_ratio         3.0      0.992942      0.001543   0.991228   \n",
       "remove_time_ratio        3.0      0.000000      0.000000   0.000000   \n",
       "productive_time_ratio    3.0      0.992942      0.001543   0.991228   \n",
       "\n",
       "                             25%        50%           75%           max  \n",
       "events_count            2.000000   2.000000      2.000000      2.000000  \n",
       "total_time             80.500000  94.000000  30090.500000  60087.000000  \n",
       "total_chars             0.500000   1.000000      1.000000      1.000000  \n",
       "mean_action_time       65.750000  75.000000     80.500000     86.000000  \n",
       "std_action_time         8.131728  14.849242     20.859650     26.870058  \n",
       "...                          ...        ...           ...           ...  \n",
       "paste_time_total        0.000000   0.000000      0.000000      0.000000  \n",
       "nonprod_time_total      0.000000   0.000000      0.000000      0.000000  \n",
       "input_time_ratio        0.992303   0.993377      0.993799      0.994220  \n",
       "remove_time_ratio       0.000000   0.000000      0.000000      0.000000  \n",
       "productive_time_ratio   0.992303   0.993377      0.993799      0.994220  \n",
       "\n",
       "[122 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>events_count</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_time</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20082.666667</td>\n",
       "      <td>34644.771558</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>30090.500000</td>\n",
       "      <td>60087.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chars</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_action_time</th>\n",
       "      <td>3.0</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>14.908052</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_action_time</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.377838</td>\n",
       "      <td>12.734468</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>8.131728</td>\n",
       "      <td>14.849242</td>\n",
       "      <td>20.859650</td>\n",
       "      <td>26.870058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paste_time_total</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonprod_time_total</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input_time_ratio</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.992942</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.992303</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.993799</td>\n",
       "      <td>0.994220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remove_time_ratio</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>productive_time_ratio</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.992942</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.992303</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.993799</td>\n",
       "      <td>0.994220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Save Features\n",
   "id": "31c3e2b7c88aa61b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:53.006440Z",
     "start_time": "2025-11-10T17:53:53.004496Z"
    }
   },
   "cell_type": "code",
   "source": "Behavioural_features = Behavioral_features_temp\n",
   "id": "75e8a93767934cc2",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "This notebook extracts **comprehensive behavioural features** from keystroke logging data. The features capture:\n",
    "\n",
    "### Feature Categories (150+ features total):\n",
    "\n",
    "1. **Base Features**: Event counts, total time, typing speed, activity ratios\n",
    "2. **Pause Features**: Gaps between keystrokes at different thresholds (2s, 5s, 10s)\n",
    "3. **Burst Features**: When they're typing continuously and how fluently\n",
    "4. **Activity Sequence**: How activities transition from one to another, streaks, variety\n",
    "5. **Text Change**: How fast they produce/remove text, editing efficiency\n",
    "6. **Temporal Patterns**: What they do in early/middle/late stages\n",
    "7. **Keystroke Velocity**: Typing speed variations, rhythm, consistency\n",
    "8. **Word Count Velocity**: How the word count changes over time\n",
    "9. **Activity Timing**: How much time on each type of activity\n",
    "10. **Revision Patterns**: Where they edit, review cycles, going backwards\n",
    "11. **Cursor Movement**: How they navigate around, jump distances\n",
    "12. **Rolling Window**: Trends and changes in typing behaviour\n",
    "13. **Distribution Features**: Statistical properties (skew, kurtosis, IQR)\n",
    "\n",
    "### Output:\n",
    "\n",
    "- `data/train_behaviour_features.csv` - One row per essay ID with all behavioural features\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Combine with text features from `FeatureExtraction_Essay.ipynb`\n",
    "- Merge with TF-IDF/SVD features from `tfidf/tfidf.ipynb`\n",
    "- Build predictive models using these features\n"
   ],
   "id": "fc07717197013b52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "------------\n",
    "\n",
    "### Essay Text Feature"
   ],
   "id": "cb953272bc7cee9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:56.152375Z",
     "start_time": "2025-11-10T17:53:54.731817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reuse code from text_process\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "def Text_Feature_Extraction(extracted_text):\n",
    "    features = extracted_text.drop('text',axis=1)\n",
    "    texts = extracted_text['text']\n",
    "    processor = TextProcessor()\n",
    "    for i in range(0,texts.shape[0]):\n",
    "        words = processor.split_to_word(texts[i])\n",
    "        sentences = processor.split_to_sentence(texts[i])\n",
    "        word_lengths = [len(w) for w in words]\n",
    "        sent_lengths = [len(processor.split_to_word(s)) for s in sentences]\n",
    "        \n",
    "        features.loc[i,'word_count'] = len(word_lengths)\n",
    "        if len(word_lengths) > 0:\n",
    "            features.loc[i,'word_length_mean'] = sum(word_lengths)/len(word_lengths)\n",
    "            features.loc[i,'word_length_std'] = pd.Series(word_lengths).std()\n",
    "        else:\n",
    "            features.loc[i,'word_length_mean'] = 0\n",
    "            features.loc[i,'word_length_std'] = 0\n",
    "        \n",
    "        if len(sent_lengths) > 0:\n",
    "            features.loc[i,'sent_length_mean'] = sum(sent_lengths)/len(sent_lengths)\n",
    "            features.loc[i,'sent_length_std'] = pd.Series(sent_lengths).std()\n",
    "        else:\n",
    "            features.loc[i,'sent_length_mean'] = 0\n",
    "            features.loc[i,'sent_length_std'] = 0\n",
    "    return features\n",
    "\n",
    "Text_Essay_Features = Text_Feature_Extraction(extracted_text)\n",
    "Train_Text_Essay_Features = Text_Feature_Extraction(extracted_text_train)"
   ],
   "id": "882c88cecc74d863",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:59.223149Z",
     "start_time": "2025-11-10T17:53:59.216712Z"
    }
   },
   "cell_type": "code",
   "source": "Text_Essay_Features",
   "id": "d05b5b531fbe8e69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id  len_text  sentence_count  paragraph_count  word_count  \\\n",
       "0  0000aaaa         2               0                0         0.0   \n",
       "1  2222bbbb         2               1                1         1.0   \n",
       "2  4444cccc         2               1                1         1.0   \n",
       "\n",
       "   word_length_mean  word_length_std  sent_length_mean  sent_length_std  \n",
       "0               0.0              0.0               0.0              0.0  \n",
       "1               2.0              NaN               1.0              NaN  \n",
       "2               1.0              NaN               1.0              NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>len_text</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_length_mean</th>\n",
       "      <th>word_length_std</th>\n",
       "      <th>sent_length_mean</th>\n",
       "      <th>sent_length_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:53:59.964310Z",
     "start_time": "2025-11-10T17:53:59.957351Z"
    }
   },
   "cell_type": "code",
   "source": "Train_Text_Essay_Features",
   "id": "84323f811dcaef79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id  len_text  sentence_count  paragraph_count  word_count  \\\n",
       "0     001519c8      1528              14                3       256.0   \n",
       "1     0022f953      1675              16                1       330.0   \n",
       "2     0042269b      2587              19                6       408.0   \n",
       "3     0059420b      1154              13                1       208.0   \n",
       "4     0075873a      1425              16                5       255.0   \n",
       "...        ...       ...             ...              ...         ...   \n",
       "2466  ffb8c745      1634              13                1       276.0   \n",
       "2467  ffbef7e5      2335              30                6       444.0   \n",
       "2468  ffccd6fd      2761               4                3       203.0   \n",
       "2469  ffec5b38      2552              27                1       417.0   \n",
       "2470  fff05981      1492              11                5       244.0   \n",
       "\n",
       "      word_length_mean  word_length_std  sent_length_mean  sent_length_std  \n",
       "0             4.835938         2.472682         18.285714         6.497675  \n",
       "1             3.900000         2.140718         20.625000        13.608208  \n",
       "2             5.254902         2.743538         21.473684         5.263801  \n",
       "3             4.485577         2.633816         16.000000         6.493587  \n",
       "4             4.450980         2.437516         15.937500         8.667708  \n",
       "...                ...              ...               ...              ...  \n",
       "2466          4.717391         2.606256         21.230769         6.622611  \n",
       "2467          4.155405         2.163872         14.800000         7.526505  \n",
       "2468          4.497537         2.464349         50.750000        10.719919  \n",
       "2469          4.997602         2.915887         15.444444         5.631869  \n",
       "2470          5.049180         2.944906         22.181818        17.040060  \n",
       "\n",
       "[2471 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>len_text</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_length_mean</th>\n",
       "      <th>word_length_std</th>\n",
       "      <th>sent_length_mean</th>\n",
       "      <th>sent_length_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>1528</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4.835938</td>\n",
       "      <td>2.472682</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>6.497675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>1675</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>330.0</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2.140718</td>\n",
       "      <td>20.625000</td>\n",
       "      <td>13.608208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>2587</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>408.0</td>\n",
       "      <td>5.254902</td>\n",
       "      <td>2.743538</td>\n",
       "      <td>21.473684</td>\n",
       "      <td>5.263801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>1154</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>208.0</td>\n",
       "      <td>4.485577</td>\n",
       "      <td>2.633816</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.493587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>1425</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>255.0</td>\n",
       "      <td>4.450980</td>\n",
       "      <td>2.437516</td>\n",
       "      <td>15.937500</td>\n",
       "      <td>8.667708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>1634</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>276.0</td>\n",
       "      <td>4.717391</td>\n",
       "      <td>2.606256</td>\n",
       "      <td>21.230769</td>\n",
       "      <td>6.622611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>2335</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>444.0</td>\n",
       "      <td>4.155405</td>\n",
       "      <td>2.163872</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>7.526505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>2761</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>203.0</td>\n",
       "      <td>4.497537</td>\n",
       "      <td>2.464349</td>\n",
       "      <td>50.750000</td>\n",
       "      <td>10.719919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>2552</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>417.0</td>\n",
       "      <td>4.997602</td>\n",
       "      <td>2.915887</td>\n",
       "      <td>15.444444</td>\n",
       "      <td>5.631869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>1492</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>244.0</td>\n",
       "      <td>5.049180</td>\n",
       "      <td>2.944906</td>\n",
       "      <td>22.181818</td>\n",
       "      <td>17.040060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---------\n",
    "### TF-IDF Feature Extraction\n",
    "\n",
    "### Works taken from `texts_tfidf.ipynb`"
   ],
   "id": "c1c02e81ddbeab5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:54:05.436747Z",
     "start_time": "2025-11-10T17:54:01.076117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "\n",
    "texts_train = extracted_text_train[['id', 'text']]\n",
    "texts_test = extracted_text[['id', 'text']]\n",
    "\n",
    "\n",
    "# refactor version, completely separate train and test\n",
    "\n",
    "\n",
    "# only fit on train\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=30000,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(texts_train['text'])\n",
    "\n",
    "n_features = X_train_tfidf.shape[1]\n",
    "svdsize = min(64, max(1, n_features - 1))\n",
    "\n",
    "svd = TruncatedSVD(\n",
    "    n_components=svdsize,\n",
    "    random_state=42,\n",
    "    n_iter=7\n",
    ")\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "\n",
    "\n",
    "# train_svd_df = pd.DataFrame(\n",
    "#     X_train_svd,\n",
    "#     columns=[f'{i:02d}' for i in range(svdsize)]\n",
    "# )\n",
    "# train_svd_df.insert(0, 'id', texts_train['id'].values)\n",
    "# train_svd_df.to_csv(\"/data/train_tfidf_svd.csv\", index=False)\n"
   ],
   "id": "29ae800624b42880",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:54:06.271034Z",
     "start_time": "2025-11-10T17:54:06.264956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test_tfidf = vectorizer.transform(texts_test['text'])\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "svdsize = X_test_svd.shape[1]\n",
    "test_svd_df = pd.DataFrame(\n",
    "    X_test_svd,\n",
    "    columns=[f'{i:02d}' for i in range(svdsize)]\n",
    ")\n",
    "test_svd_df.insert(0, 'id', texts_test['id'].values)\n",
    "TFIDF_Features = test_svd_df"
   ],
   "id": "5430c69169f27dfd",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Works taken from `operation_tfidf.ipynb` ",
   "id": "39ffc8baeead9aac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:54:14.887721Z",
     "start_time": "2025-11-10T17:54:07.729790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === TRAIN ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "activity_df = train_df[['id', 'activity']]\n",
    "\n",
    "print(train_df.head(3))\n",
    "\n",
    "\n",
    "def rebuild_text(grp):\n",
    "    buf = []\n",
    "    for op in grp['activity']:\n",
    "        buf.append(op[0])\n",
    "    return \"\".join(buf)\n",
    "\n",
    "operations = (\n",
    "    activity_df.groupby('id')\n",
    "               .apply(rebuild_text)\n",
    "               .reset_index(name='operation')\n",
    ")\n",
    "\n",
    "print(operations.head(3))\n",
    "\n",
    "# 3) TF-IDF（在训练集上 fit）\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=30000,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "X_tfidf = vectorizer.fit_transform(operations['operation'])\n",
    "\n",
    "\n",
    "# 4) SVD（在训练集上 fit）\n",
    "n_features = X_tfidf.shape[1]\n",
    "svdsize = min(64, n_features - 1)\n",
    "\n",
    "svd = TruncatedSVD(\n",
    "    n_components=svdsize,\n",
    "    random_state=42,\n",
    "    n_iter=7\n",
    ")\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "\n",
    "svd_df = pd.DataFrame(\n",
    "    X_svd,\n",
    "    columns=[f'{i:02d}' for i in range(svdsize)]\n",
    ")\n",
    "svd_df.insert(0, 'id', operations['id'].values)\n",
    "\n",
    "print(svd_df.head())\n",
    "train_svd_df_operation = svd_df\n"
   ],
   "id": "28f568d38cf240f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        id  event_id  down_time  up_time  action_time activity  \\\n",
      "0           0  001519c8         1      60147    60238           91    Input   \n",
      "1           1  001519c8         2      60657    60784          127    Input   \n",
      "2           2  001519c8         3      60757    60861          104    Input   \n",
      "\n",
      "  down_event up_event text_change  cursor_position  word_count  id_encoded  \n",
      "0          q        q           q                1           1           0  \n",
      "1          q        q           q                2           1           0  \n",
      "2          q        q           q                3           1           0  \n",
      "         id                                          operation\n",
      "0  001519c8  IIIIIIIIIIIIIIIRIIIIIIIIIIIIIIRIIIIIIIIIIIIIII...\n",
      "1  0022f953  IIIIIIIIIIIIIIIIIIIRRRRRRRRRRRIIIIIIIIIIIIIIRR...\n",
      "2  0042269b  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...\n",
      "         id        00        01        02        03        04        05  \\\n",
      "0  001519c8  0.997578  0.063804 -0.024434 -0.005175 -0.003506 -0.002932   \n",
      "1  0022f953  0.999739 -0.007825 -0.020793 -0.003091 -0.001099 -0.000876   \n",
      "2  0042269b  0.999841 -0.009007  0.015063  0.000017 -0.001906 -0.001066   \n",
      "3  0059420b  0.998693 -0.023286 -0.040242  0.001943  0.018245 -0.007376   \n",
      "4  0075873a  0.990940  0.125214  0.048481 -0.000407  0.002281  0.000420   \n",
      "\n",
      "         06        07        08  ...            54        55        56  \\\n",
      "0 -0.000140  0.001695  0.000111  ...  4.461480e-04 -0.003222  0.000042   \n",
      "1  0.003140  0.000388  0.000878  ...  4.057834e-05 -0.000022 -0.000068   \n",
      "2 -0.000430 -0.000686 -0.001471  ... -1.284344e-06  0.000002  0.000014   \n",
      "3 -0.000376  0.000458  0.000934  ... -6.877342e-04 -0.000288  0.000120   \n",
      "4 -0.000107 -0.000848  0.000552  ...  6.727018e-07 -0.000004  0.000023   \n",
      "\n",
      "         57        58        59        60            61        62        63  \n",
      "0  0.001053 -0.000749 -0.000043  0.000245  4.230309e-04 -0.000221  0.000815  \n",
      "1  0.000072 -0.000065  0.000114 -0.000172 -8.227809e-05 -0.000044 -0.000059  \n",
      "2 -0.000027 -0.000010 -0.000001 -0.000004  4.888203e-07 -0.000011 -0.000006  \n",
      "3 -0.000135  0.000953  0.000890  0.000045  4.158111e-04  0.000188 -0.000177  \n",
      "4 -0.000005  0.000009 -0.000004  0.000057  2.256565e-05  0.000004  0.000027  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:54:16.584753Z",
     "start_time": "2025-11-10T17:54:16.577748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === TEST ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "test_df_activity = df[['id', 'activity']]\n",
    "\n",
    "\n",
    "\n",
    "test_operations = (\n",
    "    test_df_activity.groupby('id')\n",
    "           .apply(rebuild_text)\n",
    "           .reset_index(name='operation')\n",
    ")\n",
    "\n",
    "# 4) only transform, do not fit on tests\n",
    "X_test_tfidf = vectorizer.transform(test_operations['operation'])\n",
    "X_test_svd   = svd.transform(X_test_tfidf)\n",
    "\n",
    "svdsize = X_test_svd.shape[1]\n",
    "test_svd_df = pd.DataFrame(\n",
    "    X_test_svd,\n",
    "    columns=[f'{i:02d}' for i in range(svdsize)]\n",
    ")\n",
    "test_svd_df.insert(0, 'id', test_operations['id'].values)\n",
    "TFIDF_Features_Operations = test_svd_df"
   ],
   "id": "9bcad21791b5710e",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Concat Data and send to model",
   "id": "93c3fac084d83060"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:54:18.495848Z",
     "start_time": "2025-11-10T17:54:18.486849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def merge_preprocessed_data(data_dir='data'):\n",
    "    dataset_behaviour = Behavioural_features\n",
    "    dataset_text = Text_Essay_Features\n",
    "    dataset_tfidf_text = TFIDF_Features\n",
    "    dataset_tfidf_operation = TFIDF_Features_Operations\n",
    "\n",
    "    # merge on 'id'\n",
    "    merged = dataset_behaviour.merge(dataset_text, on='id', how='inner')\n",
    "\n",
    "    # rename column name\n",
    "    tfidf_text_renamed = dataset_tfidf_text.rename(\n",
    "        columns={col: f'tfidf_text_{col}' if col != 'id' else col\n",
    "                 for col in dataset_tfidf_text.columns}\n",
    "    )\n",
    "    tfidf_operation_renamed = dataset_tfidf_operation.rename(\n",
    "        columns={col: f'tfidf_operation_{col}' if col != 'id' else col\n",
    "                 for col in dataset_tfidf_operation.columns}\n",
    "    )\n",
    "\n",
    "    merged = merged.merge(tfidf_text_renamed, on='id', how='inner')\n",
    "    merged = merged.merge(tfidf_operation_renamed, on='id', how='inner')\n",
    "\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merged_df = merge_preprocessed_data(\"data\")\n",
    "    "
   ],
   "id": "979e903b931499d1",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test Submission only",
   "id": "9d1f483db27f7409"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:54:20.238963Z",
     "start_time": "2025-11-10T17:54:20.219648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === TEST ONLY: load model(s) -> predict on test -> write submission ===\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- paths ----------------\n",
    "sub_path  = 'submission.csv'\n",
    "\n",
    "# ---------------- load test ----------------\n",
    "df_test = merged_df\n",
    "test_ids = df_test[\"id\"].values\n",
    "X_test = df_test.drop(columns=[\"id\"])\n",
    "\n",
    "\n",
    "model_file_candidate = \"lgbm_.pkl\"\n",
    "\n",
    "\n",
    "model_obj = None\n",
    "if os.path.exists(model_file_candidate):\n",
    "    model_obj = joblib.load(model_file_candidate)\n",
    "    print(f\"Loaded model from: {model_file_candidate}\")\n",
    "if model_obj is None:\n",
    "    raise FileNotFoundError(\"No saved model found. Expected one of: \" + \", \".join(model_file_candidate))\n",
    "\n",
    "def predict_with_model_obj(model_obj, X):\n",
    "    # Multiple Fold Model\n",
    "    if isinstance(model_obj, (list, tuple)):\n",
    "        preds = np.mean([m.predict(X) for m in model_obj], axis=0)\n",
    "        return preds\n",
    "    # Single Model\n",
    "    return model_obj.predict(X)\n",
    "\n",
    "test_preds = predict_with_model_obj(model_obj,X_test)\n",
    "\n",
    "# ---------------- write submission ----------------\n",
    "submission = pd.DataFrame({\"id\": test_ids, \"score\": test_preds})\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(f\"Submission saved to: {sub_path}\")"
   ],
   "id": "790b8f9d765beb53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from: lgbm_.pkl\n",
      "Submission saved to: submission.csv\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d55cbf6704fe9270",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
